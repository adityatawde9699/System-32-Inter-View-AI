{
  "version": 1,
  "session_id": "2c838ac6",
  "state": "intro",
  "resume_text": "MANNING Eli Stevens Luca Antiga Essential Excerpts Deep Learning with PyTorch Essential Excerpts Eli Stevens and Luca Antiga Manning Author Picks Copyright 2019 Manning Publications To pre-order or learn more about this book go to www.manning.com 'PSPOMJOFJOGPSNBUJPOBOEPSEFSJOHPGUIJTBOEPUIFS.BOOJOHCPPLT QMFBTFWJTJU XXXNBOOJOHDPN5IFQVCMJTIFSPGGFSTEJTDPVOUTPOUIFTFCPPLTXIFOPSEFSFEJORVBOUJUZ 'PSNPSFJOGPSNBUJPO QMFBTFDPOUBDU 4QFDJBM4BMFT%FQBSUNFOU .BOOJOH1VCMJDBUJPOT$P #BMEXJO3PBE 10#PY 4IFMUFS*TMBOE /: &NBJM&SJO5XPIFZ DPSQTBMFT!NBOOJOHDPN \u00a5CZ.BOOJOH1VCMJDBUJPOT$P\"MMSJHIUTSFTFSWFE /PQBSUPGUIJTQVCMJDBUJPONBZCFSFQSPEVDFE TUPSFEJOBSFUSJFWBMTZTUFN PSUSBOTNJUUFE JO BOZGPSNPSCZNFBO TFMFDUSPOJD NFDIBOJDBM QIPUPDPQZJOH PSPUIFSXJTF XJUIPVUQSJPSXSJUUFO QFSNJTTJPOPGUIFQVCMJTIFS .BOZPGUIFEFTJHOBUJPOTVTFECZNBOVGBDUVSFSTBOETFMMFSTUPEJTUJOHVJTIUIFJSQSPEVDUTBSF DMBJNFEBTUSBEFNBSLT8IFSFUIPTFEFTJHOBUJPOTBQQFBSJOUIFCPPL BOE.BOOJOH 1VCMJDBUJPOTXBTBXBSFPGBUSBEFNBSLDMBJN UIFEFTJHOBUJPOTIBWFCFFOQSJOUFEJOJOJUJBMDBQT PSBMMDBQT 3FDPHOJ[JOHUIFJNQPSUBODFPGQSFTFSWJOHXIBUIBTCFFOXSJUUFO JUJT.BOOJOHTQPMJDZUPIBWF UIFCPPLTXFQVCMJTIQSJOUFEPOBDJEGSFFQBQFS BOEXFFYFSUPVSCFTUFGGPSUTUPUIBUFOE 3FDPHOJ[JOHBMTPPVSSFTQPOTJCJMJUZUPDPOTFSWFUIFSFTPVSDFTPGPVSQMBOFU .BOOJOHCPPLT are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine. Manning Publications Co. 20 Baldwin Road Technical PO Box 761 Shelter Island, NY 11964 Cover designer: Leslie Haimes ISBN: 9781617297120 Printed in the United States of America 1 2 3 4 5 6 7 8 9 10 - EBM - 24 23 22 21 20 19 iii contents preface v 1I n t r o d u c i n g d e e p l e a r n i n g a n d t h e P y T o r c h l i b r a r y 1 1.1 What is PyTorch? 2 1.2 What is this book? 2 1.3 Why PyTorch? 3 1.4 PyTorch has the batteries included 10 2I t s t a r t s w i t h a t e n s o r 1 5 2.1 Tensor fundamentals 18 2.2 Tensors and storages 22 2.3 Size, storage offset, and strides 24 2.4 Numeric types 30 2.5 Indexing tensors 31 2.6 NumPy interoperability 31 2.7 Serializing tensors 32 2.8 Moving tensors to the GPU 34 2.9 The tensor API 35 3R e a l - w o r l d d a t a r e p r e s e n t a t i o n w i t h t e n s o r s 3 9 3.1 Tabular data 40 3.2 Time series 49 3.3 Text 54 3.4 Images 60 3.5 Volumetric data 63 iv CONTENTS 4T h e m e c h a n i c s o f l e a r n i n g 6 7 4.1 Learning is parameter estimation 70 4.2 PyTorch\u2019s autograd: Backpropagate all things 83 5U s i n g a n e u r a l n e t w o r k t o f i t y o u r d a t a 1 0 1 5.1 Artificial neurons 102 5.2 The PyTorch nn module 110 5.3 Subclassing nn.Module 120 index 127 vi about the authors Eli Stevens has worked in Silicon Valley for the past 15 years as a software engineer, and the past 7 years as Chief Technical O fficer of a startup ma king medical device software. Luca Antiga is co-founder and CEO of an AI engineering company located in Bergamo, Italy, and a regu lar contributor to PyTorch. Save 50% on the full book \u2013 eBook, pBook, and MEAP. Enter ebstevens50  in the Promotional Code box when you checkout. Only at manning.com. Deep Learning with PyTorch by Eli Stevens and Luca Antiga ISBN 9781617295263 400 pages (estimated) $49.99 Publication in Winter, 2019 (estimated) 1 Introducing deep learning and the PyTorch library We\u2019re living through exciting times. Th e landscape of what computers can do is changing by the week. Tasks that only a few years ago were thought to require higher cognition are getting solved by machines at near-superhuman levels of per - formance. Tasks such as describing a phot ographic image with a sentence in idiom - atic English, playing complex strategy g a me, and diagnosing a tumor from a radiological scan are all approachable no w by a computer. Even more impressively, computers acquire the abilit y to solve such tasks through examples, rather than human-encoded of handcrafted rules. It would be disingenuous to assert that machines are learning to \u201cthink\u201d in any human sense of the word. Rather, a general class of algorithms is able to approxi- This chapter covers What this book will teach you PyTorch\u2019s role as a library for building deep learning projects The strengths and weaknesses of PyTorch The hardware you\u2019ll need to follow along with the examples 2 CHAPTER 1 Introducing deep learning and the PyTorch library mate complicated, nonlinear pr ocesses extremely effectively. In a way, we\u2019re learning that intelligence, as we subjectively perceive it, is a notion that\u2019s often conflated with self-awareness, and self-awarene ss definitely isn\u2019t required to solve or carry out these kinds of problems. In the end, the question of computer intelligence may not even be important. As pioneering computer scientist Edsger W. Dijkstra said in \u201cThe Threats to Computing Science,\u201d Alan M. Turing thought about . . . th e question of whether Machines Can Think, a question . . . about as relevant as the qu estion of whether Submarines Can Swim. That general class of algorithms we\u2019re ta lking about falls under the category of deep learning, which deals with training mathematical entities named deep neural networks on the basis of examples. Deep learning levera ges large amounts of data to approximate complex functions whos e inputs and outputs are far apar t, such as an image (input) and a line of text describing the input (output); a written script (input) and a natural- sounding voice reciting the script (outpu t); or, even more simply, associating an image of a golden retriever with a flag that indicates that a golden retriever is present. This capability allows developers to crea te programs with functionality that until recently was the exclusive domain of human beings. 1.1 What is PyTorch? PyTorch is a library for Python programs that facilitates building deep learning proj- ects. It emphasizes flexibility and allows d eep learning models to be expressed in idi- omatic Python. This approachability and ease of use found early adopters in the research community, and in the years since th e library\u2019s release, it has grown into one of the most prominent deep learning tools for a broad range of applications. PyTorch provides a core data structure, the Tensor, a multidimensional array that has many similarities with NumPy arrays. Fr om that foundation, a laundry list of fea - tures was built to make it easy to get a pr oject up and running, or to design and train investigation into a new neur al network architecture. Tensors accelerate mathematical operations (assuming that the appropriat e combination of hardware and software is present), and PyTorch has packages for dist ributed training, worker processes for effi - cient data loading, and an extensive libr ary of common deep learning functions. As Python is for programming, PyTorch is both an excellent introduction to deep learning and a tool usable in professional contexts for real-world, high-level work. W e b e l i e v e t h a t P y T o r c h s h o u l d b e t h e f i r s t d e e p l e a r n i n g l i b r a r y y o u l e a r n . Whether it should be the last is a decision that we\u2019ll leave to you. 1.2 What is this book? This book is intended to be a starting point for software engineers, data scientists, and motivated students who are fluent in Pyth on and want to become comfortable using PyTorch to build deep learning projects. To that end, we take a hands-on approach; we encourage you to keep your computer at the ready so that you can play with the examples and take them a step further. 3Why PyTorch? T h o u g h w e s t r e s s t h e p r a c t i c a l a p p l i c a tions, we also believe that providing an accessible introduction to foundational deep learning tools like PyTorch is more than a way to facilitate the acquisition of new tec hnical skills. It is also a step toward equip - ping a new generation of scientists, engine ers, and practitioners from a wide range of disciplines with a working knowledge of th e tools that will be the backbone of many software projects during the decades to come. To get the most out of this book, you need two things: Some experience programming in Python\u2014We\u2019re not going to pull any punches on that one: you\u2019ll need to be up on Python data types, classes, floating-point num- bers, and the like. Willingness to dive in and get your hands dirty \u2014It\u2019ll be much easier for you to learn if you follow along with us. Deep learning is a huge space. In this book, we\u2019ll be covering a tiny part of that space\u2014specifically, using PyTorch for smalle r-scope projects. Most of the motivating examples use image processing of 2D and 3D data sets. We focus on practical PyTorch, with the aim of covering en ough ground to allow you to solve realistic problems with deep learning or explore new models as they pop up in research literature. A great resource for the latest publications relate d to deep learning research is the ArXiV public preprint repository, hosted at https://arxiv.org.1 1.3 Why PyTorch? As we\u2019ve said, deep learning allows you to carry out a wide range of complicated tasks\u2014such as performing machine translat ion, playing strategy games, and identify - ing objects in cluttered scenes\u2014by exposing your model to illustrative examples. To do so in practice, you need tools that are fl exible so that they can be adapted to your specific problem and efficient, to allow trai ning to occur over large amounts of data in reasonable times. You also need the trained network to perform correctly in the pres - ence of uncertainty in the inputs. In this section, we take a look at some of the reasons why we decided to use PyTorch. PyTorch is easy to recommend because of its simplicity. Many researchers and prac- titioners find it easy to learn, use, extend , and debug. It\u2019s Pythonic, and although (like any complicated domain) it ha s caveats and best practices, using the library generally feels familiar to developers who have used Python previously. For users who are familiar with NumPy arrays, the PyTorch Tensor class will be immediately familiar. PyTorch feels like NumPy, but with GPU acceleration and auto - matic computation of gradient s, which makes it suitable for calculating backward pass data automatically starting from a forward expression. The Tensor API is such that the additional fe atures of the class relevant to deep learning are unobtrusive; the user is mostly free to pretend that those features don\u2019t exist until need for them arises. 1We also recommed http://www.arxiv-sanity.com to help organize research papers of interest. 4 CHAPTER 1 Introducing deep learning and the PyTorch library A design driver for PyTorch is expressivi ty, allowing a developer to implement com - plicated models without undue complexity being imposed by the library. (The library isn\u2019t a framework!) PyTorch arguably offers one of the most seamless translations of ideas into Python code in th e deep learning landscape. For this reason, PyTorch has seen widespread adoption in research, as witnessed by the high citation counts in international conferences. 2 PyTorch also has a compelling story for th e transition from research and develop - ment to production. Although it initially focused on research wo rkflows, PyTorch has been equipped with a high-performance C++ runtime that users can leverage to deploy models for inference without relying on Python, keeping most of the flexibility of PyTorch without paying the overhead of the Python runtime. Claims of ease of use and high performa nce are trivial to make, of course. We hope that by the time you\u2019re in the thick of this book, you\u2019ll agree that our claims here are well founded. 1.3.1 The deep learning revolution In this section, we take a step back and provide some context for where PyTorch fits into the current and historical la ndscape of deep learning tools. U n t i l t h e l a t e 2 0 0 0 s , t h e b r o a d e r c l a s s of systems that fell into the category \u201cmachine learning\u201d relied heavily on feature engineering. Features are transformations of input data resulting in numerical features th at facilitate a downstream algorithm, such as a classifier, to produce correct outcomes on new data. Feature engineering aims to take the original data and come up with representations of the same data that can be fed to an algorithm to solve a problem. To tell ones from zeros in images of handwritten digits, for example, you\u2019d come up with a set of filters to estimate the direction of edges over the image and then train a classifi er to predict the correct digit, given a dis- tribution of edge directions. Another useful feature could be the number of enclosed holes in a zero, an eight, or particularly loopy twos. Deep learning, on the other hand, deal s with finding such representations auto- matically, from raw data, to perform a task successfully. In the ones-versus-zeros exam - ple, filters would be refined during training by iteratively looking at pairs of examples and target labels. This isn\u2019t to say that fe ature engineering has no place in deep learn- ing; developers often need to inject some form of knowledge into a learning system. The ability of a neural network to ingest data and extract useful representations on the basis of examples, however, is what makes deep learning so powerful. The focus of deep learning practitioners is not so much on handcrafting those representations but on operating on a mathematical entity so that it discovers repr esentations from the training data autonomously. Often, these automatically created features are better than those that are handcrafted! As in many disruptive technologies, this fact has led to a change in perspective. 2At ICLR 2019, PyTorch appeared as a citation in 2 52 papers, up from 87 the previous year and at the same level as TensorFlow, which appeared in 266 papers. 5Why PyTorch? On the left side of figure 1.1, a practiti oner is busy defining engineering features and feeding them to a learning algorithm. The results of the task will be as good as the features he engineers. On the right side of the figure, with deep learning, the raw data is fed to an algorithm that extracts hierarchical features automatically, based on opti - mizing the performance of the algorithm on the task. The results will be as good as the practitioner\u2019s ability to drive the algorithm toward its goal. Figure 1.1 The change in perspective brought by deep learning 1.3.2 Immediate versus deferred execution One key differentiator for deep learning lib raries is immediate versus deferred execu - tion. Much of PyTorch\u2019s ease of use is du e to how it implements immediate execution, so we briefly cover that implementation here. Consider the expression (a**2 + b**2) ** 0.5 that implements the Pythagorean theorem. If you want to execute this expression, you need to have an a and b handy, like so: >>> a = 3 >>> b = 4 >>> c = (a**2 + b**2) ** 0.5 >>> c 5.0 Immediate execution like this consum es inputs and produces an output value (c here). PyTorch, like Python in general, defaults to immediate execution (referred to as eager mode i n t h e P y T o r c h d o c u m e n t a t i o n ). Immediate execution is useful 6 CHAPTER 1 Introducing deep learning and the PyTorch library because if problems arise in executing the expression, the Python interpreter, debug - ger, and similar tools have direct access to the Python objects involved. Exceptions can be raised directly at the po int where the issue occurred. Alternatively, you could define the Pyth agorean expression even before knowing what the inputs are and use that definition to produce the output when the inputs are available. That callable function that you de fine can be used later, repeatedly, with var- ied inputs: >>> p = lambda a, b: (a**2 + b**2) ** 0.5 >>> p(1, 2) 2.23606797749979 >>> p(3, 4) 5.0 In the second case, you defined a series of operations to perform, resulting in a out - put function (p in this case). You didn\u2019t execute anything until later, when you passed in the inputs\u2014an example of deferred execution. Deferred execution means that most exceptions are be raised when the func tion is called, not when it\u2019s defined. For normal Python (as you see he re), that\u2019s fine, because th e interpreter and debuggers have full access to the Python state at the time when the error occurred. Things get tricky when specialized classe s that have heavy operator overloading are used, allowing what looks li ke immediate execution to be deferred under the hood. These classes can look like the following: >>> a = InputParameterPlaceholder() >>> b = InputParameterPlaceholder() >>> c = (a**2 + b**2) ** 0.5 >>> callable(c) True >>> c(3, 4) 5.0 Often in libraries that use this form of fu nction definition, the operations of squaring a and b, adding, and taking the square root aren \u2019t recorded as high-level Python byte code. Instead, the point usually is to compile the expression into a static computation graph (a graph of basic oper ations) that has some advant age over pure Python (such as compiling the math directly to machine code for performance reasons). T h e f a c t t h a t t h e c o m p u t a t i o n g r a p h i s b u i l t i n o n e p l a c e a n d u s e d i n a n o t h e r makes debugging more difficult , because exceptions often lack specificity about what went wrong and Python debugging tools don\u2019t have any visibility into the intermediate states of the data. Also, stat ic graphs usually don\u2019t mix well with standard Python flow control: they\u2019re de-facto domain-specific la nguages implemented on top of a host lan - guage (Python in this case). N e x t , w e t a k e a m o r e c o n c r e t e l o o k a t t h e d i f f e r e n c e s b e t w e e n i m m e d i a t e a n d deferred execution, specifically regarding issu es that are relevant to neural networks. We won\u2019t be teaching these concepts in an y depth here, instead giving you a high-level introduction to the terminology and the relationships among these concepts. Under - standing those concepts and relationships lays the groundwork for understand how 7Why PyTorch? libraries like PyTorch that use immediate execution differ from deferred-execution frameworks, even though the underlying math is the same for both types. T h e f u n d a m e n t a l b u i l d i n g b l o ck of a neural network is a neuron. Neurons are strung together in large nu mbers to form the network. You see a typical mathematical expression for a single neuron in the first row of figure 1.2: o = tanh(w * x + b). As we explain the execution modes in the foll owing figures, keep these facts in mind: x is the input to the single-neuron computation. w and b are the parameters or weights of the neuron and can be changed as needed. To update the parameters (to produce ou tput that more closely matches what we desire), we assign error to each of the weights via backpropagation and then tweak the weights accordingly. Backpropagation requires computing the gradient of the output with respect to the weights (among other things). We use automatic differentiation to co mpute the gradient automatically, saving us the trouble of writing the calculations by hand. In figure 1.2, the neuron gets compiled in to a symbolic graph in which each node rep- resents individual operations (second row), Figure 1.2 Static graph for a simple computation corresponding to a single neuron u s i n g p l a c e h o l d e r s f o r i n p u t s a n d o u t- puts. Then the graph is evaluated numerica lly (third row) when concrete numbers are plugged into the placeholders (in this case, the numbers are the values stored in w, 8 CHAPTER 1 Introducing deep learning and the PyTorch library x, and b). The gradient of the output with re spect to the weights is constructed sym - bolically by automatic differentiation, wh ich traverses the graph backward and multi - plies the gradients at indivi dual nodes (fourth row). The corresponding mathematical expression is shown in the fifth row. One of the major competing deep learning frameworks is TensorFlow, which has a graph mode that uses a similar kind of defe rred execution. Graph mode is the default mode of operation in TensorFlow 1.0. By c o n t r a s t , P y T o r c h s p o r t s a d e f i n e - b y - r u n dynamic graph engine in which the computation graph is built node by node as the code is eagerly evaluated. T h e t o p h a l f o f f i g u r e 1 . 3 s h o w s t h e same calculation running under a dynamic graph engine. Figure 1.3 Dynamic graph for a simple computation corresponding to a single neuron The computation is broken into indi vidual expressions, which are greed - ily evaluated as they\u2019re encountered. The pr ogram has no advance notion of the inter - connection between computations. The bottom half of the figure shows the behind-the- scenes construction of a dynamic computation graph for the same expression. The expression is still broken into individual operations, but here those operations are eagerly evaluated, and the graph is built incrementally. Automatic differentiation is achieved by traversing the resulting grap h backward, similar to s t a t i c c o m p u t a t i o n graphs. Note that this does not mean dyna mic graph libraries are inherently more capa - 9Why PyTorch? ble than static graph libraries, just that it\u2019s often easier to accomplish looping or condi- tional behavior with dynamic graphs. Dynamic graphs can change during succe ssive forward passes. Different nodes can be invoked according to conditions on th e outputs of the preceding nodes, for exam - ple, without a need for such conditions to be represented in the graph itself\u2014a dis - tinct advantage over static graph approaches. The major frameworks are converging toward supporting both modes of opera - tion. PyTorch 1.0 gained the ability to record the execution of a model in a static com - putation graph or define it through a precompiled scripting language, with the goal of improved performance and ease of putt ing the model into production. TensorFlow has also gained \u201ceager mode,\u201d a new define-by-run API, increasing the library\u2019s flexi- bility as we have discussed. 1.3.3 The deep learning competitive landscape Although all analogies are flawed, it seems that the release of Py Torch 0.1 in January 2017 marked the transition from a Cambrian Explosion\u2013like proliferation of deep learning libraries, wrappers, and data exch ange formats to an era of consolidation and unification. NOTE T h e d e e p l e a r n i n g l a n d s c a p e h a s b e e n moving so quickly lately that by the time you read this book , some aspects may be out of date. If you\u2019re unfa- miliar with some of the libraries mentioned here, that\u2019s fine. At the time of PyTorch\u2019s first beta release Theano and TensorFlow were the premiere low-level deferred-execution libraries. Lasagne and Keras were high-level wrappers around Theano, with Keras wrap- ping TensorFlow and CNTK as well. Caffe, Chainer, Dynet, Torch (the Lu a-based precursor to PyTorch), mxnet, CNTK, DL4J, and others filled va rious niches in the ecosystem. In the roughly two years that followed, th e landscape changed dramatically. The com- munity has largely consolidated behind Py Torch or TensorFlow, with the adoption of other libraries dwindling or filling specific niches: Theano, one of the first deep learning frameworks, has ceased active develop- ment. TensorFlow \u2013C o n s u m e d K e r a s , p r o m o t i n g it to a first-class API \u2013P r o v i d e d a n i m m e d i a t e e x e c u t i o n e a g e r m o d e \u2013 Announced that TF 2.0 will enable eager mode by default PyTorch \u2013C o n s u m e d C a f f e 2 f o r i t s b a c k e n d \u2013R e p l a c e d m o s t o f t h e l o w -level code reused from the Lua-based Torch project 10 CHAPTER 1 Introducing deep learning and the PyTorch library \u2013A d d e d s u p p o r t f o r O N N X , a v e n d o r - n e utral model description and exchange format \u2013A d d e d a d e l a y e d e x e c u t i o n g r a p h mode runtime called TorchScript \u2013R e l e a s e d v e r s i o n 1 . 0 TensorFlow has a robust pipeline to produc tion, an extensive industrywide community, and massive mindshare. PyTorch has made huge inroads with the research and teaching community, thanks to its ease of use, and has picked up momentum as researchers and graduates train students and move to industry. Interestingly, with the advent of Torch - Script and eager mode, both libraries have se en their feature sets start to converge. 1.4 PyTorch has the batteries included We\u2019ve already hinted at a few components of PyTorch. Now we\u2019ll take some time to formalize a high-level map of the main components that form PyTorch. First, PyTorch has the Py from Python, but there\u2019s a lot of non-Python code in it. For performance reasons, most of Py Torch is written in C++ and CUDA 3, a C++-like lan- guage from NVIDIA that can be compiled to run with ma ssive parallelism on NVIDIA GPUs. There are ways to run PyTorch directly from C. One of the main motivations for this capability is providing a reliable strategy for deploying models in production. Most of the time, however, you\u2019ll interact with PyTorch from Python, building models, training them, and using the trained models to solve problems. Depending on a given use case\u2019s requirements for performance and scale, a pure-Python solution can be suf - ficient to put models into production. It can b e p e r f e c t l y v i a b l e t o u s e a F l a s k w e b server to wrap a PyTorch model us ing the Python API, for example. Indeed, the Python API is where PyTorch shines in term of usability and integra - tion with the wider Python ecosystem. Next, we take a peek at the mental model of PyTorch. At its core, PyTorch is a library that provides multidimensional arrays, called tensors in PyTorch parlance, and an extensive library of operations on them is provided by the torch module. Both tensors and related operat ions can run on the CPU or GPU. Run - ning on the GPU results in massive speedups compared with CPU (especially if you\u2019re willing to pay for a top-end GPU), and with PyTorch doing so, it doesn\u2019t require more than an additional function call or two. The second core thing that PyTorch provides allows tensors to keep track of the operations performed on them and to compute derivatives of an output with respect to any of its inputs analytically via backpropagation. This capability is provided natively by tensors and further refined in torch.autograd. We could argue that by having tensors and the autograd-enabled tensor standard library, PyTorch could be used for more th an neural networks, and we\u2019d be correct: PyTorch can be used for physics, renderin g, optimization, simulation, modeling, and so on. We\u2019re likely to see PyTorch being used in creative ways across the spectrum of scientific applications. 3https://www.geforce.com/hardware/technology/cuda 11PyTorch has the batteries included But PyTorch is first and foremost a deep le arning library, and as such, it provides all the building blocks needed to build and tr ain neural networks. Figure 1.4 shows a stan - dard setup that loads data, trains a model, and then deploys that model to production. The core PyTorch modules for building neural networks are located in torch.nn, which provides common neural network laye rs and other architectural components. Fully connected layers, convolutional layers , activation functions, and loss functions can all be found here. These components ca n be used to build and initialize the untrained model shown in th e center of figure 1.4. Figure 1.4 Basic high-level structure of a PyTorch project, with data loading, training, and deployment to production To train this model, you need a few things (besides the loop itself, which can be a stan - dard Python for loop): a source of training data , an optimizer to adapt the model to the training data, and a way to get the model and data to the hardware that will be per - forming the calculations needed for training the model. Utilities for data loading and handling can be found in torch.util.data. The two main classes you\u2019ll work with are Dataset, which acts as the bridge between your cus - tom data (in whatever format it migh t be in), and a standardized PyTorch Tensor. The other class you\u2019ll see a lot of is DataLoader, which can spawn child processes to load data from a Dataset in the background so that it\u2019s ready and waiting for the training loop as soon as the loop can use it. 12 CHAPTER 1 Introducing deep learning and the PyTorch library In the simplest case, the model will be running the required calculations on the local CPU or on a single GPU, so when th e training loop has the data, computation can start immediately. It\u2019s more common, however, to want to use specialized hard - ware such as multiple GPUs or to have mu ltiple machines contribute their resources to training the model. In those cases, torch.nn.DataParallel and torch.distrib- uted can be employed to leverage th e additional hardware available. W h e n y o u h a v e r e s u l t s f r o m r u n n i n g your model on the training data, torch.optim provides standard ways of updating the model so that the output starts to more closely resemble the answers specified in the training data. As mentioned earlier, PyTorch defaults to an immediate execution model (eager mode). Whenever an instruct ion involving PyTorch is executed by the Python inter - preter, the corresponding operation is imme diately carried out by the underlying C++ or CUDA implementation. As more instruct ions operate on tensors, more operations are executed by the backend implementation. This process is as fast as it typically can be on the C++ side, but it incurs the cost o f c a l l i n g t h a t i m p l e m e n t a t i o n t h r o u g h Python. This cost is minute, but it adds up. To bypass the cost of the Python interpreter and offer the opportunity to run mod - els independently from a Python runtime, Py Torch also provides a deferred execution model named TorchScript. Using TorchScript, PyTorch ca n serialize a set of instruc - tions that can be invoked independently from Python. You can think of this model as being a virtual machine with a limited instruction set specific to te n s o r o p e r a t i o n s . Besides not incurring the cost s of calling into Python, this execution mode gives PyTorch the opportunity to Just in Time (JIT) transform sequences of known opera - tions into more efficient fused operations. These features are the basis of the produc - tion deployment capabilities of PyTorch. 1.4.1 Hardware for deep learning Running a pretrained network on new data is within the capabilities of any recent lap - top or personal computer. Even retraining a small portion of a pretrained network to specialize it on a new data set doesn\u2019t ne cessarily require specialized hardware. You can follow along with this book on a standa rd personal computer or laptop. We antici- pate, however, that completing a full tr aining run for more-advanced examples will require a CUDA-capable graphi cal processing unit (GPU), such as a GPU with 8GB of RAM (we suggest an NVIDIA GTX 1070 or b e t t e r ) . B u t t h o s e p a r a m e t e r s c a n b e adjusted if your hardwa re has less RAM available. To be clear: such hardware isn\u2019t mandatory if you\u2019re willing to wait, but running on a GPU cuts training time by at least an order of magnitude (and usually is 40 to 50 times faster). Taken individually, the operations required to compute parameter updates are fast (from fractions of a second to a few seco nds) on modern hardware such as a typical laptop CPU. The issue is that training invo lves running these operations over and over, many times, incrementally updating the network parameters to minimize training error. Moderately large networks can take hours to days to train from scratch on large, real-world data sets on workstations equipped with good GPUs. That time can be 13PyTorch has the batteries included reduced by using multiple GPUs on the sa me machine and even further by using clus - ters of machines equipped with multiple GPUs. Th ese setups are less prohibitive to access than they sound thanks to the offe rings of cloud comput ing providers. DAWN- Bench4 is an interesting initiative from Stanford University aimed at providing bench - marks on training time and cloud computing costs relate d to common deep learning tasks on publicly available data sets. If you have a GPU around, great. Otherwise, we sugges t checking out the offerings of the various cloud platforms, many of which offer GPU-enabled Jupyter notebooks with PyTorch preinstalled, often with a free quota. Last consideration: the operating system (OS). PyTorch has supported Linux and macOS from its first release and gained Windows support during 2018. Because current Apple laptops don\u2019t include GPUs that support CUDA, the precompiled macOS pack - ages for PyTorch are CPU-only. We try to av oid assuming that you run a particular OS; scripts\u2019 command lines should convert to a Windows-compatible form readily. For con - venience, whenever possible we list code as though it\u2019s running on a Jupyter Notebook. For installation information, please s ee the Getting Started guide on the official website.5 We suggest that Windows users insta ll with Anaconda or Miniconda. Other operating systems, such as Linux, typicall y have a wider variety of workable options, with Pip being one of the most common inst allers. Experienced users, of course, are free to install packages in the way that\u2019s mo st compatible with their preferred develop - ment environments. 1.4.2 Using Jupyter Notebooks We\u2019re going to assume that you have PyTo rch and the other dependencies installed and have verified that things are working. We\u2019re going to be making heavy use of Jupy - ter Notebooks for example code. A Jupyter Notebook shows itself as a page in the browser through which you can run code interactively. The code gets evaluated by a kernel, a process running on a server that\u2019s ready to receive code to execute and send back the results, which are rendered inli ne on the page. A notebook maintains the state of the kernel, such as variables define d during the evaluation of code, in memory until it\u2019s terminated or restarted. The fund amental unit with which you interact with a notebook is a cell, a box on the page where you can type code and have the kernel evaluate it (by choosing the menu item or pressing Shift-Enter). You can add multiple cells to a notebook, and the ne w cells see the variables you created in the earlier cells. The value returned by the last line of a ce ll is printed below the cell after execution, and the same goes for plots. By mixing so urce code, results of evaluations, and Mark - down-formatted text cells, you can generate beautiful interactive documents. You can read everything about Jupyter No tebooks on the project website. 6 4https://dawn.cs.stanford.edu/benchmark/index.html 5https://pytorch.org/get-started/locally 6https://jupyter.org 14 CHAPTER 1 Introducing deep learning and the PyTorch library At this point, you\u2019ll need to start the notebook server from the root directory of the code checkout from GitHub. How starting the server looks depends on the details of your operating system and on how and where you installed Ju pyter. If you have questions, feel free to ask on our forums. 7 W h e n t h e n o t e b o o k s e r v e r s t a r t s , y o u r default browser pops up, showing a list of local notebook files. J u p y t e r N o t e b o o k s a r e p o w e r f u l t o o l s f o r e x p r e s s i n g a n d i n v e s t i g a ting ideas through code. Although we think that they make a good fit with our use case, they\u2019re not for everyone. We would argue that it\u2019s important to focus on removing friction and minimizing cognitive overhead, which is going to be different for everyone. Use what you like during your experimentation with PyTorch. You can find full working code for the li stings in this book in our repository on GitHub.8 Exercises Start Python to get an interactive prompt. \u2013W h a t P y t h o n v e r s i o n a r e y o u u s i n g : 2 . x o r 3 . x ? \u2013C a n y o u import torch? What version of PyTorch do you get? \u2013W h a t i s t h e r e s u l t o f torch.cuda.is_available()? Does it match your expectation based on the hardware you\u2019re using? Start the Jupyter Notebook server. \u2013W h a t v e r s i o n o f P y t h o n i s J u p y t e r u s i n g ? \u2013I s t h e l o c a t i o n o f t h e torch library used by Jupyter the same as the one you imported from the interactive prompt? Summary Deep learning models auto matically learn to associate inputs and desired out- puts from examples. Libraries like PyTorch allow you to bu ild and train neural network models efficiently. PyTorch minimizes cognitive overhead while focusing on flexibility and speed. It also defaults to immediate execution for operations. TorchScript is a precompiled deferred-execution mode that can be invoked from C++. Since the release of PyTorch in early 201 7, the deep learning tooling ecosystem has consolidated significantly. PyTorch provides several utility librarie s to facilitate deep learning projects. 7https://forums.manning.com/forums/deep-learning-with-pytorch 8https://github.com/deep-learn ing-with-pytorch/dlwpt-code 15 It starts with a tensor Deep learning enables many applications, wh ich invariably consist of taking data in some form, such as images or text, and producing data in another form, such as labels, numbers, or more text. Taken from t h i s a n g l e , d e e p l e a r n i n g c o n s i s t s o f building a system that can transform data from one representation to another. This transformation is driven by extracting co mmonalities from a series of examples that demonstrate the desired mapping. The syst em might note the general shape of a dog and the typical colors of a golden retr iever, for example. By combining the two image properties, the system can correctly map images with a given shape and color to the golden-retriever label instead of a black lab (or a tawny tomcat, for that mat - ter). The resulting system can consume br oad swaths of similar inputs and produce meaningful output for those inputs. The first step of this process is conver ting the input into floating-point numbers, as you see in the first step of figure 2.1 (along with many other types of data). This chapter covers Tensors, the basic data structure in PyTorch Indexing and operating on PyTorch tensors to explore and manipulate data Interoperating with NumPy multidimensional arrays Moving computations to the GPU for speed 16 CHAPTER 2 It starts with a tensor Because a network uses floating-point number s to deal with information, you need a way to encode real-world data of the kind you want to process into something that\u2019s digestible by a network and then decode th e output back to something you can under - stand and use for a purpose. The transformation from one form of data to another is typically learned by a deep neural network in stages, which means that you can think of the partially transformed data between stages as being a sequence of intermediate representations. For image rec - ognition, early representations can be things (like edge detection) or textures (like fur). Deeper representations can capture more-compl ex structures (like ears, noses, or eyes). I n g e n e r a l , s u c h i n t e r m e d i a t e r e p r e s e ntations are colle ctions of floating-point numbers that characterize th e input and capture the struct ure in the data, in a way that\u2019s instrumental for descri bing how inputs are mapped to the outputs of the neural network. Such characterization is specific to the task at hand and is learned from rele - vant examples. These collections of floa ting-point numbers and their manipulation are at the heart of modern AI . It\u2019s important to keep in mind that these intermediate representations (such as the ones shown in the second step of figure 2.1) are the results of combining the input with the weig hts of the previous layer of neurons. Each intermediate representation is unique to the inputs that preceded it. Figure 2.1 A deep neural network learns how to transform an i nput representation to an output representation. (Note: The number of neurons and outputs is not to scale.) Before you can begin the process of conver ting data to floating-point input, you must have a solid understanding of how PyTo rch handles and stores data: as input, as 17 intermediate representations, and as output. This chapter is devoted to providing pre - cisely that understanding. T o t h i s e n d , P y T o r c h i n t r o d u c e s a f u n damental data structure: the tensor. For those who come from mathematics, ph ysics, or engineering, the term tensor c o m e s bundled with the notion of spaces, refe rence systems, and transformations between them. For everyone else, tensor refers to the generalization of vectors and matrices to an arbitrary number of dimensions, as sh own in figure 2.2. Another name for the same concept is multidimensional arrays. The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor. Figure 2.2 Tensors are the building blocks for representing data in PyTorch PyTorch isn\u2019t not the only library that deals with multidimen sional arrays. NumPy is by far the most popular mu ltidimensional-array library, to the point that it has argu - ably become the lingua f r a n c a o f d a t a s c i e n c e . I n f a c t , P y T o r c h f e a t u r e s s e a m l e s s interoperability with NumPy, which brings with it first-class integration with the rest of the scientific libraries in Python, such as SciPy 1, Scikit-learn2, and Pandas3. Compared with NumPy arrays, PyTorch te nsors have a few superpowers, such as the ability to perform fast operations on gr aphical processing units (GPUs), to distrib - ute operations on multiple devices or ma chines, and to keep track o f the graph of computations that created them. All these features are important in implementing a modern deep learning library. W e s t a r t t h e c h a p t e r b y i n t r o d u c i n g P y T orch tensors, covering the basics to set things in motion. We show you how to mani pulate tensors by using the PyTorch tensor library, covering things such as how the data is stored in memory and how certain operations can be performed on arbitraril y large tensors in constant time; then we move on to the aforementioned NumPy interoperability and the GPU acceleration. Understanding the capabilities and API of te nsors is important if they\u2019r e to be go- to tools in your programming toolbox. 1https://www.scipy.org 2https://scikit-learn.org/stable 3https://pandas.pydata.org 18 CHAPTER 2 It starts with a tensor 2.1 Tensor fundamentals You\u2019ve already learned that tensors are th e fundamental data structures in PyTorch. A tensor is an array\u2014that is, a data struct ure storing collection of numbers that are accessible individually by means of an in dex and that can be indexed with multiple indices. T a k e a l o o k a t list i n d e x i n g i n a c t i o n s o t h a t y o u c a n c o m p a r e i t w i t h t e n s o r indexing. The following listing shows a list of three numbers in Python. # In[1]: a = [1.0, 2.0, 1.0] You can access the first element of the list by using the corresponding 0-based index: # In[2]: a[0] # Out[2]: 1.0 # In[3]: a[2] = 3.0 a # Out[3]: [1.0, 2.0, 3.0] It\u2019s not unusual for simple Python programs that deal with vectors of numbers, such as the coordinates of a 2D line, to use Python lists to store the vect or. This practice can be suboptimal, however, for several reasons: Numbers in Python are full-fledged objects. Whereas a floating-point number might take only 32 bits to be represented on a computer, Python boxes them in a full- fledged Python object with reference counting and so on. This situation isn\u2019t a problem if you need to store a small nu mber of numbers, but allocating mil- lions of such numbers gets inefficient. Lists in Python are meant for sequential collections of objects. No operations are defined for, say, efficiently taking th e dot product of two vectors or summing vectors. Also, Python lists have no way of optimizing the layout of their content in memory, as they\u2019re indexable collectio ns of pointers to Python objects (of any kind, not numbers alone). Finally, Python lists are one-dimensional, and although you can create li sts of lists, again, this practice is inefficient. The Python interpreter is slow compared with optimized, compiled code . Performing mathematical operations on l a r g e c o l l e c t i o n s o f n umerical data can be must faster using optimized code written in a compiled, low-level language like C. For these reasons, data science libraries rely on NumPy or introduce dedicated data structures such as PyTorch tensors that pr ovide efficient low-level implementations of Listing 2.1 code/p1ch3/1_tensors.ipynb 19Tensor fundamentals numerical data structures and related oper ations on them, wrapped in a convenient high-level API. Many types of data\u2014from images to time series, audio, and even sentences\u2014can be represented by tensors. By defining op erations over tensors, some of which you explore in this chapter, you can slice and ma nipulate data expressively and efficiently at the same time, even from a high-level (and not particularly fast) language such as Python. Now you\u2019re ready to construct your first PyTorch tensor to see what it looks like. This tensor won\u2019t be particularly meaningf ul for now, being three ones in a column: # In[4]: import torch a = torch.ones(3) a # Out[4]: tensor([1., 1., 1.]) # In[5]: a[1] # Out[5]: tensor(1.) # In[6]: float(a[1]) # Out[6]: 1.0 # In[7]: a[2] = 2.0 a # Out[7]: tensor([1., 1., 2.]) Now take a look at what you did here. After importing the torch module, you called a function that creates a (one-d imensional) tensor of size 3 filled with the value 1.0. You can access an element by using its 0-based index or assign a new value to it. Although on the surface, this example doesn\u2019t differ much from a list of number objects, under the hood, things are completely different. Python lists or tuples of num - bers are collections of Python objects th at are individually allocated in memory, as shown on the left side of figure 2.3. PyTorch tensors or NumPy arrays, on the other hand, are views over (typically) contiguo us memory blocks containing unboxed C numeric types, not Python object s. In this case, 32 bits (4 bytes) float, as you see on the right side of figure 2.3. So a 1D tensor of 1 million float numbers requires 4 million contiguous bytes to be stored, plus a sma ll overhead for the metadata (dimensions, numeric type, and so on). Figure 2.3 Python object (boxed) numeric values vers us tensor (unboxed array) numeric values 20 CHAPTER 2 It starts with a tensor Suppose that you have a list of 2D coordina tes that you\u2019d like to manage to represent a geometrical object, such as a triangle. The example isn\u2019t particularly pertinent to deep learning, but it\u2019s easy to follow. Instead of having coordinates as numbers in a Python list, you can use a one-dimensional tensor by storing xs in the even indices and ys in the odd indices, like so: # In[8]: points = torch.zeros(6) points[0] = 1.0 points[1] = 4.0 points[2] = 2.0 points[3] = 1.0 points[4] = 3.0 points[5] = 5.0 You can also pass a Python list to the constructor to the same effect # In[9]: points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0]) points # Out[9]: tensor([1., 4., 2., 1., 3., 5.]) to get the coordinates of the first point: # In[10]: float(points[0]), float(points[1]) # Out[10]: (1.0, 4.0) This technique is OK, although it would be practical to have the first index refer to individual 2D points rather than point coordinates. For this purpose, you can use a 2D tensor: The use of .zeros here is a way to get an appropriately sized array. Overwrite those zeros with the values you want. 21Tensor fundamentals # In[11]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points # Out[11]: tensor([[1., 4.], [2., 1.], [3., 5.]]) Here, you passed a list of lists to the constructor. You can ask the tensor about its shape, # In[12]: points.shape # Out[12]: torch.Size([3, 2]) which informs you of the size of the tensor along each dimension. You could also use zeros or ones to initialize the tensor, providing the size as a tuple: # In[13]: points = torch.zeros(3, 2) points # Out[13]: tensor([[0., 0.], [0., 0.], [0., 0.]]) Now you can access an individual elemen t in the tensor by using two indices: # In[14]: points = torch.FloatTensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points # Out[14]: tensor([[1., 4.], [2., 1.], [3., 5.]]) # In[15]: points[0, 1] # Out[15]: tensor(4.) This code returns the y c o o r d i n a t e o f t h e 0th point in your data set. You can also access the first element in the tensor as you did before to get the 2D coordinates of the first point: # In[16]: points[0] # Out[16]: tensor([1., 4.]) 22 CHAPTER 2 It starts with a tensor Note that what you get as the output is another tensor, but a 1D tensor of size 2 contain - ing the values in the first row of the points tensor. Does this output mean that a new chunk of memory was allocated, values were copied into it, and the new memory was returned wrapped in a new tensor object? No, because that process would be ineffi - cient, especially if you had millions of po ints. What you got back instead was a differ - ent view of the same underlying data, limited to the first row. 2.2 Tensors and storages In this section, you start getting hints a bout the implementation under the hood. Val- ues are allocated in contiguous chunks of memory, managed by torch.Storage instances. A storage is a one-dimensional array of nu merical data, such as a contiguous block of memory containing numb ers of a given type, perhaps a float o r int32. A PyTorch Tensor is a view over such a Storage that\u2019s capable of indexing into that storage by using an offset and per-dimension strides. Multiple tensors can index the same storag e even if they index into the data differ - ently. You can see an example in figu re 2.4. In fact, when you requested points[0] in the last snippet, what you got back was another tensor that indexes the same storage as the points tensor, but not all of it and with di fferent dimensionality (1D versus 2D). The underlying memory is allocated only once, however, so creating alternative tensor views on the data can be done quickly, regardless of the size of the data managed by the Storage instance. Figure 2.4 Tensors are views over a Storage instance Next, you see how indexing into the storage works in practice with 2D points. You can access the storage for a given tensor by using the .storage property: 23Tensors and storages # In[17]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points.storage() # Out[17]: 1.0 4.0 2.0 1.0 3.0 5.0 [torch.FloatStorage of size 6] Even though the tensor reports itself as having three rows and two columns, the stor - age under the hood is a contiguous array of size 6. In this sense, the tensor knows how to translate a pair of indices into a location in the storage. You can also index into a storage manually: # In[18]: points_storage = points.storage() points_storage[0] # Out[18]: 1.0 # In[19]: points.storage()[1] # Out[19]: 4.0 You can\u2019t index a storage of a 2D tensor by using two indices. The layout of a storage is always one-dimensional, irrespective of the dimensionality of any tensors that may refer to it. At this point, it shouldn\u2019t come as a su rprise that changing the value of a storage changes the content of its referring tensor: # In[20]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points_storage = points.storage() points_storage[0] = 2.0 points # Out[20]: tensor([[2., 4.], [2., 1.], [3., 5.]]) You\u2019ll seldom, if ever, use storage instance s directly, but understanding the relation - ship between a tensor and th e underlying storage is useful for understanding the cost (or lack thereof) of certain operations late r. This mental model is a good one to keep in mind when you want to write effective PyTorch code. 24 CHAPTER 2 It starts with a tensor 2.3 Size, storage offset, and strides To index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them: si ze, storage offset, and stride (figure 2.5). The size (or shape, in NumPy parlance) is a tuple in dicating how many elements across each dimension the tensor represents. The storage offset is the index in the storage that corresponds to the first el ement in the tensor. The stride is the number of elements in the storage that need to be skipped to obt ain the next element along each dimension. Figure 2.5 Relationship among a tensor\u2019s offset, size, and stride You can get the second point in the tens or by providing the corresponding index: # In[21]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) second_point = points[1] second_point.storage_offset() # Out[21]: 2 # In[22]: second_point.size() # Out[22]: torch.Size([2]) 25Size, storage offset, and strides The resulting tensor has offset 2 i n t h e s t o r a g e ( b e c a u s e w e n e e d t o s k i p t h e f i r s t point, which has two items) and th e size is an instance of the Size class containing one element because the tensor is one-dime nsional. Important note: this information is the same information contained in the shape property of tensor objects: # In[23]: second_point.shape # Out[23]: torch.Size([2]) Last, stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increase d by 1 in each dimension. Your points tensor, for example, has a stride: # In[24]: points.stride() # Out[24]: (2, 1) Accessing an element i, j in a 2D tensor results in accessing the storage_offset + stride[0] * i + stride[1] * j element in the storage. The offset will usually be zero; if this tensor is a view into a storag e created to hold a larger tensor the offset might be a positive value. T h i s i n d i r e c t i o n b e t w e e n Tensor a n d Storage l e a d s s o m e o p e r a t i o n s , s u c h a s transposing a tensor or extracti ng a subtensor, to be inexpensive, as they don\u2019t lead to memory reallocations; instead, they consist of allocating a new tensor object with a dif - ferent value for size, storage offset, or stride. You saw how to extract a subtensor when you indexed a specific point and saw the storage offset increasing. Now see what happens to size and stride: # In[25]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) second_point = points[1] second_point.size() # Out[25]: torch.Size([2]) # In[26]: second_point.storage_offset() # Out[26]: 2 # In[27]: second_point.stride() # Out[27]: (1,) 26 CHAPTER 2 It starts with a tensor Bottom line, the subtensor ha s one fewer dimension (as you\u2019d expect) while still indexing the same storage as the original points tensor. Changing the subtensor has a side effect on the original tensor too: # In[28]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) second_point = points[1] second_point[0] = 10.0 points # Out[28]: tensor([[ 1., 4.], [10., 1.], [ 3., 5.]]) This effect may not always be desirable, so you can eventually clone the subtensor into a new tensor: # In[29]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) second_point = points[1].clone() second_point[0] = 10.0 points # Out[29]: tensor([[1., 4.], [2., 1.], [3., 5.]]) Try transposing now. Take your points tensor, which has individual points in the rows and x and y coordinates in the columns, and turn it around so that individual points are along the columns: # In[30]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points # Out[30]: tensor([[1., 4.], [2., 1.], [3., 5.]]) # In[31]: points_t = points.t() points_t # Out[31]: tensor([[1., 2., 3.], [4., 1., 5.]]) You can easily verify that the two tensors share storage # In[32]: id(points.storage()) == id(points_t.storage()) # Out[32]: True 27Size, storage offset, and strides and that they differ only in shape and stride: # In[33]: points.stride() # Out[33]: (2, 1) # In[34]: points_t.stride() # Out[34]: (1, 2) This result tells you that increasing the first index by 1 in points\u2014that is, going from points[0,0] t o points[1,0]\u2014skips along the storage by two elements, and that increasing the second index from points[0,0] to points[0,1] skips along the stor - age by one. In other words, the storage ho lds the elements in the tensor sequentially row by row. You can transpose points into points_t as shown in figure 2.6. You change the order of the elements in the stride. After that, increasing the row (the first index of the tensor) skips along the storage by 1, as when you were moving along columns in points. This is the definition of transposing. No new memory is allocated: transpos - ing is obtained only by creating a new Tensor instance with different stride ordering from the original. Figure 2.6 Transpose operation applied to a tensor 28 CHAPTER 2 It starts with a tensor Transposing in PyTorch isn\u2019t limited to matrices. You can transpose a multidimen - sional array by specifying the two dimensions along which transposing (such as flip - ping shape and stride) should occur: # In[35]: some_tensor = torch.ones(3, 4, 5) some_tensor_t = some_tensor.transpose(0, 2) some_tensor.shape # Out[35]: torch.Size([3, 4, 5]) # In[36]: some_tensor_t.shape # Out[36]: torch.Size([5, 4, 3]) # In[37]: some_tensor.stride() # Out[37]: (20, 5, 1) # In[38]: some_tensor_t.stride() # Out[38]: (1, 5, 20) A tensor whose values are laid out in the storage starting from the rightmost dimen - sion onward (moving along rows for a 2D tensor, for example) is defined as being con- tiguous. Contiguous tensors are convenient because you can visit them efficiently and in order without jumping around in the st orage. (Improving data locality improves performance because of the way memo ry access works in modern CPUs.) In this case, points is contiguous but its transpose is not: # In[39]: points.is_contiguous() # Out[39]: True # In[40]: points_t.is_contiguous() # Out[40]: False You can obtain a new contiguous tensor from a noncontiguous one by using the con- tiguous method. The content of the tensor stay s the same, but the stride changes, as does the storage: 29Size, storage offset, and strides # In[41]: points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]]) points_t = points.t() points_t # Out[41]: tensor([[1., 2., 3.], [4., 1., 5.]]) # In[42]: points_t.storage() # Out[42]: 1.0 4.0 2.0 1.0 3.0 5.0 [torch.FloatStorage of size 6] # In[43]: points_t.stride() # Out[43]: (1, 2) # In[44]: points_t_cont = points_t.contiguous() points_t_cont # Out[44]: tensor([[1., 2., 3.], [4., 1., 5.]]) # In[45]: points_t_cont.stride() # Out[45]: (3, 1) # In[46]: points_t_cont.storage() # Out[46]: 1.0 2.0 3.0 4.0 1.0 5.0 [torch.FloatStorage of size 6] Notice that the storage has been reshuffled for elements to be laid out row by row in the new storage. The stride has been changed to reflect the new layout. 30 CHAPTER 2 It starts with a tensor 2.4 Numeric types All right, you know the basics of how te nsors work. But we haven\u2019t touched on the numeric types you can store in a Tensor. The dtype argument to tensor constructors (that is, functions such as tensor, zeros, and ones) specifies the numerical data type that will be contained in the tensor. The data type specifies the possible values that the tensor can hold (integers versus floating-p oint numbers) and the number of bytes per value.4 The dtype argument is deliberately simila r to the standard NumPy argument of the same name. Here\u2019s a list of the possible values for the dtype argument: torch.float32 or torch.float\u201432-bit floating-point torch.float64 or torch.double\u201464-bit, double-precision floating-point torch.float16 or torch.half\u201416-bit, half-precision floating-point torch.int8\u2014Signed 8-bit integers torch.uint8\u2014Unsigned 8-bit integers torch.int16 or torch.short\u2014Signed 16-bit integers torch.int32 or torch.int\u2014Signed 32-bit integers torch.int64 or torch.long\u2014Signed 64-bit integers Each of torch.float, torch.double, and so on has a corresponding concrete class of torch.FloatTensor, torch.DoubleTensor, and so on. The class for torch.int8 is torch.CharTensor, and the class for torch.uint8 is torch.ByteTensor. torch.Ten- sor is an alias for torch.FloatTensor. The default data type is 32-bit floating-point. To allocate a tensor of the right nu meric type, you can specify the proper dtype as an argument to the constructor, as follows: # In[47]: double_points = torch.ones(10, 2, dtype=torch.double) short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short) You can find out about the dtype for a tensor by accessing the corresponding attri - bute: # In[48]: short_points.dtype # Out[48]: torch.int16 You can also cast the output of a tensor-creat ion function to the right type by using the corresponding casting method, such as # In[49]: double_points = torch.zeros(10, 2).double() short_points = torch.ones(10, 2).short() 4And signedness, in the case of uint8 31NumPy interoperability or the more convenient to method: # In[50]: double_points = torch.zeros(10, 2).to(torch.double) short_points = torch.ones(10, 2).to(dtype=torch.short) Under the hood, type a n d to p e r f o r m t h e s a m e t y p e c h e c k - a n d - c o n v e r t - i f - n e e d e d operation, but the to method can take additional arguments. You can always cast a tensor of one type as a tensor of another type by using the type method: # In[51]: points = torch.randn(10, 2) short_points = points.type(torch.short) 2.5 Indexing tensors You\u2019ve seen that points[0] returns a tensor containing th e 2D point at the first row of the tensor. What if you need to obtain a te nsor that contains all points but the first? That task is easy when you use range indexi ng notation, the same kind that applies to standard Python lists: # In[53]: From element 1 inclusive to element 4 exclusive in steps of 2 some_list = list(range(6)) some_list[:] some_list[1:4] some_list[1:] some_list[:4] some_list[:-1] some_list[1:4:2] To achieve your goal, you can use the same notation for PyTorch tensors, with the added benefit that as in NumPy and in othe r Python scientific libraries, we can use range indexing for each dimension of the tensor: # In[54]: points[1:] points[1:, :] points[1:, 0] All rows after first, first column In addition to using ranges, PyTorch features a powerful form of indexing called advanced indexing. 2.6 NumPy interoperability Although we don\u2019t consider experience in NumPy to be a prerequisite for reading this book, we strongly encourage you to get fami liar with NumPy due to its ubiquity in the Python data science ecosystem. PyTorch tensors can be converted to NumPy arrays and vice versa efficiently. By doing so, you can leverage the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This randn initializes the tensor elements to random numbers between 0 and 1. All elements in the list From element 1 inclusive to element 4 exclusive From element 1 inclusive to the end of the list From the start of the list to element 4 exclusiveFrom the start of the list to one before the last element All rows after first, implicitly all columnsAll rows after first, all columns 32 CHAPTER 2 It starts with a tensor zero-copy interoperabilit y with NumPy arrays is due to the storage system that works with the Python buffer protocol. 5 To get a NumPy array out of your points tensor, call # In[55]: points = torch.ones(3, 4) points_np = points.numpy() points_np # Out[55]: array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32) which returns a NumPy multidimensional ar ray of the right size, shape, and numeri - cal type. Interestingly, the returned array sh ares an underlying buffer with the tensor storage. As a result, the numpy method can be executed effectively at essentially no cost as long as the data sits in CPU RAM, an d modifying the NumPy array leads to a change in the originating tensor. If the tensor is allocated on the GPU, PyTorch makes a copy of the content of the tensor into a NumPy array allocated on the CPU. Conversely, you can obtain a PyTorch tensor from a NumPy array this way # In[56]: points = torch.from_numpy(points_np) which uses the same buffer-sharing strategy. 2.7 Serializing tensors Creating a tensor on the fly is all well and g ood, but if the data inside it is of any value to you, you want to save it to a file and load it back at some point. After all, you don\u2019t want to have to retrain a model from scra tch every time you start running your pro - gram! PyTorch uses pickle under the hood to serialize the tensor object, as well as dedicated serialization code for the storage. Here\u2019s how you can save your points ten- sor to a ourpoints.t file: # In[57]: torch.save(points, '../data/p1ch3/ourpoints.t') As an alternative, you can pass a file descriptor in lieu of the filename: # In[58]: with open('../data/p1ch3/ourpoints.t','wb') as f: torch.save(points, f) Loading your points back is similarly a one-liner: # In[59]: points = torch.load('../data/p1ch3/ourpoints.t') 5https://docs.python.org/3/c-api/buffer.html 33Serializing tensors The equivalent is # In[60]: with open('../data/p1ch3/ourpoints.t','rb') as f: points = torch.load(f) This technique allows you to save tensors qu ickly in case you only want to load them with PyTorch, but the file format itself is n\u2019t interoperable. You can\u2019t read the tensor with software other than PyTorch. Dependin g on the use case, this situation may not be a limitation, but you should learn how to save tensors interoperably for those times when it is. Although every use case is unique, we suspect that this one will be more common when you introduce PyTorch into exis ting systems that already rely on differ - ent libraries. New projects probably won\u2019t n eed to save tensors interoperably as often. F o r t h os e c a s es w h e n y o u n ee d to , h o w e v e r , y o u c a n u s e t h e H DF 5 f or ma t a n d library.6 HDF5 is a portable, widely supported fo rmat for representing serialized multi - dimensional arrays, organized in a nested key-value dictionary. Python supports HDF5 through the h5py library7, which accepts and returns data under the form of NumPy arrays. You can install h5py by using $ conda install h5py At this point, you can save your points tensor by converting it to a NumPy array (at no cost, as noted earlier) and passing it to the create_dataset function: # In[61]: import h5py f = h5py.File('../data/p1ch3/ourpoints.hdf5', 'w') dset = f.create_dataset('coords', data=points.numpy()) f.close() Here, 'coords' is a key into the HDF5 file. You ca n have other keys, even nested ones. One interesting thing in HDF5 is that you can index the data set while on disk and access only the elements you\u2019re interested in . Suppose that you want to load only the last two points in your data set: # In[62]: f = h5py.File('../data/p1ch3/ourpoints.hdf5', 'r') dset = f['coords'] last_points = dset[1:] Here, data wasn\u2019t loaded when the file was opened or the data set was required. Rather, data stayed on disk until you requ ested the second and last rows in the data set. At that point, h5py accessed those two columns an d returned a NumPy array-like object encapsulating that region in that da ta set that behaves like a NumPy array and has the same API. 6https://www.hdfgroup.org/solutions/hdf5 7http://www.h5py.org 34 CHAPTER 2 It starts with a tensor Owing to this fact, you can pass the returned object to the torch.from_numpy func- tion to obtain a tensor directly. Note that in this case, the data is copied over to the tensor\u2019s storage: # In[63]: last_points = torch.from_numpy(dset[1:]) f.close() >>> last_points = torch.from_numpy(dset[1:]) When you finish loading data, close the file. 2.8 Moving tensors to the GPU One last point about PyTorch tensors is related to computing on the GPU. Every Torch tensor can be transferred to a GPUs to perform fast, massively parallel compu - tations. All operations to be performed on the tensor are carried out by GPU-specific routines that come with PyTorch. NOTE As of early 2019, main PyTorch releases have acceleration only on GPUs that have support for CUDA. Proo f-of-concept versions of PyTorch run- ning on AMD\u2019s ROCm 8 platform exist, but full support hasn\u2019t been merged into PyTorch as of version 1.0. Support for Google\u2019s TPUs is a work in prog- ress9, with the current proof of concept available to the public in Google Colab.10 Implementation of data structures and kernels on other GPU tech- nology, such as OpenCL, wasn\u2019t planned at the time we wrote this chapter. In addition to the dtype, a PyTorch tensor has a notion of device, which is where on the computer the tensor data is being plac ed. Here\u2019s how to create a tensor on the GPU by specifying the correspond ing argument to the constructor: # In[64]: points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device='cuda') You could instead copy a tensor created on the CPU to the GPU by using the to method: # In[65]: points_gpu = points.to(device='cuda') This code returns a new tensor that has the same numerical data but is stored in the RAM of the GPU rather than in regular system RAM. Now that the data is stored locally on the GPU, you start to see speedups when per - forming mathematical operations on the tens or. Also, the class of this new GPU-backed tensor changes to torch.cuda.FloatTensor. (Given the starting type of torch.Float- Tensor; the corresponding set of torch.cuda.DoubleTensor a n d s o o n e x i s t s . ) I n almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making 8https://rocm.github.io 9https://github.com/pytorch/xla 10https://colab.research.google.com 35The tensor API it much easier to write code that is agno stic to where the heavy number-crunching pro - cess is running. In case your machine has more than one GPU, you can decide which GPU to allo - cate the tensor to by passing a zero-based integer identifying the GPU on the machine: # In[66]: points_gpu = points.to(device='cuda:0') At this point, any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU: # In[67]: points = 2 * points points_gpu = 2 * points.to(device='cuda') Multiplication performed on the GPU Note that the points_gpu tensor isn\u2019t brought back to the CPU when the result has been computed. Here\u2019s what happened: 1The points tensor was copied to the GPU. 2A new tensor was allocated on the GPU an d used to store the result of the mul- tiplication. 3A handle to that GPU tensor was returned. Therefore, if you also add a constant to the result, # In[68]: points_gpu = points_gpu + 4 the addition is still performed on the GP U, and no information flows to the CPU (except if you print or access the resulting tensor). To move the tensor back to the CPU, you need to provide a cpu argument to the to method: # In[69]: points_cpu = points_gpu.to(device='cpu') You can use the shorthand methods cpu and cuda instead of the to method to achieve the same goal: # In[70]: points_gpu = points.cuda() points_gpu = points.cuda(0) points_cpu = points_gpu.cpu() It\u2019s worth mentioning that when you use the to method, you can change the place - ment and the data type si multaneously by providing device and dtype as arguments. 2.9 The tensor API At this point, you know what PyTorch tensors are and how they work under the hood. Before we wrap up this chapter, we\u2019ll take a look at the tensor operations that PyTorch offers. It would be of little use to list all of them all here. Instead, we\u2019re going to give you a general feel for the API and show yo u where to find things in the online docu - mentation at http://pytorch.org/docs . Multiplication performed on the CPU Defaults to GPU index 0 36 CHAPTER 2 It starts with a tensor First, the vast majority of operations on and between tensors are available under the torch module and can also be called as methods of a tensor object. The trans- pose function that you encountered earlier, for example, can be used from the torch module # In[71]: a = torch.ones(3, 2) a_t = torch.transpose(a, 0, 1) or as a method of the a tensor: # In[72]: a = torch.ones(3, 2) a_t = a.transpose(0, 1) No difference exists between the two for ms, which can be used interchangeably. A caveat, though: a small number of operations exist only as methods of the tensor object. They\u2019re recognizable by the tra iling underscore in their name, such as zero_, which indicates that the method operates in-place by modifying the input instead of creating a new output tens or and returning it. The zero_ method, for example, zeros out all the elements of the input. Any method without the trailing underscore leaves the source tensor unchanged and returns a new tensor: # In[73]: a = torch.ones(3, 2) # In[74]: a.zero_() a # Out[74]: tensor([[0., 0.], [0., 0.], [0., 0.]]) Earlier, we mentioned the online docs 11, which are exhaustive and well organized with the tensor operations divided into groups: Creation ops\u2014Functions for constructing a tensor, such as ones and from_numpy Indexing, slicing, joining, and mutating ops \u2014Functions for changing the shape, stride, or content of a tensor, such as transpose Math ops\u2014Functions for manipulating the co ntent of the tensor through com- putations: \u2013Pointwise ops\u2014Functions for obtaining a new te nsor by applying a function to each element independently, such as abs and cos \u2013Reduction ops\u2014Functions for computing aggregat e values by iterating through tensors, such as mean, std, and norm 11http://pytorch.org/docs 37The tensor API \u2013Comparison ops\u2014Functions for evaluating nume rical predicates over tensors, such as equal and max \u2013Spectral ops\u2014Functions for transforming in and operating in the frequency domain, such as stft and hamming_window \u2013Other ops\u2014Special functions operatin g on vectors, such as cross, or matrices, such as trace \u2013BLAS and LAPACK ops \u2014Functions that follow the BLAS (Basic Linear Alge- bra Subprograms) specificat ion for scalar, vector-vector, matrix-vector, and matrix-matrix operations Random sampling ops \u2014Functions for generating values by drawing randomly from probability distributions, such as randn and normal Serialization ops\u2014Functions for saving and loading tensors, such as load a n d save Parallelism ops\u2014Functions for controlling the number of threads for parallel CPU execution, such as set_num_threads It\u2019s useful to play with the general tensor API. This chapter should provide all the pre - requisites for this kind of interactive exploration. Exercises Create a tensor a from list(range(9)). Predict then check what the size, off- set, and strides are. Create a tensor b = a.view(3, 3). What is the value of b[1,1]? Create a tensor c = b[1:,1:]. Predict then check what t h e s i z e , o ff s e t , a n d strides are. Pick a mathematical operation like cosine or square root. Can you find a corre- sponding function in the torch library? Is there a version of your function that operates in-place? Summary Neural networks transform floating-point r e p r e s e n t a t i o n s i n t o o t h e r f l o a t i n g - point representations, with the starting a n d e n d i n g r e p r e s e n t a t i o n s t y p i c a l l y being human-interpretable. The interm ediate representations are less so. These floating-point representa tions are stored in tensors. Tensors are multidimensional arrays an d the basic data structure in PyTorch. PyTorch has a comprehensive standard li brary for tensor creation and manipu- lation and for mathematical operations. Tensors can be serialized to disk and loaded back. All tensor operations in PyTorch can execute on the CPU as well as on the GPU with no change in the code. PyTorch uses a trailing underscore to in dicate that a function operates in-place on a tensor (such as Tensor.sqrt_). 39 Real-world data representation with tensors Tensors are the building blocks for data in PyTorch. Neural networks take tensors in input and produce tensors as outputs. In fact, all operations within a neural net - work and during optimizati on are operations between tensors, and all parameters (such as weights and biases) in a neural network are tensors. Having a good sense of how to perform operations on tensors and index them effectively is central to using tools like PyTorch succe ssfully. Now that you know th e basics of tensors, your dexterity with them will grow. This chapter covers Representing different types of real-world data as PyTorch tensors Working with range of data types, including spreadsheet, time series, text, image, and medical imaging Loading data from file Converting data to tensors Shaping tensors so that they can be used as inputs for neural network models 40 CHAPTER 3 Real-world data representation with tensors We can address one question at this point: how do you take a piece of data, a video, or text, and represent it with a tensor, and do that in a way that\u2019s appropriate for train - ing a deep learning model? The answer is what you\u2019ll learn in this chapter. We cover different types of data and show you how to get them represented as te nsors. Then we show you how to load the data from the most common on -disk formats and also get a feeling for those data types structure so that you can see how to prep are them for training a neural network. Often, your raw data won\u2019t be perfectly formed for the problem you\u2019d like to solve, so you\u2019ll have a chance to prac tice your tensor manipulation skills on a few more inter - esting tensor operations. You\u2019ll be using a lot of image and volumetric data because those data types are common and reproduce well in book format. We also cover tabu - lar data, time series, and text, which ar e also of interest to many readers. Each section of the chapter describes a da ta type, and each comes with its own data set. Although we\u2019ve structured the chapter so that each data type builds on the pre - ceding one, you should feel free to sk ip around a bit if you\u2019re so inclined. We start with tabular data of data about wines, as you\u2019d find in a spreadsheet. Next, we move to ordered tabular data, with a time-series data set from a bike-sharing pro - gram. After that, we show you how to work with text data from Jane Austen. Text data retains the ordered aspect bu t introduces the problem of representing words as arrays of numbers. Because a picture is worth a thousand words, we demonstrate how to work with image data. Finally, we dip into medical data with a 3D array that represents a volume containing patient anatomy. I n e v e r y s e c t i o n , w e s t o p w h e r e a d e e p l e a r n i n g r e s e a r c h e r w o u l d s t a r t : r i g h t before feeding the data to a model. We encourage you to keep these data sets around. They\u2019ll constitute excellent material when you start learning how to train neural net - work models. 3.1 Tabular data The simplest form of data you\u2019ll encounter in your machine learning job is sitting in a spreadsheet, in a CSV (comma-separated values) file, or in a database. Whatever the medium, this data is a table containing one row per sample (or record), in which col - umns contain one piece of information about the sample. At first, assume that there\u2019s no meanin g in the order in which samples appear in the table. Such a table is a collection of independent samples, unlike a time-series, in which samples are relate d by a time dimension. Columns may contain numerical values, such as temperatures at specific locations, or labels, such as a string expressi ng an attribute of the sample (like \"blue\"). There- fore, tabular data typically isn\u2019t homogene ous; different columns don\u2019t have the same type. You might have a column showing th e weight of apples and another encoding their color in a label. PyTorch tensors, on the other hand, ar e homogeneous. Other data science pack- ages, such as Pandas, have the concept of the data frame, an object representing a data set with named, heterogenous columns. By contrast, information in PyTorch is encoded 41Tabular data as a number, typically floating-point (tho ugh integer types are supported as well). Numeric encoding is deliberate, because neur al networks are mathematical entities that take real numbers as inputs and produce real numbers as output through successive application of matrix multiplications and nonlinear functions. Your first job as a deep learning practitioner, therefore, is to encode heterogenous, real-world data in a tensor of floating-poi nt numbers, ready for consumption by a neu - ral network. A l a r g e n u m b e r o f t a b u l a r d a t a s e t s is freely available on the internet. See https://github.com/caesar030 1/awesome-public-data sets, for example. We start with something fun: wine. The Wi ne Quality data set is a freely available table containing chemical characterizations of samples of vinho verde ( a w i n e f r o m northern Portugal) together with a sensory qu ality score. You can download the data set for white wines at https://archive.ics.uci.edu/ml/machine-learning-databases/wine- quality/winequality-white.csv . For convenience, we created a copy of the data set on the Deep Learning with PyTorch Git repository, under data/p1ch4/tabular-wine. The file contains a comma-separated coll ection of values organized in 12 columns preceded by a header line containing th e column names. The first 11 columns con - tain values of chemical variables; the last column contains the sensory quality score from 0 (worst) to 10 (excellent). Follow ing are the column names in the order in which they appear in the data set: fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality A possible machine learning task on this da ta set is predicting the quality score from chemical characterization al one. Don\u2019t worry, though\u2014mac hine learning isn\u2019t going to kill wine tasting anytime soon. We have to get the training data from somewhere! As shown in figure 3.1, you hope to find a relationship between one of the chemi - cal columns in your data and the quality column. Here, you\u2019re expecting to see quality increase as sulfur decreases. Before you can get to that observation, however, you need to be able to examine the data in a more usable way than opening the file in a text editor. We\u2019ll show you how to load the data by using Python and then turn it into a PyTorch tensor. Python offers several options for loading a CSV file quickly. Three popular options are The csv module that ships with Python NumPy Pandas Figure 3.1 The relationship between sulfur and quality in wine 42 CHAPTER 3 Real-world data representation with tensors The third option is the most time- and me mory-efficient, but we \u2019ll avoid introducing an additional library into your learning trajectory merely to load a file. Because we\u2019ve already introduced NumPy and PyTorch has excellen t NumPy interoperability, you\u2019ll go with it. Load your file and turn the re sulting NumPy array into a PyTorch tensor, as shown in the following listing. # In[2]: import csv wine_path = \"../data/p1ch4/tabular-wine/winequality-white.csv\" wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1) wineq_numpy # Out[2]: array([[ 7. , 0.27, 0.36, ..., 0.45, 8.8 , 6. ], [ 6.3 , 0.3 , 0.34, ..., 0.49, 9.5 , 6. ], [ 8.1 , 0.28, 0.4 , ..., 0.44, 10.1 , 6. ], ..., [ 6.5 , 0.24, 0.19, ..., 0.46, 9.4 , 6. ], [ 5.5 , 0.29, 0.3 , ..., 0.38, 12.8 , 7. ], [ 6. , 0.21, 0.38, ..., 0.32, 11.8 , 6. ]], dtype=float32) Here, you prescribed the type of the 2D a rray (32-bit floating-point) and the delimiter used to separate values in each row, and stated that the first line shouldn\u2019t be read because it contains the column names. Next , check that all the data has been read, Listing 3.1 code/p1ch4/1_tabular_wine.ipynb 43Tabular data # In[3]: col_list = next(csv.reader(open(wine_path), delimiter=';')) wineq_numpy.shape, col_list # Out[3]: ((4898, 12), ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']) and proceed to convert the NumP y array to a PyTorch tensor: # In[4]: wineq = torch.from_numpy(wineq_numpy) wineq.shape, wineq.type() # Out[4]: (torch.Size([4898, 12]), 'torch.FloatTensor') At this point, you have a torch.FloatTensor containing all columns, including the last, which refers to the quality score. Interval, ordinal, and categorical values You should be aware of three kinds of numerical values as you attempt to make sense of your data. The first kind is continuous values. These values are the most intuitive when repre- sented as numbers; they\u2019re strictly ordered, and a difference between various values has a strict meaning. Stating that package A is 2 kilograms heavier than package B or that package B came from 100 miles farther away than package A has a fixed meaning, no matter whether package A weighs 3 kilograms or 10, or whether B came from 200 miles away or 2,000. If you\u2019re counting or measuring something with units, the value probably is a continuous value. Next are ordinal values. The strict ordering of continuous values remains, but the fixed relationship between values no longer applies. A good example is ordering a small, medium, or large drink, with small mapped to the value 1, medium to 2, and large to 3. The large drink is bigger than the medium, in the same way that 3 is bigger than 2, but it doesn\u2019t tell you anything about how much bigger. If you were to convert 1, 2, and 3 to the actual volumes (say, 8, 12, and 24 fluid ounces), those values would switch to inter- val values. It\u2019s important to remember that you can\u2019t do math on the values beyond order- ing them; trying to average large=3 and small=1 does not result in a medium drink! 44 CHAPTER 3 Real-world data representation with tensors You could treat the score as a continuous va riable, keep it as a real number, and per - form a regression task, or treat it as a labe l and try to guess such label from the chemi- cal analysis in a classification task. In bo th methods, you typically remove the score from the tensor of input data and keep it in a separate tensor, so that you can use the score as the ground truth without it being input to your model: # In[5]: data = wineq[:, :-1] data, data.shape # Out[5]: (tensor([[ 7.0000, 0.2700, ..., 0.4500, 8.8000], [ 6.3000, 0.3000, ..., 0.4900, 9.5000], ..., [ 5.5000, 0.2900, ..., 0.3800, 12.8000], [ 6.0000, 0.2100, ..., 0.3200, 11.8000]]), torch.Size([4898, 11])) # In[6]: target = wineq[:, -1] target, target.shape # Out[6]: (tensor([6., 6., ..., 7., 6.]), torch.Size([4898])) If you want to transform the target tensor in a tensor of labels, you have two options, depending on the strategy or how you want to use the categorical data. One option is to treat a label as an integer vector of scores: # In[7]: target = wineq[:, -1].long() target # Out[7]: tensor([6, 6, ..., 7, 6]) If targets were string labels (such as wine color), assigning an integer number to each string would allow you to follow the same approach. The other approach is to build a one-hot encoding of the scores\u2014that is, encode each of the ten scores in a vector of ten elements, with all elements set to zero but one, at a different index for each score. This way, a score of 1 could be mapped to the vector (1,0,0,0,0,0,0,0,0,0), a score of 5 to (0,0,0,0,1,0,0,0,0,0) and so on. (continued) Finally, categorical values have neither ordering nor numerical meaning. These values are often enumerations of possibilities, a ssigned arbitrary numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example. Placing water first and milk last has no real logic; you simply need distinct values to differentiate them. You could assign coffee to 10 and milk to \u20133 with no significant change (although assigning values in the range 0..N-1 will have advantages when we di scuss one-hot encoding later). Select all rows and all columns except the last. Select all rows and the last column. 45Tabular data The fact that the score corresponds to the index of the nonzero element is purely inci - dental; you could shuffle the assignment, an d nothing would change from a classifica- tion standpoint. The two approaches have marked differen ces. Keeping wine-quali ty scores in an integer vector of scores induces an orderi ng of the scores, which may be appropriate in this case because a score of 1 is lower than a score of 4. It also induces some dis - tance between scores. (T he distance between 1 a n d 3 i s t h e s a m e a s t h e d i s t a n c e between 2 and 4, for example.) If this holds for your quantity, great. If, on the other hand, scores are purely qualitative, such as color, one-hot encoding is a much better fit, as no implied ordering or distance is involved. One-hot encoding is appropriate for quantitative scores when fractional va lues between integer scores (such as 2.4) make no sense for the application (w hen score is either this or that). You can achieve one-hot encoding by using the scatter_ method, which fills the tensor with values from a source tensor along the indices provided as arguments. # In[8]: target_onehot = torch.zeros(target.shape[0], 10) target_onehot.scatter_(1, target.unsqueeze(1), 1.0) # Out[8]: tensor([[0., 0., ..., 0., 0.], [0., 0., ..., 0., 0.], ..., [0., 0., ..., 0., 0.], [0., 0., ..., 0., 0.]]) Now take a look at what scatter_ does. First, notice that its name ends with an under - score. This convention in PyTorch indicate s that the method won\u2019t return a new ten - sor but modify the tensor in place. The arguments for scatter_ are The dimension along which the foll owing two arguments are specified A column tensor indicating the indices of the elements to scatter A tensor containing the elements to sc atter or a single scalar to scatter ( 1, in this case) In other words, the preceding invocation reads this way: \u201cFor each row, take the index of the target label (which coincides with the sc ore in this case), and use it as the column index to set the value 1.0. The result is a tensor encoding categorical information.\u201d The second argument of scatter_, the index tensor, is required to have the same number of dimensions as the te nsor you scatter into. Because target_onehot has two dimensions (4898x10), you need to add an extra dummy dimension to target b y using unsqueeze: # In[9]: target_unsqueezed = target.unsqueeze(1) target_unsqueezed # Out[9]: 46 CHAPTER 3 Real-world data representation with tensors tensor([[6], [6], ..., [7], [6]]) The call to unsqueeze adds a singleton dimension, from a 1D tensor of 4898 elements to a 2D tensor of size (4898x1), withou t changing its contents. No elements were added; you decided to use an extra index to access the elements. That is, you accessed the first element of target as target[0] and the first element of its unsqueezed coun - terpart as target_unsqueezed[0,0]. PyTorch allows you to use class indices directly as targets while training neural net - works. If you want to use the score as a categorical input to the network, however, you\u2019d have to transform it to a one-hot encoded tensor. Now go back to your data tensor, containing the 11 variables associated with the chemical analysis. You can use the functions in the PyTorch Tensor API to manipulate your data in tensor form. First, obtain me ans and standard deviations for each column: # In[10]: data_mean = torch.mean(data, dim=0) data_mean # Out[10]: tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01, 1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01]) # In[11]: data_var = torch.var(data, dim=0) data_var # Out[11]: tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02, 1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00]) In this case, dim=0 indicates that the reduction is performed along dimension 0. At this point, you can normalize the data by subtracting the mean and dividing by the standard deviation, which help s with the learning process. # In[12]: data_normalized = (data - data_mean) / torch.sqrt(data_var) data_normalized # Out[12]: tensor([[ 1.7209e-01, -8.1764e-02, ..., -3.4914e-01, -1.3930e+00], [-6.5743e-01, 2.1587e-01, ..., 1.3467e-03, -8.2418e-01], ..., [-1.6054e+00, 1.1666e-01, ..., -9.6250e-01, 1.8574e+00], [-1.0129e+00, -6.7703e-01, ..., -1.4882e+00, 1.0448e+00]]) Next, look at the data with an eye to find ing an easy way to tell good and bad wines apart at a glance. First, use the torch.le function to determine which rows in target correspond to a score less than or equal to 3: 47Tabular data # In[13]: bad_indexes = torch.le(target, 3) bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum() # Out[13]: (torch.Size([4898]), torch.uint8, tensor(20)) Note that only 20 of the bad_indexes entries are set to 1! By leveraging a feature in PyTorch called advanced indexing, you can use a binary tensor to index the data tensor. This tensor essentially filters data to be only items (or rows) that correspond to 1 in the indexing tensor. The bad_indexes tensor has the same shape as target, with a value of 0 or 1 depending on the outcome of the comparison between your threshold and each element in the original target tensor: # In[14]: bad_data = data[bad_indexes] bad_data.shape # Out[14]: torch.Size([20, 11]) Note that the new bad_data tensor has 20 rows, the same as the number of rows with a 1 in the bad_indexes tensor. It retains all 11 columns. Now you can start to get information about wines grouped into good, middling, and bad categories. Take the .mean() of each column: # In[15]: bad_data = data[torch.le(target, 3)] mid_data = data[torch.gt(target, 3) & torch.lt(target, 7)] good_data = data[torch.ge(target, 7)] bad_mean = torch.mean(bad_data, dim=0) mid_mean = torch.mean(mid_data, dim=0) good_mean = torch.mean(good_data, dim=0) for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)): print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args)) # Out[15]: 0 fixed acidity 7.60 6.89 6.73 1 volatile acidity 0.33 0.28 0.27 2 citric acid 0.34 0.34 0.33 3 residual sugar 6.39 6.71 5.26 4 chlorides 0.05 0.05 0.04 5 free sulfur dioxide 53.33 35.42 34.55 6 total sulfur dioxide 170.60 141.83 125.25 7 density 0.99 0.99 0.99 8 pH 3.19 3.18 3.22 9 sulphates 0.47 0.49 0.50 10 alcohol 10.34 10.26 11.42 It looks as though you\u2019re on to something here. At first glance, the bad wines seem to have higher total sulfur dioxide, among other differences. You could use a threshold on total sulfur dioxide as a crude criterion for discriminating good wines from bad For numpy arrays and PyTorch tensors, the & operator does a logical and operation. 48 CHAPTER 3 Real-world data representation with tensors ones. Now get the indexes in which the tota l sulfur dioxide column is below the mid - point you calculated earlier, like so: # In[16]: total_sulfur_threshold = 141.83 total_sulfur_data = data[:,6] predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold) predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum() # Out[16]: (torch.Size([4898]), torch.uint8, tensor(2727)) Your threshold implies that slightly more than half of the wines are going to be high-quality. Next, you need to get the indexes of the good wines: # In[17]: actual_indexes = torch.gt(target, 5) actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum() # Out[17]: (torch.Size([4898]), torch.uint8, tensor(3258)) Because you have about 500 more good wines than your threshold predicted, you already have hard evidence th at the threshold isn\u2019t perfect. Now you need to see how well your predictions line up with the act ual rankings. Perform a logical and b e t w e e n y o u r p r e d i c t i o n i ndexes and the good indexes (remembering that each index is an array of 0s and 1s), and use that intersection of wines in agreement to determine how well you did: # In[18]: n_matches = torch.sum(actual_indexes & predicted_indexes).item() n_predicted = torch.sum(predicted_indexes).item() n_actual = torch.sum(actual_indexes).item() n_matches, n_matches / n_predicted, n_matches / n_actual # Out[18]: (2018, 0.74000733406674, 0.6193984039287906) You got around 2,000 wines right! Because you had 2,700 wines predicted, a 74 per - cent chance exists that if you predict a wine to be high-quality, it is. Unfortunately, you have 3,200 good wines and identified only 61 percent of them. Well, we guess you got what you signed up for; that resu lt is barely better than random. This example is na\u00efve, of course. You know for sure that multiple variables contrib - ute to wine quality and that the relationships between the values of these variables and the outcome (which could be the actual scor e rather than a binarized version of it) is likely to be more complicated than a simple threshold on a single value. Indeed, a simple neural network would over come all these limitations, as would a lot of other basic machine learning me thods. You\u2019ll have the tools to tackle this problem after completing chapters 5 and 6, in which you bu ild your first neural network from scratch. 49Time series 3.2 Time series In the preceding section, we covered how to represent data organized in a flat table. As we noted, every row in the table was in dependent from the others; their order did not matter. Equivalently, no column encode d information on what rows came before and what rows came after. Going back to the wine data set, you coul d have had a Year column that allowed you to look at how wine quality evolved year over ye ar. (Unfortunately, we don\u2019t have such data at hand, but we\u2019re working hard on collecting the data samples manually, bottle by bottle.) In the meantime, we\u2019ll switch to another interesting data set: data from a Washing - ton, D.C., bike sharing system reporting the hourly count of rental bikes between 2011 and 2012 in the Capital bike-share sy stem with the corresponding weather and seasonal information.1 The goal is to take a flat 2D data set and transform it into a 3D one, as shown in figure 3.2. In the source data, each row is a separa te hour of data (Figure 3.2 shows a trans - posed version of this to bett er fit on the printed page). Figure 3.2 Transforming a 1D multichannel data set into a 2D multichannel data set by separating the date and hour of each sample into separate axes We want to change the row-per- 1https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset 50 CHAPTER 3 Real-world data representation with tensors hour organization so that you have one axis that increases at a rate of one day per index increment and another axis that represents hour of day (independent of the date). The third axis is different columns of da ta (weather, temperature, and so on). Load the data, as shown in the following listing. # In[2]: bikes_numpy = np.loadtxt(\"../data/p1ch4/bike-sharing-data set/hour- fixed.csv\", dtype=np.float32, delimiter=\",\", skiprows=1, converters={1: lambda x: float(x[8:10])}) bikes = torch.from_numpy(bikes_numpy) bikes # Out[2]: tensor([[1.0000e+00, 1.0000e+00, ..., 1.3000e+01, 1.6000e+01], [2.0000e+00, 1.0000e+00, ..., 3.2000e+01, 4.0000e+01], ..., [1.7378e+04, 3.1000e+01, ..., 4.8000e+01, 6.1000e+01], [1.7379e+04, 3.1000e+01, ..., 3.7000e+01, 4.9000e+01]]) For every hour, the data set reports the following variables: instant # index of record day # day of month season # season (1: spring, 2: summer, 3: fall, 4: winter) yr # year (0: 2011, 1: 2012) mnth # month (1 to 12) hr # hour (0 to 23) holiday # holiday status weekday # day of the week workingday # working day status weathersit # weather situation # (1: clear, 2:mist, 3: light rain/snow, 4: heavy rain/snow) temp # temperature in C atemp # perceived temperature in C hum # humidity windspeed # windspeed casual # number of causal users registered # number of registered users cnt # count of rental bikes In a time-series data set such as this one, r o w s r e p r e s e n t s u c c essive time points: a dimension along which they\u2019r e ordered. Sure, you could treat each row as indepen - dent and try to predict the nu mber of circulating bikes base d on, say, a particular time of day regardless of what happened earlier. This existence of an ordering, however, gives you the opportunity to exploit causal relationships across time. You ca n predict bike rides at one time based on the fact that it was raining at an earlier time, for example. For the time being, you\u2019re going to focus Listing 3.2 code/p1ch4/2_time_series_bikes.ipynb Convert date strings to numbers corresponding to the day of the month in column 1. 51Time series on learning how to turn your bike-sharing data set into something that your neural network can ingest in fixed-size chunks. T h i s n e u r a l n e t w o r k m o d e l needs to see sequences of values for each quantity, such as ride count, time of day, te mperature, and weather conditions, so N parallel sequences of size C. C stands for channel, in neural network parlance, and is the same as column for 1D data like you have here. The N dimension represents the time axis\u2014 here, one entry per hour. You may want to break up the 2-year data set in wider observation periods, such as days. This way, you\u2019ll have N (for number of samples) collections of C sequences of length L. In other words, your time-series data set is a tensor of dimension 3 and shape N x C x L. The C remains your 17 channels, and L would be 24, one per hour of the day. There\u2019s no particular reason why we must use chunks of 24 hours, though the general daily rhythm is likely to give us pattern s we can exploit for predictions. We could instead use 7*24=168 hour blocks to chunk by week instead, if we desired. N o w g o b a c k t o y o u r b i k e - s h a r i n g d a t a s e t . T h e f i r s t c o l u m n i s t h e i n d e x ( t h e global ordering of the data); the second is the date; the sixth is the time of day. You have everything you need to create a data set of daily sequences of ride counts and other exogenous variables. Your data set is already sorted, but if it weren\u2019t, you could use torch.sort on it to order it appropriately. NOTE T h e v e r s i o n o f t h e file you\u2019re using here, hour-fixed.csv, has had some processing done to include rows t h a t w e r e m i s s i n g f r o m t h e o r i g i n a l data set. We presumed that the missing hours had zero bikes active (typically the early-morning hours). All you have to do to obtain your daily hours data set is view the same tensor in batches of 24 hours. Take a look at the shape and strides of your bikes tensor: # In[3]: bikes.shape, bikes.stride() # Out[3]: (torch.Size([17520, 17]), (17, 1)) That\u2019s 17,520 hours, 17 colu mns. Now reshape the data to have three axes (day, hour, and then your 17 columns): # In[4]: daily_bikes = bikes.view(-1, 24, bikes.shape[1]) daily_bikes.shape, daily_bikes.stride() # Out[4]: (torch.Size([730, 24, 17]), (408, 17, 1)) What happened here? First, the bikes.shape[1] is 17, which is the number of columns in the bikes tensor. But the real crux of the code is the call to view, which is important: it changes the way that the tensor looks at the same data as contained in storage. Calling view on a tensor returns a new tensor that changes the number of dimen - sions and the striding inform ation without changing the storage. As a result, you can 52 CHAPTER 3 Real-world data representation with tensors rearrange your tensor at zero cost because no data is copied at all. Your call to view requires you to provide the new shap e for the returned tensor. Use the -1 as a place- holder for \u201chowever many indexes are left, given the other dimensions and the origi - nal number of elements.\u201d Remember that Storage is a contiguous, linear co ntainer for numbers\u2014floating- point, in this case. Your bikes tensor has rows stored one after the other in corre - sponding storage, as confirmed by the output from the call to bikes.stride() earlier. For daily_bikes, stride is telling you that adva ncing by 1 along the hour dimen - sion (the second) requires yo u to advance by 17 places in the storage (or one set of columns), whereas advancing along the day dimension (the first) requires you to advance by a number of elements equal to the length of a row in the storage times 24 (here, 408, which is 17 * 24). The rightmost dimension is the number of columns in the original data set. In the middle dimension, you have time split into chunks of 24 sequential hours. In other words, you now have N sequences of L hours in a day for C channels. To get to your desired NxCxL ordering, you need to transpose the tensor: # In[5]: daily_bikes = daily_bikes.transpose(1, 2) daily_bikes.shape, daily_bikes.stride() # Out[5]: (torch.Size([730, 17, 24]), (408, 1, 17)) We mentioned earlier that the weather-situation variable is ordinal. In fact, it has 4 lev - els: 1 for the best weather and 4 for the worst. You could treat this variable as categori - cal, with levels interpreted as labels, or co ntinuous. If you choose categorical, you turn the variable into a one-hot encoded vector and concatenate the columns with the data set. To make rendering your data easier, li mit yourself to the first day for now. First, initialize a zero-filled matrix with a number of rows equal to the number of hours in the day and a number of columns equal to the number of weather levels: # In[6]: first_day = bikes[:24].long() weather_onehot = torch.zeros(first_day.shape[0], 4) first_day[:,9] # Out[6]: tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2]) Then scatter ones into our matrix accordin g to the corresponding level at each row. Remember the use of unsqueeze to add a singleton dimension earlier: # In[7]: weather_onehot.scatter_( dim=1, index=first_day[:,9].unsqueeze(1) - 1, value=1.0) # Out[7]: You\u2019re decreasing the values by 1 because the weather situation ranges from 1 to 4, whereas indices are 0-based. 53Time series tensor([[1., 0., 0., 0.], [1., 0., 0., 0.], ..., [0., 1., 0., 0.], [0., 1., 0., 0.]]) The day started with weather 1 and ended with 2, so that seems right. L a s t , c o n c a t e n a t e y o u r m a t r i x t o your original data set, using the cat f u n c t i o n . Look at the first of your results: # In[8]: torch.cat((bikes[:24], weather_onehot), 1)[:1] # Out[8]: tensor([[ 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 6.0000, 0.0000, 1.0000, 0.2400, 0.2879, 0.8100, 0.0000, 3.0000, 13.0000, 16.0000, 1.0000, 0.0000, 0.0000, 0.0000]]) Here, you prescribed your original bikes data set and your one-hot encoded weather- situation matrix to be concatenated along the column dimension (such as 1). In other words, the columns of the two data sets are stacked together, or the new one-hot encoded columns are appended to the original data set. For cat to succeed, the ten- sors must have the same size along the ot her dimensions (the row dimension, in this case). Note that your new last four columns are 1, 0, 0, 0\u2014exactly what you\u2019d expect with a weather value of 1. Y o u c o u l d h a v e d o n e t h e s a m e t h i n g w i t h t h e r e s h a p e d daily_bikes t e n s o r . Remember that it\u2019s shaped (B, C, L), where L = 24. First, create the zero tensor, with the same B and L but with the number of additional columns as C: # In[9]: daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2]) daily_weather_onehot.shape # Out[9]: torch.Size([730, 4, 24]) Then scatter the one-hot encodi ng into the tensor in the C dimension. Because opera - tion is performed in place, only the content of the tensor changes: # In[10]: daily_weather_onehot.scatter_(1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0) daily_weather_onehot.shape # Out[10]: torch.Size([730, 4, 24]) Concatenate along the C dimension: # In[11]: daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1) 54 CHAPTER 3 Real-world data representation with tensors We mentioned earlier that this method isn\u2019t the only way to treat the weather-situation variable. Indeed, its labels have an ordi nal relationship, so you could pretend that they\u2019re special values of a continuous va riable. You might transform the variable so that it runs from 0.0 to 1.0: # In[12]: daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0 As we mention in section 4.1, rescaling variables to the [0.0, 1.0] interval or the [-1.0, 1.0] interval is something that you\u2019ll want to do for all quantitative variables, such as temperature (column 10 in your data set). You\u2019ll see why later; for now, we\u2019ll say that it\u2019s beneficial to the training process. You have multiple possibilities for resc aling variables. You can map their range to [0.0, 1.0] # In[13]: temp = daily_bikes[:, 10, :] temp_min = torch.min(temp) temp_max = torch.max(temp) daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - temp_min) / (temp_max - temp_min) or subtract the mean and divide by the standard deviation: # In[14]: temp = daily_bikes[:, 10, :] daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - torch.mean(temp)) / torch.std(temp) In this latter case, the variable has zero mean and unitary standard deviation. If the variable were drawn from a Ga ussian distribution, 68 percent of the samples would sit in the [-1.0, 1.0] interval. Great\u2014you\u2019ve built another nice data set th at you\u2019ll get to use later. For now, it\u2019s important only that you got an idea of how a time series is laid out and how you can wrangle the data into a form that a network will digest. Other kinds of data look like a time series, in that strict ordering exists. The top two in that category are text and audio. 3.3 Text Deep learning has taken the field of natura l language processing (NLP) by storm, par - ticularly by using models that repeatedly consume a combination of new input and previous model output. These models are called recurrent neural networks , and they\u2019ve been applied with great succe ss to text categorization, text generation, and automated translation systems. Previous NLP workloads were characte rized by sophisticated mul - tistage pipelines that included rules encoding the gramm ar of a language. 2 3 N o w , 2Nadkarni et al., \u201cNatural language processing: an introduction\u201d. JAMIA https://www.ncbi.nlm.nih.gov/pmc/arti- cles/PMC3168328 3Wikipedia entry for natural language processing: https://en.wikipedia.org/wik i/Natural-language_processing 55Text state-of-the-art work trains n e t w o r k s e n d t o e n d o n l a r g e c o r p u s e s s t a r t i n g f r o m scratch, letting those rules emerge from data . For the past several years, the most-used automated translation systems available as se rvices on the internet have been based on deep learning. Your goal in this chapter is to turn text into something that a neural network can process, which, like the previous cases, is a tensor of numbers. If you can do that and later choose the right architecture for your te xt processing job, you\u2019ll be in the posi - tion of doing NLP with PyTorch. You see ri ght away how powerful this capability is: you can achieve state-of-the-art performance on tasks in different domains with the same PyTorch tools if you cast your problem in the righ t form. The first part of this job is reshaping data. Networks operate on text at two levels: at cha racter level, by processing one charac- ter at a time, and at word level, in which individual words are t he finest-grained enti - ties seen by the network. The technique you u s e t o e n c o d e t ext information into tensor form is the same whether you operate at character level or at word level. This technique is nothing magic; you stumbled upon it earlier. It\u2019s one-hot encoding. Start with a character-level example. First, get some text to process. An amazing resource is Project Gutenberg 4, a volunteer effort that digitizes and archives cultural work and makes it available for free in op en formats, including plain-text files. If you\u2019re aiming at larger-scale corpora, th e Wikipedia corpus stands out: it\u2019s the complete collection of Wikipedia articles containing 1.9 billion words and more than 4.4 million articles. You can find se veral other corpora at the English Corpora website.5 Load Jane Austen\u2019s Pride and Prejudice from the Project Gutenberg website. 6 Save the file and read it in, as shown in the following listing. # In[2]: with open('../data/p1ch4/jane-austin/1342-0.txt', encoding='utf8') as f: text = f.read() You need to take care of one more detail before you proceed: encoding. Encoding is a vast subject, so all we\u2019ll do now is touch on it. Every written character is represented by a code, a sequence of bits of appropriat e length that allow each character to be uniquely identified. The simplest such en coding is ASCII (American Standard Code for Information Interchange), dating back to the 1960s. ASCII encodes 128 characters using 128 integers. Letter a, for example, corresponds to binary 1100001 or decimal 97; letter b corresponds to binary 1100010 or decimal 98, and so on. The encoding would fit 8 bits, which was a big bonus in 1965. 4http://www.gutenberg.org 5https://www.english-corpora.org 6http://www.gutenberg.org/files/1342/1342-0.txt Listing 3.3 code/p1ch4/3_text_jane_austin.ipynb 56 CHAPTER 3 Real-world data representation with tensors NOTE Clearly, 128 characters aren\u2019t en ough to account for all the glyphs, accents, ligatures, and other features that are needed to properly represent written text in languages other than En glish. To this end, other encodings have been developed, usin g a larger number of bits as a code for a wider range of characters. That wider range of characters got standardized as Uni- code, which maps all known characters to numbers, wi th the representation in bits of those numbers being provided by a specific encoding. Popular encod- ings include UTF-8, UTF-16 and UTF-32, in which the numbers are a sequence of 8-, 16-, or 32-bit integers. String s in Python 3.x are Unicode strings. You\u2019re going to one-hot encode your charac ters to limit the one-hot encoding to a character set that\u2019s useful for the text being analyzed. In this case, because you loaded text in English, it\u2019s quite safe to use AS CII and deal with a small encoding. You could also make all characters lowe rcase to reduce the number of characters in your encod - ing. Similarly, you could screen out punctu ation, numbers, and other characters that aren\u2019t relevant to the expected kinds of text, which may or may not make a practical difference to your neural network, depending on the task at hand. At this point, you need to parse the charac ters in the text and provide a one-hot encod- ing for each of them. Each character will be represented by a vector of length equal to the number of characters in the encoding. This vector will contain all zeros except for a 1 at the index corresponding to the location of the character in the encoding. First, split your text into a list of lines and pick an arbitrary line to focus on: # In[3]: lines = text.split('\\n') line = lines[200] line # Out[3]: '\u201cImpossible, Mr. Bennet, impossible, when I am not acquainted with him' Create a tensor that can hold the total nu mber of one-hot encoded characters for the whole line: # In[4]: letter_tensor = torch.zeros(len(line), 128) letter_tensor.shape # Out[4]: torch.Size([70, 128]) Note that letter_tensor holds a one-hot encoded character per row. Now set a 1 on each row in the right position so that each row represents the right character. The index where the 1 has to be set corresponds to the index of the character in the encoding: # In[5]: for i, letter in enumerate(line.lower().strip()): letter_index = ord(letter) if ord(letter) < 128 else 0 letter_tensor[i][letter_index] = 1 128 hardcoded due to the limits of ASCII\\ The text uses directional double quotes, which aren\u2019t valid ASCII, so screen them out here. 57Text You\u2019ve one-hot encoded your sentence into a representation that a neural network can digest. You could do word-level encodi ng the same way by establishing a vocabu - lary and one-hot encoding sentences, sequen ces of words, along the rows of your ten - sor. Because a vocabulary contains many wo rds, this method produces wide encoded vectors that may not be practical. Later in this chapter, you see a more efficient way to represent text at word level by using embe ddings. For now, stick with one-hot encod - ings to see what happens. Define clean_words, which takes text and returns it lowercase and stripped of punc - tuation. When you call it on your \u201cImpossible, Mr. Bennet\u201d line, you get the following: # In[6]: def clean_words(input_str): punctuation = '.,;:\"!?\u201d\u201c_-' word_list = input_str.lower().replace('\\n',' ').split() word_list = [word.strip(punctuation) for word in word_list] return word_list words_in_line = clean_words(line) line, words_in_line # Out[6]: ('\u201cImpossible, Mr. Bennet, impossible, when I am not acquainted with him', ['impossible', 'mr', 'bennet', 'impossible', 'when', 'i', 'am', 'not', 'acquainted', 'with', 'him']) Next, build a mapping of words to indexes in your encoding: # In[7]: word_list = sorted(set(clean_words(text))) word2index_dict = {word: i for (i, word) in enumerate(word_list)} len(word2index_dict), word2index_dict['impossible'] # Out[7]: (7261, 3394) Note that all_words i s n o w a d i c t i o n a r y w i t h w o r d s as keys and an integer as value. You\u2019ll use this dictionary to efficiently find the index of a word as you one-hot encode it. Now focus on your sentence. Break it into words and one-hot encode it\u2014that is, populate a tensor with one one-hot encoded vector per word. Create an empty vector, and assign the one-hot encoded values of the word in the sentence: # In[8]: word_tensor = torch.zeros(len(words_in_line), len(word2index_dict)) for i, word in enumerate(words_in_line): 58 CHAPTER 3 Real-world data representation with tensors word_index = word2index_dict[word] word_tensor[i][word_index] = 1 print('{:2} {:4} {}'.format(i, word_index, word)) print(word_tensor.shape) # Out[8]: 0 3394 impossible 1 4305 mr 2 813 bennet 3 3394 impossible 4 7078 when 5 3315 i 6 415 am 7 4436 not 8 239 acquainted 9 7148 with 10 3215 him torch.Size([11, 7261]) At this point, tensor represents one sentence of length 11 in an encoding space of size 7261\u2014the number of words in your dictionary. 3.3.1 Text embeddings One-hot encoding is a useful technique for representing categorical data in tensors. As you may have anticipated, however, on e-hot encoding starts to break down when the number of items to encode is effectiv ely unbound, as with words in a corpus. In one book, you had more than 7,000 items! You certainly could do some work to ded uplicate words, condense alternative spell- ings, collapse past and future tenses into a single token, and that kind of thing. Still, a general-purpose English-language encoding is going to be huge. Worse, every time you encounter a new word, you have to add a new column to the vector, which means add - ing a new set of weights to the model to account for that new vocabulary entry, which is going to be painful from a training perspective. How can you compress your encoding to a more manageable size and put a cap on the size growth? Well, instead of usin g vectors of many zeros and a single 1, you could use vectors of floating-point numbers. A vect or of, say, 100 floating-point numbers can indeed represent a large number of words. Th e trick is to find an effective way to map individual words to this 100-dimensional sp ace in a way that facilitates downstream learning. This technique is called embedding. In principle, you could iterate over your vocabulary and generate a set of 100 ran - dom floating-point numbers for each word. This method would work, in that you could cram a large vocabulary into 100 nu mbers, but it would forgo any concept of distance between words based on meaning or context. A model that used this word embedding would have to deal with little st ructure in its input vectors. An ideal solu - tion would be to generate the embedding in such a way that words used in similar con - texts map to nearby regions of the embedding. 59Text If you were to design a solution to this problem by hand , you might decide to build your embedding space by mapping ba sic nouns and adjectives along the axes. You can generate a 2D space in which axes map to nouns \"fruit\" (0.0\u20130.33), \"flower\" (0.33\u20130.66), and \"dog\" (0.66\u20131.0), and to adjectives \"red\" (0.0\u20130.2), \"orange\" (0.2\u20130.4), \"yellow\" (0.4\u20130.6), \"white\" (0.6\u20130.8), and \"brown\" (0.8\u2013 1.0). Your goal now is to take actual fruit, flowers, and dogs and lay them out in the embedding. As you start embedding words, you can map \"apple\" to a number in the \"fruit\" and \"red\" quadrant. Likewise, you can easily map \"tangerine\", \"lemon\", \"lychee\", and \"kiwi\" (to round out your list of colorful fr uits). Then you can start on flowers, assigning \"rose\", \"poppy\", \"daffodil\", \"lily\", and . . . well, there aren\u2019t many brown flowers out there. Well, \"sunflower\" c a n g e t \"flower\", \"yellow\", and \"brown\", and \"daisy\" can get \"flower\" \"white\", and \"yellow\". Perhaps you should update \"kiwi\" to map close to \"fruit\", \"brown\", and \"green\". For dogs and color, you can embed \"redbone\", \"fox\" perhaps for \"orange\", \"golden retriever\", \"poo- dles\" for \"white\", and . . . most kinds of dogs are \"brown\". Although doing this mapping manually is n\u2019t feasible for a large corpus, you should note that although you had an embedding size of 2, you described 15 different words besides the base 8 and probably could cram quite a few more in if you take the time to be creative. As you\u2019ve probably guessed, this kind of work can be automated. By processing a large corpus of organic text, you can genera te embeddings similar to this one. The main differences are that the embedding ve ctor has 100 to 1,000 elements and that axes don\u2019t map directly to concepts, but conceptually similar words map to neigh - boring regions of an embe dding space whose axes ar e arbitrary floating-point dimensions. Although the exact algorithms 7 used are a bit out of scope for what we wanting to focus on here, we\u2019d like to mention that embeddings are often generated by using neural networks, trying to pr edict a word from nearby wo rds (the context) in a sen - tence. In this case, you could start from one-hot encoded words and use a (usually rather shallow) neural ne twork to generate the embedding. When the embedding is available, you could use it for downstream tasks. One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also with consistent spatial relationships with other words. If you were to take the embedding vector for \"apple\" and begin to add and subtract the vectors for othe r words, you could begin to perform analogies such as apple - red - sweet + yellow + sour and end up with a vector similar to the one for \"lemon\". We won\u2019t be using text embeddings here, but they\u2019re essential tools when a large number of entries in a set has to be represented with numeric vectors. 7One example is https://en.wikipedia.org/wiki/Word2vec 60 CHAPTER 3 Real-world data representation with tensors 3.4 Images The introduction of convolutional neural networks revolutionized computer vision 8, and image-based systems have since acquired a new set of capabilities. Problems that required complex pipelines of highly tune d algorithmic building blocks became solv - able at unprecedented levels of performa nce by training end-to-end networks with paired input-and-desired-outpu t examples. To participate in this revolution, you need to be able to load images from common image formats and then transform the data into a tensor representation that has the various parts of the image arranged in the way that PyTorch expects. An image is represented as a collection of s calars arranged in a regular grid, hav - ing a height and a width (in pixels). You migh t have a single scalar per grid point (the pixel), which would be represented as a gray scale image, or multiple scalars per grid point, which typically represent different colors or different features, such as depth from a depth camera. Scalars representing values at individual pixels are often encoded with 8-bit inte - gers, as in consumer cameras, for example. In medical, scientific, and industrial appli - cations, you not infrequently find pixels with higher numerical precision, such as 12- bit and 16-bit. This precision provides a wide r range or increased sensitivity in cases in which the pixel encodes information on a ph ysical property, such a s b o n e d e n s i t y , temperature, or depth. Y o u h a v e s e v e r a l w a y s o f e n c o d i n g n u m b e r s i n t o c o l o r s .9 T h e m o s t c o m m o n i s RGB, which defines a color with three numb ers that represent the intensity of red, green and blue. You can think of a color ch annel as being a grayscale intensity map of only the color in question, similar to what you\u2019d see if you looked at the scene in ques - tion through a pair of pure-red sunglasses. Figure 3.3 shows a rainbow in which each of the RGB channels captures a certain port ion of the spectrum. (The figure is simpli - fied, in that it elides things. The orange and yellow bands, for example, are repre - sented as a combination of red and green.) Images come in several file formats, but luckily, you have plenty of ways to load images in Python. Start by loading a PNG image with the imageio module. You\u2019ll use imageio throughout the chapter because it ha ndles different data types with a uni - form API. Now load an image, as in the following listing. # In[2]: import imageio img_arr = imageio.imread('../data/p1ch4/image-dog/bobby.jpg') img_arr.shape # Out[2]: (720, 1280, 3) 8https://en.wikipedia.org/wiki/Conv olutional_neural_network#History 9Something of an understatement: https://en.wikipedia.org/wiki/Color_model Listing 3.4 code/p1ch4/5_image_dog.ipynb Figure 3.3 A rainbow broken into red, green, and blue channels 61Images At this point, img i s a N u m P y a r r a y - l i k e o b j e c t w i t h t h r e e d i m e n s i o n s : t w o s p a t i a l dimensions (width and height) and a thir d dimension corresponding to the channels red, green, and blue. Any li brary that outputs a NumPy array does so to obtain a PyTorch tensor. The only thing to watch out for is the layout of dimensions. PyTorch modules that deal with image data require tensors to be laid out as C x H x W (chan- nels, height, and width, respectively). You can use the transpose function to get to an appropriate layout. Given an input tensor W x H x C, you get to a proper layout by sw apping the first and last channels: # In[3]: img = torch.from_numpy(img_arr) out = torch.transpose(img, 0, 2) You\u2019ve seen this example before, but note that this operation doesn\u2019t make a copy of the tensor data. Instead, out uses the same underlying storage as img and plays with the size and stride information at the tensor level. This arrangement is convenient because the operation is cheap, but (hea ds up) changing a pixel in img leads to a change in out. Also note that other deep learning frameworks use di fferent layouts. Originally, TensorFlow kept the channel di mension last, resulting in a H x W x C layout. (Now it supports multiple layouts.) This strategy has pros and cons from a low-level perfor - mance standpoint, but it doesn\u2019t make a diffe rence to you as long as you reshape your tensors properly. So far, you\u2019ve described a single image. Following the same strategy that you used for earlier data types, to create a data set of multiple images to use as an input for your neural networks, you st ore the images in a batch along the first dimension to obtain a N x C x H x W tensor. As a more efficient alternative to using stack to build up the tensor, you can preal - locate a tensor of appropriate size and fill it with images loaded from a directory, 62 CHAPTER 3 Real-world data representation with tensors # In[4]: batch_size = 100 batch = torch.zeros(100, 3, 256, 256, dtype=torch.uint8) which indicates that your batch will consis t of 100 RGB images 256 pixels in height and 256 pixels in width. Notice the type of the tensor: you\u2019re expecting each color to be represented as a 8-bit integer, as in mo st photographic format s from standard con- sumer cameras. Now you can load all png images from an input directory and store them in the tensor: # In[5]: import os data_dir = '../data/p1ch4/image-cats/' filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name) == '.png'] for i, filename in enumerate(filenames): img_arr = imageio.imread(filename) batch[i] = torch.transpose(torch.from_numpy(img_arr), 0, 2) As we mentioned earlier, neur al networks usually work wi th floating-point tensors as their input. As you\u2019ll also see in upcoming chapters, neural networks exhibit the best training performance when inpu t data ranges from roughly 0 to 1 or \u20131 to 1 (an effect of how their building blocks are defined). A typical thing that you\u2019ll want to do is cast a tensor to floating-point and normal - ize the values of the pixels. Casting to floati ng-point is easy, but normalization is trick - ier, as it depends on what range of the input you decide should lie between 0 and 1 (or \u20131 and 1). One possibility is to divide the values of pixels by 255 (the maximum representable number in 8-bit unsigned): # In[6]: batch = batch.float() batch /= 255.0 Another possibility is to compute mean and standard deviation of the input data and scale it so that the output has zero mean a n d u n i t s t a n d a r d d e v i a t i o n a c r o s s e a c h channel: # In[7]: n_channels = batch.shape[1] for c in range(n_channels): mean = torch.mean(batch[:, c]) std = torch.std(batch[:, c]) batch[:, c] = (batch[:, c] - mean) / std You can perform several other operations on inputs, including geometric transfor - mations such as rotation, scaling, and cropping. These operations may help with training or may be required to make an arbitrary input conform to the input requirements of a network, such as the size of the image. You\u2019ll stumble onto quite a few of these strategies. For now, just re member that you have image manipulation options available. 63Volumetric data 3.5 Volumetric data You\u2019ve learned how to load and represent 2D images, like the ones you take with your camera. In contexts such as medical imag ing applications involving, say, CT (Com - puted Tomography) scans, you typically deal with sequences of images stacked along the head-to-feet direction, each correspondi ng to a slice across the body. In CT scans, the intensity represents the density of the di fferent parts of the body: lungs, fat, water, muscle, bone, in order of increasing density, mapped from dark to bright when CT scans are displayed on clinical workstations. The density at each point is computed from the amount of x-ray reaching a dete ctor after passing through the body, with some complex math us ed to deconvolve the raw sensor data into the full volume. CTs have a single intensity channel, simi lar to a grayscale image. Often, in native data formats, the channel dimension is left out, so the raw data typically has three dimensions. By stacking individual 2D slices into a 3D tensor, you can build volumetric data representing the 3D anatomy of a subjec t. Unlike figure 3.3, the extra dimension in figure 3.4 represents an offset in physical space rather than a particular band of the visible spectrum. Figure 3.4 Slices of a CT scan, from the top of the head to the jawline We won\u2019t go into detail here on medical imaging data formats. For now, it suffices to say that no fundamental difference exis ts between a tensor that stores volumetric data and one that stores image da ta. You have an extra dimension, depth, after the channel dimension, leading to a 5D tensor of shape N x C x D x H x W. Load a sample CT scan by using the volread function in the imageio module, which takes a directory as argument and assembles all DICOM (Digital Imaging Communica - tion and Storage) files10 in a series in a NumPy 3D array, as shown in the following listing. 10https://wiki.cancerimagingarchiv e.net/display/Public/CPTAC-LSCC#dd4a08a246524596add33b9f8f00f288 64 CHAPTER 3 Real-world data representation with tensors # In[2]: import imageio dir_path = \"../data/p1ch4/volumetric-dicom/2-LUNG 3.0 B70f-04083\" vol_arr = imageio.volread(dir_path, 'DICOM') vol_arr.shape # Out[2]: Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%) Found 1 correct series. Reading DICOM (loading data): 87/99 (87.999/99 (100.0%) # Out[2]: (99, 512, 512) Also in this case, the layout is different fr om what PyTorch expects, due to the lack of channel information. You\u2019ll have to make room for the channel dimension by using unsqueeze: # In[3]: vol = torch.from_numpy(vol_arr).float() vol = torch.transpose(vol, 0, 2) vol = torch.unsqueeze(vol, 0) vol.shape # Out[3]: torch.Size([1, 512, 512, 99]) At this point, you could assemble a 5D data set by stacking multiple volumes along the batch direction, as you did earlier in the chapter. Conclusion You covered a lot of ground in this chap ter. You learned to load the most common types of data and shape them up for cons umption by a neural network. There are more data formats in the wild than we coul d hope to describe in a single volume, of course. Some, like medical histories, are t oo complex to cover in this volume. For the interested reader, however, we do provid e short examples of audio and video tensor creation in bonus Jupyter notebooks in our code repository 11. Exercises Take several pictures of red, blue, and green items with your phone or other digital camera.12 \u2013L o a d e a c h i m a g e , a n d c o n v e r t i t t o a t e n s o r . \u2013 For each image tensor, use the .mean() method to get a sense of how bright the image is. Listing 3.5 code/p1ch4/6_volumetric_ct.ipynb 11https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/p1ch4 12Or download some from the internet if a camera isn\u2019t available. 65Volumetric data \u2013N o w t a k e t h e m e a n o f e a c h c h a n n e l o f y o u r i m a g e s . C a n y o u i d e n t i f y t h e r e d , green, and blue items from only the channel averages? Select a relatively large file containing Python source code. \u2013B u i l d a n i n d e x o f a l l t h e w o r d s i n t he source file. (Feel free to make your tokenization as simple or as comple x as you like; we suggest starting by replacing r\"[^a-zA-Z0-9_]+\" with spaces.) \u2013C o m p a r e y o u r i n d e x w i t h t h e o n e y o u m a d e f o r Pride and Prejudice. Which is larger? \u2013 Create the one-hot encoding for the source code file. \u2013W h a t i n f o r m a t i o n i s l o s t w i t h t h i s e n c o d i n g ? H o w d o e s t h a t i n f o r m a t i o n compare with what\u2019s lost in the Pride and Prejudice encoding? Summary Neural networks require data to be re presented as multidimensional numerical tensors, often 32-bit floating-point. Thanks to how the PyTorch libraries inte ract with the Python standard library and surrounding ecosystem, loading the most common types of data and con- verting them to PyTorch tensors is convenient. In general, PyTorch expects data to be laid out along specific dimensions, according to the model architecture (suc h as convolutional versus recurrent). Data reshaping can be achieved effe ctively with the PyTorch tensor API. Spreadsheets can be straightforward to convert to tensors. Categorical- and ordi- nal-valued columns should be handled di fferently from interval-valued columns. Text or categorical data can be encode d to a one-hot representation through the use of dictionaries. Images can have one or many channels. The most common are the red, green, and blue channels of typical digital photos. Single-channel data formats sometimes omit an explicit channel dimension. Volumetric data is similar to 2D imag e data, with the exception of adding a third dimension: depth. Many images have a per-channel bit depth of 8, though 12 and 16 bits per chan- nel are not uncommon. These bit-depths ca n be stored in a 32-bit floating-point number without loss of precision. 67 The mechanics of learning With the blooming of machine learning th at has occurred over the past decade, the notion of machines that learn fr om experience has become a mainstream theme in both technical and journalistic ci rcles. Now, how is it exactly that a machine learns? What are the mechanics of it, or the algorithm behind it? From the point of view of an outer observer, a lear ning algorithm is presented input data that is paired with desired outputs. When learning has occurred, that algorithm is capa - ble of producing correct outputs when it\u2019s fed new data that is similar enough to the input data on which it was trained. With deep learning, this process works even when the input data and the desired outp ut are far from each other\u2014when they come from different domain s, such as an image and a sentence describing it. As a matter of fact, models that allow you to explain input/output relationships date back centuries. When Johannes Ke pler, a German mathematical astronomer This chapter covers Understanding how algorithms can learn from data Reframing learning as parameter estimation, using differentiation and gradient descent Walking through a simple learning algorithm from scratch Seeing how PyTorch supports learning with autograd 68 CHAPTER 4 The mechanics of learning who lived between 1571 and 1630, figured out his three laws of planetary motion in the early 1600s, he based them on data collected by his mentor, Tycho Brahe, during naked- eye observations (yep, naked eye and a piec e of paper). Not having Newton\u2019s Law of gravitation at his disposal (in fact, Newton us ed Kepler\u2019s work to figure things out), he extrapolated the simplest possible geometric model that could fit the data. By the way, it took him six years of staring at data that didn \u2019t make sense to him, as well as incremental realizations, to formulate these laws. 1 You can see this process in figure 4.1. The first law reads: \u201cThe orbit of every planet is an ellipse with the Sun at one of the two foci.\u201d He didn\u2019t know what caused orbits to be ellipses, but given a set of obser - vations for a planet (or a moon of a large planet, such as Jupiter), he could at that point estimate the shape (the eccentricity) and size (the semi-latus rectum) of the ellipse. With those two parameters computed from the data, he could tell where the planet could possibly be during its journey in the sky. When he figured out the second law\u2014 \u201cA line joining a planet and the Sun sweeps out equal areas during equal intervals of time\u2014he could also tell when a planet would be at a part icular point in space given observations in time.2 Figure 4.1 Johannes Kepler considers multiple candidate models that might fit the data at hand, settling on an ellipse. 1As recounted by Michael Fowler: http://galileoandeinstein.physics.vi rginia.edu/1995/lectures/morekepl.html 2Understanding the details of Kepler\u2019s laws isn\u2019t needed for understanding the chapter, but you can find more information at https://en.wikipedia.org/wiki/Kep ler%27s_laws_of_planetary_motion . 69 How would Kepler estimate the eccentricity and size of the ellipse without comput - ers, pocket calculators or even calculus, none of which had been invented yet? You learn the answer from Kepler\u2019s own recollection in his book New Astronomy or from how J.V. Field put it in his The Origins of Proof series:3 Essentially, Kepler had to try different shapes, using a certai n number of observations to find the curve, then use the curve to find some more positions, for times when he had observations available, and then check wheth er these calculated positions agreed with the observed ones. To sum things up, over those six years, Kepler 1Got lots of good data from his frie nd Brahe (not without some struggle). 2Tried to visualize the heck out of that data because he felt that something fishy was going on. 3Chose the simplest possible model that had a chance to fit the data (an ellipse). 4Split the data so that he could work on part of it and keep an independent set for validation. 5Started with a tentative ecce ntricity and size, and iterated until the model fit the observations. 6Validated his model on the independent observations. 7Looked back in disbelief. There\u2019s a data-science handbook for you, all the way from 1609. The history of science is constructed on these seven steps, and as scientists have learned over the centuries, deviating from them is a recipe for disaster.4 T h e s e s t e p s a r e e x a c t l y w h a t y o u \u2019 l l f o l l o w t o learn s o m e t h i n g f r o m d a t a . H e r e , there\u2019s virtually no difference between saying that you\u2019ll fit the data and saying that you\u2019ll make an algorithm learn from data. The process always involves a function with unknown parameters whose values ar e estimated from data\u2014in short, a model. Y o u c a n a r g u e t h a t learning from data p r e s u m e s t h a t t h e u n d e r l y i n g m o d e l i s n \u2019 t engineered to solve a specific problem (a s was the ellipse in Kepler\u2019s work) and is capable of approximating a much wider family of functions. A neural network would have predicted Tycho Brahe\u2019s trajectories wi thout requiring Kepler\u2019s flash of insight to try fitting the data to an ellipse. Sir Is aac Newton, however, would have had a much harder time deriving his laws of gravitation from a generic model. You\u2019re interested here in the latter kind s of models: one that are n\u2019t engineered to solve specific narrow tasks an d can be adapted automatically to specialize in solving many similar tasks, using input and outp ut pairs\u2014in other wo rds, general models trained on data relevant to the task at hand. In particular, PyTorch is designed to make it easy to create models for which the derivatives of the fitting error, with respect to the parameters, can be expressed analytical ly. Don\u2019t worry if the last sentence didn\u2019t make sense; section 1.1 sh ould clear it up for you. 3https://plus.maths.org/content/o rigins-proof-ii-keplers-proofs 4Unless you\u2019re a theoretical physicist ;) 70 CHAPTER 4 The mechanics of learning This chapter is about how to automate this generic function-fitting, which is all you do with deep learning, deep neural ne tworks being the generic functions, and PyTorch makes this process as simple and transparent as possible. To make sure that you get the key concepts right and to allow you to understand the mechanics of learn - ing algorithms from first princi ples, we\u2019ll start with a model that\u2019s a lot simpler than a deep neural network. 4.1 Learning is parameter estimation In this section, you learn how you can take data, choose a model, and estimate the parameters of the model so th at it gives good predictions on new data. To do so, you\u2019ll leave the intricacies of planet ary motion and divert your attention to the second-hard - est problem in physics: calibrating instruments. Figure 4.2 shows a high-level overview of what you\u2019ll have implemented by the end of the chapter. Given input data and th e corresponding desired outputs (ground truth), as well as initial values for the we ights, the model is fed input data (forward pass), and a measure of the error is evalua ted by comparing the resulting outputs with the ground truth. To optimize the parameter of the model, its weights\u2014the change in the error following a unit change in weights (the gradient of the error with respect to the parameters)\u2014is computed by using the chain rule for the derivative of a compos - ite function (backward pass). Then the valu e of the weights is updated in the direc - tion that leads to a decrease in the error. The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level. I f t h i s s o u n d s o b s c u r e , w e \u2019 v e g o t a w h o l e c h a p t e r t o c l a r i f y t h i n g s . B y t h e t i m e we\u2019re done, all the pieces will fall into place, and the preceding paragraph will make perfect sense to you. Next, you take a problem with a noisy da ta set, build a model, and implement a learning algorithm for it. You\u2019ll start by do ing everything by hand, but by the end of the chapter, you\u2019ll be letting PyTorch do al l the heavy lifting. By the end of the chap - ter, we\u2019ll have covered many of the essentia l concepts that underlie training deep neu- ral networks, even if the motivating exam ple is simple and the model isn\u2019t a neural network (yet!). 4.1.1 A hot problem Suppose that you took a trip to some obscure location and brought back a fancy, wall- mounted analog thermometer. It looks great, it\u2019s a perfect fit for your living room. Its only flaw is that it doesn\u2019t show units. Not to worry; you\u2019ve got a plan. You\u2019ll build a data set of readings and corresponding temperature values in your favorite units, choose a model, and adjust its weights iteratively until a measure of the error is low enough, and you\u2019ll finally be able to interpret the new readings in units you understand. S t a r t b y m a k i n g a n o t e o f t e m p e r a t u r e d a t a i n g o o d o l d C e l s i u s5 a n d m e a s u r e- ments from your new thermometer. 5Luca is Italian, so please forgive him for using sensible units. Figure 4.2 Mental model of the learning process 71Learning is parameter estimation After a couple of weeks, here\u2019s the data: # In[2]: t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] t_c = torch.tensor(t_c) t_u = torch.tensor(t_u) t_c are temperatures in Celsius, and t_u are the unknown units. You can expect noise in both measurements coming from the de vices themselves and from your approxi - mate readings. For convenience, the data is already in tensors, which you\u2019ll use soon. 4.1.2 Choosing a linear model as a first try In the absence of further kn owledge, assume the simplest possible model for convert - ing between the two sets of measurements, as Kepler might have done. The two sets may be linearly related. That is, multiplying t_u by a factor and adding a constant, you may get the temperature in Celsius: t_c = w * t_u + b Is this assumption reasonable? Probably; you\u2019ll see how well the final model performs. (You chose to name w and b after weight and bias, two common terms for linear scaling and the additive constant, which you\u2019ll bump into all the time.) Listing 4.1 code/p1ch5/1_parameter_estimation.ipynb 72 CHAPTER 4 The mechanics of learning NOTE S p o i l e r a l e r t : W e k n o w t h a t a l i n e ar model is correct because the prob- lem and data have been fabricated, but please bear with us; this model is a useful motivating example to build yo ur understanding of what PyTorch is doing under the hood. Now you need to estimate w and b, the parameters in the model, based on the data you have. You must do this so that the temperatures you obtain from running the unknown temperatures t_u through the model are close to temperatures you mea - sured in Celsius. If that process sounds like fitting a line throug h a set of measure - ments, that\u2019s exactly what you\u2019re doing. A s y o u g o t h r o u gh this simple example using PyTorch, realize that training a ne ural network essentially involves changing the model for a slightly more elaborate one with a few (or a metric ton) more parameters. To flesh out the example again, you have a model with some unknown parameters, and you need to estimate those parameters so that the error between predicted out - puts and measured values is as low as possibl e. You notice that you still need to define a measure of such error. Such measure, which we refer to as the loss function, should be high if the error is high and should idea lly be as low as possible for a perfect match. Your optimization process, ther efore, should aim at finding w and b so that the loss function is at a minimum level. 4.1.3 Less loss is what you want A loss function (or cost function) is a function that comput es a single numerical value that the learning process attempts to mi nimize. The calculation of loss typically involves taking the difference between the desired outputs for so me training samples and those produced by the model when fe d those samples\u2014in this case, the differ - ence between the predicted temperatures t_p o u t p u t b y t h e m o d e l a n d t h e a c t u a l measurements, so t_p - t_c. You nee d to make sure the loss func tion makes the loss positive both when t_p is above and when below the true t_c, because the goal is to minimize this value. (Being able to push the loss infinitely negative isn\u2019 t useful.) You have a few choices, the most straightforward being |t_p - t_c| and (t_p - t_c)^2. Based on the mathematical expression you choose, you can emphasize or discount certain errors. Conceptually, a loss function is a way of prioritizing which errors to fix from your training samples, so that your parameter updates result in adjustments to the outputs for the highly weighted sam - ples instead of changes to some other samples\u2019 output that had a smaller loss. Both of the example loss functions have a clear minimum in zero and grow mono - tonically as the predicted value moves farthe r from the true value in either direction. For this reason, both functions are said to be convex. Because your model is linear, the loss as a function of w and b is also convex. Cases in which the loss is a convex function of the model parameters are usually great to deal with because you can find a mini - mum in an efficient way through specialize d algorithms. Deep neural networks don\u2019t exhibit a convex loss, however, so those methods aren\u2019t generally useful to you. 73Learning is parameter estimation For the two loss functions |t_p - t_c| and (t_p - t_c)^2, as shown in figure 4.3, notice that the square of differences beha ves more nicely arou nd the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p equals t_c. The absolute value, on the contrary, has an undefined derivative right where you\u2019d like to converge. This issue is less important than it looks in practice, but stick to the square of differences for the time being. Figure 4.3 Absolute difference versus difference squared It\u2019s worth noting that the square difference also penaliz es wildly wrong results more than the absolute difference. Often, having more slightly wrong results is better than having a few wildly wrong ones, and the sq uared difference helps prioritize those results as desired. 4.1.4 From problem to PyTorch You\u2019ve figured out the model and the loss fu nction, so you\u2019ve already got a good part of the high-level picture figured out. No w you need to set the learning process in motion and feed it actual data. Also, enough with math notation already, so now switch to PyTorch. After all, you came here for the fun. You\u2019ve already created your data tensors, so write out the model as a Python function # In[3]: def model(t_u, w, b): return w * t_u + b in which you\u2019re expecting t_u, w, and b to be the input tensor, weight parameter, and bias parameter, respectively. In your mode l, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the pr oduct operation will use broadcasting to yield the returned tensors. Now define your loss: # In[4]: def loss_fn(t_p, t_c): squared_diffs = (t_p - t_c)**2 return squared_diffs.mean() 74 CHAPTER 4 The mechanics of learning Note that you\u2019re building a tensor of differences, taking their square elementwise and finally producing a scalar loss function by averaging all elements in the resulting ten - sor. The loss is a mean square loss. Now you can initialize the parameters, invoke the model, # In[5]: w = torch.ones(1) b = torch.zeros(1) t_p = model(t_u, w, b) t_p # Out[5]: tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000, 48.4000, 60.4000, 68.4000]) and check the value of the loss: # In[6]: loss = loss_fn(t_p, t_c) loss # Out[6]: tensor(1763.8846) In this section, you implemented the mode l and the loss. The meat of the section is how to estimate the w and b such that the loss reaches a minimum. First, you work things out by hand; then you learn how to leverage PyTorch supe rpowers to solve the same problem in a more ge neral, off-the-shelf way. 4.1.5 Down along the gradient In this section, you optimize the loss functi on with respect to the parameters by using the so-called gradient descent algorithm and build y our intuition about how gradient descent works from first principles, which wi ll help you a lot in the future. There are ways to solve this particular example more e f f i c i e n t l y , b u t t h o s e a p p r o a c h e s a r e n \u2019 t applicable to most deep learni ng tasks. Gradient descent is a simple idea that scales up surprisingly well to large neural netw ork models with millions of parameters. Start with the mental image conveniently sketched out in figure 4.4. Suppose that you\u2019re in front of a machin e sporting two knobs, labeled w and b. You\u2019re allowed to see the value of the loss on a screen and are told to minimize that value. Not knowing the effect of the knobs on the loss, yo u\u2019d probably start fiddling with them and decide for each knob what direction makes the loss decrease. You\u2019d probably decide to rotate both knobs in their direction of decreasing loss. If you\u2019re far from the opti - mal value, you\u2019re likely to see the loss decrease quickly and then slow as it gets closer to the minimum. You\u2019d notice that at some point, the loss climbs back up again, so you\u2019d invert the direction of rotation fo r one or both knobs. You\u2019d also learn that when the loss changes slowly, it\u2019s a good idea to adjust the knobs more finely to avoid reaching the point where the loss goes back up. After a while, eventually, you\u2019d con - verge to a minimum. Figure 4.4 A cartoon depiction of the optimization process, in which a person with knobs for w and b searches for the direction that makes the loss decrease 75Learning is parameter estimation Gradient descent isn\u2019t too different. The idea is to compute the rate of change of the loss with respect to each parameter and ap ply a change to each parameter in the direction of decreasing loss. As when you were fiddling with the knobs, you could esti - mate such rate of change by applying a small change to w and b to see how much the loss is changing in that neighborhood: # In[7]: delta = 0.1 loss_rate_of_change_w = \\ (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta) This code is saying that in a small ne ighborhood of the current values of w and b, a unit increase in w leads to some change in the loss. If the change is negative, you need to increase w t o m i n i m i z e t h e l o s s , w h e r e a s i f the change is positive, you need to decrease w. By how much? Applying a change to w that\u2019s proportional to the rate of change of the loss is a good idea, especi ally when the loss has several parameters: you\u2019d apply a change to those that exert a si gnificant change on the loss. It\u2019s also wise to change the parameters slowly in general, because the rate of change could be dra - matically different at a distance fr om the neighborhood of the current w value. There- fore, you should scale the rate of change by a typically small factor. This scaling factor has many names; the one used in machine learning is learning_rate. # In[8]: learning_rate = 1e-2 w = w - learning_rate * loss_rate_of_change_w You can do the same with b: # In[9]: loss_rate_of_change_b = \\ (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta) b = b - learning_rate * loss_rate_of_change_b 76 CHAPTER 4 The mechanics of learning This code represents the basic parameter upd ate step for gradient descent. By reiter- ating these evaluations (provided that yo u choose a small-enough learning rate), you\u2019d converge to an optimal value of th e parameters for which the loss computed on the given data is minimal. We\u2019ll show you the complete iterative process soon, but this method of computing rates of change is rather crude and needs an upgrade. In the next section, you see why and how. 4.1.6 Getting analytical Computing the rate of change by using repeated evaluations of model and loss to probe the behavior of the loss fu nction in the neighborhood of w and b doesn\u2019t scale well to models with many parameters. Also, it isn\u2019t always clear how large that neigh - borhood should be. You chose delta equal to 0.1 earlier, but everything depends on the shape of the loss as a function of w and b. If the loss changes too quickly compared with delta, you won\u2019t have a good idea of where downhill is. What if you could make the neighborhood infinitesimally small, as in figure 4.5? That\u2019s exactly what happens when you take th e derivative of the loss with respect to a parameter analytically. In a model with two or more parameters, you compute the individual derivatives of the loss with respec t to each parameter and put them in a vec - tor of derivatives: the gradient. Figure 4.5 Differences in the estimated downhill directions when evaluating them at discrete locations versus analytically To compute the derivative of the loss with respect to a parameter, you can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model) times the derivative of the model with respect to the parameter: d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w) Recall that the model is a linear function and the loss is a sum of squares. Now figure out the expressions for the derivatives. Recalling the expression for the loss # In[4]: def loss_fn(t_p, t_c): squared_diffs = (t_p - t_c)**2 return squared_diffs.mean() 77Learning is parameter estimation and remembering that d x^2 / d x = 2 x, you get # In[10]: def dloss_fn(t_p, t_c): dsq_diffs = 2 * (t_p - t_c) return dsq_diffs As for the model, recalling that the model is # In[3]: def model(t_u, w, b): return w * t_u + b you get derivatives of # In[11]: def dmodel_dw(t_u, w, b): return t_u # In[12]: def dmodel_db(t_u, w, b): return 1.0 Putting all this together, the function that returns the gradient of the loss with respect to w and b is # In[13]: def grad_fn(t_u, t_c, t_p, w, b): dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b) dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b) return torch.stack([dloss_dw.mean(), dloss_db.mean()]) The same idea expressed in mathematic al notation is shown in figure 4.6. Figure 4.6 The derivative of the loss function with respect to the weights Again, you\u2019re averaging (summing and divi ding by a constant) over all data points to get a single scalar quantity for each partial derivative of the loss. 78 CHAPTER 4 The mechanics of learning 4.1.7 The training loop Now you have everything in pl ace to optimize your parame ters. Starting from a tenta - tive value for a parameter, you can iterativel y apply updates to it for a fixed number of iterations or until w and b stop changing. You can use se veral stopping criteria, but stick to a fixed number of iterations for now. While we\u2019re at it, we\u2019ll introduce you to another piece of terminology. A training iteration during which you update the parame ters for all your training samples is called an epoch. The complete training loop looks like this: # In[14]: def training_loop(n_epochs, learning_rate, params, t_u, t_c): for epoch in range(1, n_epochs + 1): w, b = params t_p = model(t_u, w, b) loss = loss_fn(t_p, t_c) grad = grad_fn(t_u, t_c, t_p, w, b) params = params - learning_rate * grad print('Epoch %d, Loss %f' % (epoch, float(loss))) return params The actual logging logi c used for the output in this te xt is more complicated (see cell 15 in the same notebook), 6 but the differences are un important for understanding the core concepts in this chapter. Now invoke your training loop: # In[16]: training_loop( n_epochs = 100, learning_rate = 1e-2, params = torch.tensor([1.0, 0.0]), t_u = t_u, t_c = t_c) # Out[16]: Epoch 1, Loss 1763.884644 Params: tensor([-44.1730, -0.8260]) Grad: tensor([4517.2964, 82.6000]) Epoch 2, Loss 5802484.500000 Params: tensor([2568.4011, 45.1637]) Grad: tensor([-261257.4062, -4598.9707]) Epoch 3, Loss 19408031744.000000 Params: tensor([-148527.7344, -2616.3933]) Grad: tensor([15109615.0000, 266155.7188]) ... 6https://github.com/deep-learning-with-pytorch/dlwpt -code/blob/master/p1ch5/1_parameter_estimation.ipynb This is the forward pass. And this is the backward pass. This logging line can be verbose. 79Learning is parameter estimation Epoch 10, Loss 90901075478458130961171361977860096.000000 Params: tensor([3.2144e+17, 5.6621e+15]) Grad: tensor([-3.2700e+19, -5.7600e+17]) Epoch 11, Loss inf Params: tensor([-1.8590e+19, -3.2746e+17]) Grad: tensor([1.8912e+21, 3.3313e+19]) tensor([-1.8590e+19, -3.2746e+17]) Wait\u2014what happened? Your training process blew up, le ading to losses becoming inf. This result is a clear sign that params is receiving updates that are too large; their val - ues start oscillating back and forth as each update overshoots, and the next overcor - rects even more. The optimization process is unstable; it diverges instead of converging to a minimum. You want to see smaller and smaller updates to params, not larger, as shown in figure 4.7. Figure 4.7 Top: Diverging optimization on convex function (parabolalike) due to large steps. Bottom: Converging optimization with small steps. How can you limit the magnitude of the learning_rate * grad? Well, that process looks easy. You could simply choose a smaller learning_rate. You usually change learn- ing rates by order of magnitude, so you might try 1e-3 or 1e-4, which would decrease the magnitude of updates by orders of magnitude. Go with 1e-4 to see how it works out: # In[17]: training_loop( n_epochs = 100, learning_rate = 1e-4, 80 CHAPTER 4 The mechanics of learning params = torch.tensor([1.0, 0.0]), t_u = t_u, t_c = t_c) # Out[17]: Epoch 1, Loss 1763.884644 Params: tensor([ 0.5483, -0.0083]) Grad: tensor([4517.2964, 82.6000]) Epoch 2, Loss 323.090546 Params: tensor([ 0.3623, -0.0118]) Grad: tensor([1859.5493, 35.7843]) Epoch 3, Loss 78.929634 Params: tensor([ 0.2858, -0.0135]) Grad: tensor([765.4666, 16.5122]) ... Epoch 10, Loss 29.105242 Params: tensor([ 0.2324, -0.0166]) Grad: tensor([1.4803, 3.0544]) Epoch 11, Loss 29.104168 Params: tensor([ 0.2323, -0.0169]) Grad: tensor([0.5780, 3.0384]) ... Epoch 99, Loss 29.023582 Params: tensor([ 0.2327, -0.0435]) Grad: tensor([-0.0533, 3.0226]) Epoch 100, Loss 29.022669 Params: tensor([ 0.2327, -0.0438]) Grad: tensor([-0.0532, 3.0226]) tensor([ 0.2327, -0.0438]) Nice. The behavior is stable now. But th ere\u2019s another problem: updates to parameters are small, so the loss decreases slowly and even tually stalls. You could obviate this issue by making the learning_rate adaptive\u2014that is, change according to the magnitude of updates. You can use several optimization schemes for that purpose; you see one toward the end of this chapter, in section \u201cOptimizers a-la Carte\u201d. Another potential troublemaker exists in the update term: the gradient itself. Go back to look at grad at epoch 1 during optimization. You see that the first-epoch gradi - ent for the weight is about 50 times larger than the gradient for the bias, so the weight and bias live in differently scaled spaces. In this case, a learning rate that\u2019s large enough to meaningfully update one is so larg e that it\u2019s unstable for the other, or a rate that\u2019s appropriate for the se cond one won\u2019t be large enough to change the first mean - ingfully. You\u2019re not going to be able to update your parameters unless you change your formulation of the problem. You could have individual learning rates for each parameter, but for models with many parame ters, this approach would be too much to bother with; it\u2019s babysitting of the kind you don\u2019t like. You have a simpler way to keep things in check: change the inputs so that the gra - dients aren\u2019t so different. You can make sure that the range of the input doesn\u2019t get too far from the range of -1.0 to 1.0, roug hly speaking. In this case, you can achieve something close enough to that example by multiplying t_u by 0.1: 81Learning is parameter estimation # In[18]: t_un = 0.1 * t_u Here, you denote the normalized version of t_u by appending n to the variable name. At this point, you can run the traini ng loop on your normalized input: # In[19]: training_loop( n_epochs = 100, learning_rate = 1e-2, params = torch.tensor([1.0, 0.0]), t_u = t_un, t_c = t_c) # Out[19]: Epoch 1, Loss 80.364342 Params: tensor([1.7761, 0.1064]) Grad: tensor([-77.6140, -10.6400]) Epoch 2, Loss 37.574917 Params: tensor([2.0848, 0.1303]) Grad: tensor([-30.8623, -2.3864]) Epoch 3, Loss 30.871077 Params: tensor([2.2094, 0.1217]) Grad: tensor([-12.4631, 0.8587]) ... Epoch 10, Loss 29.030487 Params: tensor([ 2.3232, -0.0710]) Grad: tensor([-0.5355, 2.9295]) Epoch 11, Loss 28.941875 Params: tensor([ 2.3284, -0.1003]) Grad: tensor([-0.5240, 2.9264]) ... Epoch 99, Loss 22.214186 Params: tensor([ 2.7508, -2.4910]) Grad: tensor([-0.4453, 2.5208]) Epoch 100, Loss 22.148710 Params: tensor([ 2.7553, -2.5162]) Grad: tensor([-0.4445, 2.5165]) tensor([ 2.7553, -2.5162]) Even though you set your learning rate back to 1e-2, parameters didn\u2019t blow up during iterative updates. Now ta ke a look at the gradients; they were of similar magni - tudes, so using a single learning_rate for both parameters worked fine. You probably could do a better job of normalization than rescaling by a factor of ten, but because doing so is good enough for your needs, stick it for now. NOTE T h e n o r m a l i z a t i o n h e r e h e l p s y o u get the network trained, but you could make an argument that it\u2019s not st rictly needed to optimize the parame- ters for this problem. That\u2019s absolute ly true! This problem is small enough that you have numerous ways to beat the parameters into submission. For larger, more sophisticated problems, ho wever, normalization is an easy and effective (if not crucial!) tool to use to improve model convergence. You\u2019ve updated t_u to your new, rescaled t_un. 82 CHAPTER 4 The mechanics of learning Next, run the loop for enough it erations to see the changes in params g e t s m a l l . Change n_epochs to 5000: # In[20]: params = training_loop( n_epochs = 5000, learning_rate = 1e-2, params = torch.tensor([1.0, 0.0]), t_u = t_un, t_c = t_c, print_params = False) params # Out[20]: Epoch 1, Loss 80.364342 Epoch 2, Loss 37.574917 Epoch 3, Loss 30.871077 ... Epoch 10, Loss 29.030487 Epoch 11, Loss 28.941875 ... Epoch 99, Loss 22.214186 Epoch 100, Loss 22.148710 ... Epoch 4000, Loss 2.927680 Epoch 5000, Loss 2.927648 tensor([ 5.3671, -17.3012]) Good. You saw the loss decrease while you we re changing parameters along the direc - tion of gradient descent. The loss didn\u2019t go to zero, which could mean that iterations weren\u2019t enough to converge to zero or that the data points aren\u2019t sitting on a line. As anticipated, your measurements weren\u2019t pe rfectly accurate or noise was involved in the reading. But look: the value for w and b looks an awful lot like the numbers you need to use to convert Celsius to Fahrenheit (after ac counting for the earlier normalization when you multiplied your inputs by 0.1). The exact values are w=5.5556 and b=-17.7778. Your fancy thermometer was showing temper atures in Fahrenheit the whole time, which is no big discovery, but it proves th at your gradient descent optimization pro - cess works. Next, do something that you should have do ne right from the start: plot your data. We didn\u2019t introduce this topi c until now for the sake of drama (the surprise effect). But seriously, the first thing that anyone do ing data science should do is plot the heck out of the data. # In[21]: %matplotlib inline from matplotlib import pyplot as plt t_p = model(t_un, *params) Remember that you\u2019re training on the normalized unknown units. 83PyTorch\u2019s autograd: Backpropagate all things fig = plt.figure(dpi=600) plt.xlabel(\"Fahrenheit\") plt.ylabel(\"Celsius\") plt.plot(t_u.numpy(), t_p.detach().numpy()) plt.plot(t_u.numpy(), t_c.numpy(), 'o') This code produces Figure 4.8. Figure 4.8 The plot of your linear-fit model (solid line) versus input data (circles) The linear model is a good model for the data , it seems. It also seems that your mea - surements are somewhat erratic. You should e i t h er c a l l y o u r o p t o m e t r i s t f o r a n e w pair of glasses or think about returning your fancy thermometer. 4.2 PyTorch\u2019s autograd: Backpropagate all things In your little adventure so far, you saw a simple example of backpropagation. You com - puted the gradient of a composition of f u n c tions\u2014the model and the loss\u2014with respect to their innermost parameters\u2014 w and b\u2014by propagating derivatives backward via the chain rule. The basic requirement is that all functions you\u2019re dealing with are differentiable analytically. In this case, you can compute th e gradient (which we called \u201cthe rate of change of the loss\u201d earlier) with respect to the parameters in one sweep. Should you have a complicated model with millions of parameter s, as long as the model is differentiable, computing the grad ient the loss with respect to parameters amounts to writing the analytical expressi on for the derivatives and evaluating them once. Granted, writing the analytical expre ssion for the derivatives of a deep composi - tion of linear and nonlinear functions isn\u2019t a lot of fun. 7 It isn\u2019t particularly quick, either. 7Or maybe it is; we won\u2019t judge how you spend your weekend! But you\u2019re plotting the raw unknown values. 84 CHAPTER 4 The mechanics of learning This situation is where PyTorch tensors come to the rescue, with a PyTorch compo - nent called autograd. PyTorch tensors can remember where they come from in terms of the operations and parent tensors that originated them, and they can provide the chain of derivatives of such operations with respect to their inputs automatically. You won\u2019t need to derive your model by hand; 8 given a forward expression, no matter how nested, PyTorch provides the gradient of th at expression with respect to its input parameters automatically. At this point, the best way to proceed is to rewrite the thermometer calibration code, this time using autograd, and see what happens. First, recall your model and loss function, as shown in the following listing. # In[3]: def model(t_u, w, b): return w * t_u + b # In[4]: def loss_fn(t_p, t_c): squared_diffs = (t_p - t_c)**2 return squared_diffs.mean() Again initialize a parameters tensor: # In[5]: params = torch.tensor([1.0, 0.0], requires_grad=True) Notice the requires_grad=True argument to the tensor constructor? That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that has params as an ancestor has access to the chain of functions that we re called to get from params to that tensor. In case these functions are differentiable (and most Py Torch tensor operatio ns are), the value of the derivative is automatically populated as a grad attribute of the params tensor. In general, all PyTorch tensors have an attribute named grad, normally None: # In[6]: params.grad is None # Out[6]: True All you have to do to populate it is start with a tensor with requires_grad set to True, call the model, compute the loss, and then call backward on the loss tensor: # In[7]: loss = loss_fn(model(t_u, *params), t_c) loss.backward() 8Bummer! What are we going to do on Saturdays now? Listing 4.2 code/p1ch5/2_autograd.ipynb 85PyTorch\u2019s autograd: Backpropagate all things params.grad # Out[7]: tensor([4517.2969, 82.6000]) At this point, the grad a t t r i b u t e o f params c o n t a i n s t h e d e r i v a t i v e s o f t h e l o s s w i t h respect to each element of params (figure 4.9). Figure 4.9 The forward graph and backward graph of model as computed with autograd You could have any number of tensors with requires_grad set to True and any composition of functions. In this case, Py Torch would compute de rivatives of the loss throughout the chain of functions (the co mputation graph) and accumulate their val - ues in the grad attribute of those tensors (the leaf nodes of the graph). Alert: Big gotcha ahead. This is one thing that PyTorch newcomers (and a lot of more experienced folks) trip up on regularly. We wrote accumulate, not store. WARNING C a l l i n g backward leads derivatives to accumulate at leaf nodes. You need to zero the gradient explicitly after using it for parameter updates. 86 CHAPTER 4 The mechanics of learning To repeat, calling backward leads derivatives to accumulate at leaf nodes. So if back- ward has been called earlier, the loss is evaluated again, and backward is called again (as in any training loop), the gradient at each leaf is accumulated (summed) on top of the one computed at the preceding iteration, which leads to an incorrect value for the gradient. To prevent this situation fr om occurring, you need to zero the gradient explicitly at each iteration. You can do so easily by using the in-place zero_ method: # In[8]: if params.grad is not None: params.grad.zero_() NOTE Y o u m a y b e c u r i o u s w h y z e r o i n g t h e g radient is a required step instead of automatic whenever you call backward. The reason is to provide more flex- ibility and control for working with gradients in complicated models. Having this reminder drilled into your h e a d , now see how your autograd-enabled training code looks like, start to end: # In[9]: def training_loop(n_epochs, learning_rate, params, t_u, t_c): for epoch in range(1, n_epochs + 1): if params.grad is not None: params.grad.zero_() t_p = model(t_u, *params) loss = loss_fn(t_p, t_c) loss.backward() params = (params - learning_rate * params.grad).detach().requires_grad_() if epoch % 500 == 0: print('Epoch %d, Loss %f' % (epoch, float(loss))) return params Notice that when you updated params, you also did an odd .detach().requi- res_grad_() d a n c e . T o u n d e r s t a n d w h y , t h i nk about the computation graph that you\u2019ve built. Reformulate your params update line a little so that you\u2019re not reusing vari- able names: p1 = (p0 * lr * p0.grad) Here, p0 is the random weights with which you initialized the model. p0.grad is computed from a combination of p0 and your training data via the loss function. So far, so good. Now you need to look at the second iteration of the loop: p2 = (p1 * lr * p1.grad). As you\u2019ve seen, the computation graph for p1 goes back to p0, which is problematic because (a) you have to keep p0 in memory (until you\u2019re done with training), and (b) it confuses the matt er of where you should be assigning error via backpropagation. This could be done at any point in the loop prior to calling loss.backward() It\u2019s somewhat cumbersome, but as you\u2019ll see in \u201cOptimizers a-la Carte,\u201d it\u2019s not an issue in practice. 87PyTorch\u2019s autograd: Backpropagate all things I n s t e a d , d e t a c h t h e n e w params t e n s o r f r o m t h e c o m putation graph associated with its update expression by calling .detatch(). This way, params effectively loses the memory of the operations that generated it . Then you can reenable tracking by call - ing .requires_grad_(), an in_place o p e r a t i on (s e e t h e tr a i l i n g _) that reactivates autograd for the tensor. Now you can release the memory held by old versions of params and need to backpropagate thro ugh only your current weights. See whether this code works: # In[10]: training_loop( n_epochs = 5000, learning_rate = 1e-2, params = torch.tensor([1.0, 0.0], requires_grad=True), t_u = t_un, t_c = t_c) # Out[10]: Epoch 500, Loss 7.860116 Epoch 1000, Loss 3.828538 Epoch 1500, Loss 3.092191 Epoch 2000, Loss 2.957697 Epoch 2500, Loss 2.933134 Epoch 3000, Loss 2.928648 Epoch 3500, Loss 2.927830 Epoch 4000, Loss 2.927679 Epoch 4500, Loss 2.927652 Epoch 5000, Loss 2.927647 tensor([ 5.3671, -17.3012], requires_grad=True) You get the same result that you got previously. Good for you! Although you\u2019re capable of computing derivatives by ha nd, you no longer need to. 4.2.1 Optimizers a la carte This code uses vanilla gradient descent for optimization, which works fine for this sim - ple case. Needless to say, several optimizati on strategies and tricks can help conver - gence, especially when models get complicated. Now is the right time to introduce the way that PyTorch abstracts the optimization strategy away from user code, such as th e training loop, sparing you from the boiler - plate busywork of having to update ever y parameter in your model yourself. The torch module has an optim submodule where you can find classes that implement dif - ferent optimization algorithms. Here\u2019s an abridged listing: # In[5]: import torch.optim as optim dir(optim) # Out[5]: Listing 4.3 code/p1ch5/3_optimizers.ipynb Adding this requires_grad=True is key. Note that again, you\u2019re using the normalized t_un instead of t_u. 88 CHAPTER 4 The mechanics of learning ['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'Adamax', 'LBFGS', 'Optimizer', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam', ... ] Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically with requires_grad set to True) as the first input. All parameters passed to the opti - mizer are retained inside the optimizer object so that the optimizer can update their values and access their grad attribute, as represented in figure 4.10. Figure 4.10 Conceptual representation of how an optimizer holds a reference to parameters (A), and after a loss is computed from inputs (B), a call to .backward leads to .grad being populated on parameter (C). At that point, the optimizer can access .grad and compute the parameter updates (D). Each optimizer exposes two methods: zero_grad and step. The former zeros the grad attribute of all the parameters passed to the optimizer upon construction. The latter updates the value of those paramete rs according to the optimization strategy implemented by the specific optimizer. 89PyTorch\u2019s autograd: Backpropagate all things Now create params and instantiate a gradient descent optimizer: # In[6]: params = torch.tensor([1.0, 0.0], requires_grad=True) learning_rate = 1e-5 optimizer = optim.SGD([params], lr=learning_rate) Here, SGD stands for Stochastic Gradient Descent . The optimizer itself is a vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the default). The term stochastic comes from the fact that the gradie nt is typically obtained by averaging over a random subset of all input samples, called a minibatch. The optimizer itself, however, doesn\u2019t know whethe r the loss was evaluated on all the samples (vanilla) or a random subset thereof (stochastic), so th e algorithm is the same in the two cases. Anyway, take your fancy new optimizer for a spin: # In[7]: t_p = model(t_u, *params) loss = loss_fn(t_p, t_c) loss.backward() optimizer.step() params # Out[7]: tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True) The value of params was updated when step was called, and you didn\u2019t have to touch it yourself! What happened was that the optimizer looked into params.grad a n d updated params by subtracting learning_rate times grad from it, exactly as in your former hand-rolled code. Are you ready to stick this code in a training loop? Nope! The big gotcha almost got you: you forgot to zero out the gradie nts. Had you called the preceding code in a loop, gradients would have accumulate d in the leaves at every call to backward, and your gradient descent would have been all over the place! Here\u2019s the loop-ready code, with the extra zero_grad in the right spot (before the call to backward): # In[8]: params = torch.tensor([1.0, 0.0], requires_grad=True) learning_rate = 1e-2 optimizer = optim.SGD([params], lr=learning_rate) t_p = model(t_un, *params) loss = loss_fn(t_p, t_c) optimizer.zero_grad() loss.backward() optimizer.step() params # Out[8]: tensor([1.7761, 0.1064], requires_grad=True) As before, the placement of this call is somewhat arbitrary. It could be earlier in the loop as well. 90 CHAPTER 4 The mechanics of learning Perfect! See how the optim module helped you abstract aw ay the specific optimization scheme? All you have to do is provide a list of params to it (that list can be extremely long, as needed for deep neural network models) and then forget about the details. Update your training loop accordingly: # In[9]: def training_loop(n_epochs, optimizer, params, t_u, t_c): for epoch in range(1, n_epochs + 1): t_p = model(t_u, *params) loss = loss_fn(t_p, t_c) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print('Epoch %d, Loss %f' % (epoch, float(loss))) return params # In[10]: params = torch.tensor([1.0, 0.0], requires_grad=True) learning_rate = 1e-2 optimizer = optim.SGD([params], lr=learning_rate) training_loop( n_epochs = 5000, optimizer = optimizer, params = params, t_u = t_un, t_c = t_c) # Out[10]: Epoch 500, Loss 7.860116 Epoch 1000, Loss 3.828538 Epoch 1500, Loss 3.092191 Epoch 2000, Loss 2.957697 Epoch 2500, Loss 2.933134 Epoch 3000, Loss 2.928648 Epoch 3500, Loss 2.927830 Epoch 4000, Loss 2.927679 Epoch 4500, Loss 2.927652 Epoch 5000, Loss 2.927647 tensor([ 5.3671, -17.3012], requires_grad=True) Again, you get the same result as before. Great. You have further confirmation that you know how to descend a gradient by hand ! To test more optimizers, all you have to do is instantiate a different optimizer, such as Adam, instead of SGD. The rest of the code stays as is. This stuff is pretty handy. We won\u2019t go into much detail on Adam, but it suffices to say that it\u2019s a more sophis- ticated optimizer in which the learning rate is set adaptively. In addition, it\u2019s a lot less sensitive to the scaling of the parameters\u2014s o insensitive that you can go back to use It\u2019s important that both params here are the same object; otherwise, the optimizer won\u2019t know what parameters the model used. 91PyTorch\u2019s autograd: Backpropagate all things the original (non-normalized) input t_u and even increase the learning rate to 1e-1. Adam won\u2019t even blink: # In[11]: params = torch.tensor([1.0, 0.0], requires_grad=True) learning_rate = 1e-1 optimizer = optim.Adam([params], lr=learning_rate) training_loop( n_epochs = 2000, optimizer = optimizer, params = params, t_u = t_u, t_c = t_c) # Out[11]: Epoch 500, Loss 7.612901 Epoch 1000, Loss 3.086700 Epoch 1500, Loss 2.928578 Epoch 2000, Loss 2.927646 tensor([ 0.5367, -17.3021], requires_grad=True) The optimizer isn\u2019t the only flexible part of your training loop. Turn your attention to the model. To train a neural network on th e same data and the same loss, all you\u2019d need to change is the model function. Doing this wouldn\u2019t make sense in this case, because you know that converting Celsius to Fahrenheit amounts to a linear transfor - mation. Neural networks allow you to remo ve your arbitrary a ssumptions about the shape of the function you should be approx imating. Even so, neural networks manage to be trained even when the underlying pr ocesses are highly nonlinear (such in the case of describing an image with a sentence). We\u2019ve touched on a lot of the essential co ncepts that will enable you to train com- plicated deep learning models while know ing what\u2019s going on under the hood: back - propagation to estimate gradients, autograd, and optimizing weights of models by using gradient descent or other optimizers. We don\u2019t have a whole lot more to cover. The rest is mostly filling in the blanks, however extensive they are. Next, we discuss how to split up samples, which sets up a perfect use case for learn - ing to control autograd better. 4.2.2 Training, valida tion, and overfitting Johannes Kepler kept a part of the data on the side so that he could validate his mod - els on independent observations\u2014a vital thing to do, especially when the model you adopt could potentially approximate functions of any shape, as in the case of neural networks. In other words, a highly adaptable model tends to use its many parameters to make sure that the loss is minimal at the data points, but you\u2019ll have no guarantee that the model behaves well away from or between the data points. After all, that\u2019s all you\u2019re asking the optimizer to do: minimize the loss at the data points. Sure enough, if you had independent data points that you didn\u2019t use to evaluate your loss or New optimizer class here. Note that you\u2019re back to the original t_u as input. 92 CHAPTER 4 The mechanics of learning descend along its negative gradient, you\u2019d soon find out that evaluating the loss at those independent data points would yield a higher-than-expected loss. We\u2019ve already mentioned this phenomenon, called overfitting. The first action you can take to combat ov erfitting is to recognize that it might happen. To do so, as Kepler figured out in 1600, you must take a few data points out of your data set (the validation set) and fit our model to the remaining data points (the training set), as shown in figure 4.11. Then, while you\u2019re fitting the model, you can evaluate the loss once on th e training set and once on the validation set. When you\u2019re trying to decide whether you\u2019ve done a good job of fitting your model to the data, you must look at each data set! Figure 4.11 Conceptual representation of a  data-producing process and the collection and use of training data and independent validation data The first figure, the training loss, tells you whether your model can fit the training set at all\u2014in other words, wh ether your model has enough capacity to process the rele- vant information in the data. If your mysterious thermometer somehow managed to measure temperatures by using a logarithmic scale, your poor linear model wouldn\u2019t have had a chance to fit those measurements and provide a sensible conversion to Cel - sius. In that case, your trai ning loss (the loss you were printing in the training loop) would stop decreasing well before approaching zero. A deep neural network can potentially approximate complica ted functions, pro- vided that the number of neurons\u2014and, th erefore, parameters\u2014is high enough. The fewer the parameters, the simpler the shape of the function your network will be able to approximate. So here\u2019s ru le one: if the training loss isn\u2019t decreasing, chances are 93PyTorch\u2019s autograd: Backpropagate all things that the model is too simple for the data. The other possibility is that your data doesn\u2019t contain meaningful information for it to explain the output . If the nice folks at the shop sold you a barometer in stead of a thermometer, you\u2019d have little chance to pre - dict temperature in Celsius from pressure alon e, even if you used the latest neural net - work architecture from Quebec. 9 W h a t a b o u t t h e v a l i d a t i o n s e t ? W e l l , i f t h e l o s s e v a l u a t e d i n t h e v a l i d a t i o n s e t doesn\u2019t decrease along with the training set, your model is improving its fit of the sam - ples it\u2019s seeing during training, but it isn\u2019t generalizing to samples outside this precise set. As soon as you evaluate the model at new, previously unseen points, the values of the loss function are poor. Here\u2019s rule two: if the training loss and the validation loss diverge, you\u2019re overfitting. We\u2019ll delve into this phenomenon a litt le here, going back to the thermometer example. You could have decided to fit th e data with a more complicated function, such as a piecewise polynomial or a large neural network. This function could gener - ate a model that meanders its way through the data points, as in figure 4.12, because it pushes the loss close to zero. Because the be havior of the function away from the data points doesn\u2019t increase the loss, there\u2019s nothing to keep the model away from the training data points in check. Figure 4.12 Rather extreme example of overfitting 9https://www.umontreal.ca/en/artificialintelligence 94 CHAPTER 4 The mechanics of learning What\u2019s the cure, though? Go od question. Overfitting looks like a problem of making sure that the behavior of the model in between data points is sensible for the process you\u2019re trying approximate. Fi rst, you should make sure that you get enough data for the process. If you collected data from a si nusoidal process by sampling it regularly at a low frequency, you\u2019d have a ha rd time fitting a model to it. Assuming that you have enough data poin ts, you should make sure that the model that\u2019s capable of fitting the training data i s a s r eg u l a r a s p o s s i b l e b et w e e n t h e d a t a points. You have several ways to achieve this goal. One way is to add so-called penaliza- tion terms t o t h e l o s s f u n c t i o n t o m a k e i t c h e a p e r f o r t h e m o d e l t o b e h a v e m o r e smoothly and change more slowly (up to a po int). Another way is to add noise to the input samples, to artificially create new da ta points between training data samples and force the model to try to fit them too. Several other ways are somewhat related to the preceding ones. But the best favor you can do for yourself, at least as a first move, is to make your model simpler. From an intuit ive standpoint, a simpler model may not fit the training data as perfectly as a more complicated model would do, but it will likely behave more regularly between data points. You\u2019ve got some nice tradeoffs here. On one hand, you need to model to have enough capacity for it to fit the training set. On the other hand, you need the model to avoid overfitting. Therefore, the process for choosing the right size of a neural net - work model, in terms of parameters, is based on two steps: increase the size until it fits and then scale it down un til it stops overfitting. Your life will be a balancing act between fitting and overfitting. You can split the data into a training set and a validation set by shuffling t_u and t_c in the same way and then splitting the resulting shuffled tensors into two parts. Shuffling the elements of a tensor amount s to finding a permutation of its indices. The randperm function does this: # In[12]: n_samples = t_u.shape[0] n_val = int(0.2 * n_samples) shuffled_indices = torch.randperm(n_samples) train_indices = shuffled_indices[:-n_val] val_indices = shuffled_indices[-n_val:] train_indices, val_indices # Out[12]: (tensor([ 8, 0, 3, 6, 4, 1, 2, 5, 10]), tensor([9, 7])) You get index tensors that you can use to bu ild training and validation sets starting from the data tensors: # In[13]: train_t_u = t_u[train_indices] train_t_c = t_c[train_indices] val_t_u = t_u[val_indices] Because these values are ra ndom, don\u2019t be surprised if your values end up being different from here on. 95PyTorch\u2019s autograd: Backpropagate all things val_t_c = t_c[val_indices] train_t_un = 0.1 * train_t_u val_t_un = 0.1 * val_t_u Your training loop doesn\u2019t ch ange. You want to evaluate the validation loss at every epoch to have a chance to reco gnize whether you\u2019re overfitting: # In[14]: def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c): for epoch in range(1, n_epochs + 1): train_t_p = model(train_t_u, *params) train_loss = loss_fn(train_t_p, train_t_c) val_t_p = model(val_t_u, *params) val_loss = loss_fn(val_t_p, val_t_c) optimizer.zero_grad() train_loss.backward() optimizer.step() if epoch <= 3 or epoch % 500 == 0: print('Epoch {}, Training loss {}, Validation loss {}'.format( epoch, float(train_loss), float(val_loss))) return params # In[15]: params = torch.tensor([1.0, 0.0], requires_grad=True) learning_rate = 1e-2 optimizer = optim.SGD([params], lr=learning_rate) training_loop( n_epochs = 3000, optimizer = optimizer, params = params, train_t_u = train_t_un, val_t_u = val_t_un, train_t_c = train_t_c, val_t_c = val_t_c) # Out[15]: Epoch 1, Training loss 88.59708404541016, Validation loss 43.31699752807617 Epoch 2, Training loss 34.42190933227539, Validation loss 35.03486633300781 Epoch 3, Training loss 27.57990264892578, Validation loss 40.214229583740234 Epoch 500, Training loss 9.516923904418945, Validation loss 9.02982234954834 Epoch 1000, Training loss 4.543173789978027, Validation loss 2.596876621246338 Epoch 1500, Training loss 3.1108808517456055, Validation loss 2.9066450595855713 Epoch 2000, Training loss 2.6984243392944336, Validation loss 4.1561737060546875 Epoch 2500, Training loss 2.579646348953247, Validation loss 5.138668537139893 Epoch 3000, Training loss 2.5454416275024414, Validation loss 5.755766868591309 tensor([ 5.6473, -18.7334], requires_grad=True) These two pairs of lines are the same except for the train_* vs. val_* inputs. Note that you have no val_loss.backward() here because you don\u2019t want to train the model on the validation data. Because you\u2019re using SGD again, you\u2019re back to using normalized inputs. 96 CHAPTER 4 The mechanics of learning Here, we\u2019re not being entirely fair to the mo del. The validation set is small, so the val - idation loss will be meaningful only up to a point. In any case, note that the validation loss is higher than your training loss, alth ough not by an order of magnitude. The fact that a model performs better on the training set is expected since the model parame - ters are being shaped by the training set. Yo ur main goal is to also see both the train - ing loss and the validation loss decreasing. Although ideally, both losses would be roughly the same value, as long as validation loss stays reas onably close to the training loss, you know that your model is continui ng to learn generalized things about your data. In figure 4.13, case C is ideal, and D is acceptable. In case A, the model isn\u2019t learning at all, and in ca se B, you see overfitting. Figure 4.13 Overfitting scenarios for the training (blue) and validation (red) losses. (A) Training and validation losses don\u2019t decrease; the model isn\u2019t learning due to no information in the data or insufficient capacity of the model. (B) Training loss decreases wh ile validation increases (overfitting). (C) Training and validation losses decrease in tandem; performance ma y be improved further, as the model isn\u2019t at the limit of overfitting. (D) Training and validation losses have different absolute values but similar trends; overfitting is under control. 4.2.3 Nits in autograd and switching it off From the training loop, you can appr eciate that you only ever call backward on the train_loss. Therefore, errors only ever backprop agate based on the training set. The validation set is used to provide an inde pendent evaluation of the accuracy of the model\u2019s output on data that wasn\u2019t used for training. The curious reader will have an embryo of a question at this point. The model is evaluated twice\u2014once on train_t_u and then on val_t_u\u2014after which backward is 97PyTorch\u2019s autograd: Backpropagate all things called. Won\u2019t this confuse the hell out of autograd? Won\u2019t backward be influenced by the values generated during th e pass on the validation set? Luckily, this isn\u2019t the case. The firs t line in the training loop evaluates model on train_t_u t o p r o d u c e train_t_p. Then train_loss i s e v a l u a t e d from train_t_p, creating a computation graph that links train_t_u t o train_t_p t o train_loss. When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this case, a separate co mputation graph is created that links val_t_u t o val_t_p t o val_loss. Separate tensors have been ru n through the same functions, model a n d loss_fn, generating separate computation graphs, as shown in figure 4.14. Figure 4.14 Diagram showing how gradients propagat e through a graph with two losses when .backward is called on one of them The only tensors that these two graphs have in common are the parameters. When you call backward on train_loss, you run the backward on the first graph. In other words, you accumulate the derivatives of the train_loss with respect to the parame - ters based on the computation generated from train_t_u. If you (incorrectly) called backward on val_loss as well, you\u2019d have accumulated the derivatives of the val_loss with respect to the parameters on the same leaf nodes . Remember the zero_grad thing, whereby gradients would be accumulated on top of each other every time you called backward unless you zeroed out gradients explicitly? Well, here something simi lar would happen: calling backward o n val_loss w o u l d lead to gradients accumulating in the params tensor, on top of those generated during the train_loss.backward() call. In this case, you\u2019d effectively train your model on the whole data set (both training and valid ation), because the gradient would depend on both. Pretty interesting. 98 CHAPTER 4 The mechanics of learning Here\u2019s another element for discu ssion: because you\u2019re never calling backward on val_loss, why are you building the graph in the first place? You could in fact call model and loss_fn as plain functions without tracki ng history. However optimized, tracking history comes with ad ditional costs that you could forgo during the validation pass, especially when the mode l has millions of parameters. To address this situation, PyTorch allows you to switch off autograd when you don\u2019t need it by using the torch.no_grad context manager. You won\u2019t see any meaningful advantage in terms of speed or memory co nsumption on your small problem. But for larger models, the differences can add up. Yo u can make sure that this context manager works by checking the value of the requires_grad attribute on the val_loss tensor: # In[16]: def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c): for epoch in range(1, n_epochs + 1): train_t_p = model(train_t_u, *params) train_loss = loss_fn(train_t_p, train_t_c) with torch.no_grad(): val_t_p = model(val_t_u, *params) val_loss = loss_fn(val_t_p, val_t_c) assert val_loss.requires_grad == False optimizer.zero_grad() train_loss.backward() optimizer.step() Using the related set_grad_enabled context, you can also co ndition code to run with autograd enabled or disabled, according to a Boolean expression, typically indicating whether you\u2019re running in training or inference. You could define a calc_forward function that takes data in input and runs model and loss_fn with or without auto - grad, according to a Boolean train_is argument: # In[17]: def calc_forward(t_u, t_c, is_train): with torch.set_grad_enabled(is_train): t_p = model(t_u, *params) loss = loss_fn(t_p, t_c) return loss Conclusion This chapter started with a big question: how can a machine learn from examples? We spent the rest of the chapter describing the mechanism by which a model can be opti - mized to fit data. We chose to stick with a simple model to show all the moving parts without unneeded complications. Context manager here. All requires_grad args are forced to False inside this block. 99PyTorch\u2019s autograd: Backpropagate all things Exercises Redefine the model to be w2 * t_u ** 2 + w1 * t_u + b. \u2013W h a t p a r t s o f t h e t r a i n i n g l o o p a n d s o on must be changed to accommodate this redefinition? \u2022W h a t p a r t s a r e a g n o s t i c t o s w a p p i n g o u t t h e m o d e l ? \u2013I s t h e r e s u l t i n g l o s s h i g h e r or lower after training? \u2022I s t h e r e s u l t b e t t e r o r w o r s e ? Summary Linear models are the simplest reasonable model to use to fit data. Convex optimization techniques can be used for linear models, but they don\u2019t generalize to neural networks, so this chapter focuses on parameter estimation. Deep learning can be used for generic mo dels that aren\u2019t engineered to solve a specific task but can be adapted automatically to specialize in the problem at hand. Learning algorithms amount to optimizing parameters of models based on observations. Loss function is a measure of the error in carrying out a task, such as the error between predicted outputs and measured values. The goal is to get loss function as low as possible. The rate of change of the loss function with respect to model parameters can be used to update the same parameters in the direction of decreasing loss. The optim module in PyTorch provides a collection of ready-to-use optimizers for updating parameters and minimizing loss functions. Optimizers use the autograd feature of PyTorch to compute the gradient for each parameter, depending on how that parameter contributed to the final out- put. This feature allows users to rely on the dynamic computation graph during complex forward passes. Context managers such as with torch.no_grad(): can be used to control auto- grad behavior. Data is often split into separate sets of training samples and validation samples, allowing a model to be evaluated on data it wasn\u2019t trained on. Overfitting a model happens when th e model\u2019s performance continues to improve on the training set but degrades on the valida tion set. This situation usually occurs when the model doesn\u2019 t generalize and instead memorizes the desired outputs for the training set. 101 Using a neural network to fit your data You\u2019ve taken a close look at how a linear model can learn and how to make it hap - pen in PyTorch, focusing on a simple re gression problem that r e q u i r e d a l i n e a r model with one input and one output. This simple example allowed you to dissect the mechanics of a model that learns without getting over ly distracted by the imple - mentation of the model itself. Backpropag ating error to the parameters and then updating those parameters by taking the gr adient with respect to the loss is going to be the same no matter what th e underlying model is (figure 5.1). I n t h i s c h a p t e r , y o u \u2019 r e g o i n g t o m ake changes in your model architecture. You\u2019re going to implement a full artificial neural netw ork to solve your problem. This chapter covers The use of nonlinear activation functions as the key difference from linear models The many kinds of activation functions in common use PyTorch\u2019s nn module, containing neural network building blocks Solving a simple linear-fit problem with a neural network Figure 5.1 Mental model of the learning process 102 CHAPTER 5 Using a neural network to fit your data Your thermometer conv ersion training loop and Fahr enheit-to-Celsius samples split into training and validation sets remain . You could start to use a quadratic model, rewriting your model as a quadratic function of its input (such as y = a * x**2 + b * x + c). Because such a model would be differ entiable, PyTorch would take care of computing gradients, and the training loop wo uld work as usual. That wouldn\u2019t be too interesting for you, though, because you\u2019d st ill be fixing the shape of the function. This chapter is where you start to hook the foundational work you\u2019ve put in with the PyTorch features you\u2019ll be using day in and day out as you work on your projects. You\u2019ll have an understanding of what\u2019s going on underneath the porcelain of the PyTorch API rather than thinking that it\u2019s so much black magic. Before we get into the implementation of the new model, though, we\u2019ll explain what we mean by artificial neural network. 5.1 Artificial neurons At the core of deep learning are neural networks, mathematical entities capable of rep - resenting complicated functi ons through a composition of s i m p l e r f u n c t i o n s . T h e term neural network obviously suggests a link to the way the human brain works. As a matter of fact, although the initial mo dels were inspired by neuroscience, 1 modern 1http://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0042519 103Artificial neurons artificial neural networks bear only a vague re semblance to the mechanisms of neu - rons in the brain. It seems likely that arti ficial and physiological neural networks use vaguely similar mathematical strategies for approximating complicated functions because that family of strategies works effectively. NOTE W e \u2019 r e g o i n g t o d r o p artificial and refer to these constructs as neural net- works from here on. The basic building block of thes e complicated functions is the neuron, pictured in figure 5.2. At its core, a neuron is noth ing but a linear transformation of the input (such as multiplication of the input by a number [the weight] and the addition of a constant [the bias]) followed by the application of a fixed nonlinear function (referred to as the activation function). Figure 5.2 An artificial neuron: a linear transformation enclosed in a nonlinear function Mathematically, you can write this as o = f(w * x + b), with x as the input, w as the weight or scaling factor, and b as the bias or offset. f is the activation function, set to the hyperbolic tangent or \u201ctanh\u201d function here. In general, x and hence o can be simple scalars, or vector -valued (holding many scalar values). Similarly, w can be a sin- gle scalar, or a matrix, whereas b is a scalar or vector (the dimensionality of the inputs and weights must match, however). In the latte r case, the expression is referred to as a 104 CHAPTER 5 Using a neural network to fit your data layer of neurons because it represents many neurons via the multidimensional weights and biases. A multilayer neural network, as represente d in figure 5.3, is a composition of the preceding functions: x_1 = f(w_0 * x + b_0) x_2 = f(w_1 * x_1 + b_1) ... y = f(w_n * x_n + b_n) where the output of a layer of neurons is used as an input for the following layer. Remember that w_0 here is a matrix, and x is a vector! Using a vector here allows w_0 to hold an entire layer of neurons, not just a single weight. Figure 5.3 A neural network with three layers An important difference between the earlier linear model and what you\u2019ll be using for deep learning is the shape of the error func tion. The linear model and error-squared loss function had a convex error curve with a singular, clearly defined minimum. If you were to use other methods, you could solve for it automatically and definitively. Your parame - ter updates were attempting to estimate that singular correct answer as best they could. N e u r a l n e t w o r k s d o n \u2019 t h a v e t h a t s a m e property of a convex error surface, even when using the same error-squared loss func tion. There\u2019s no single right answer for each parameter that you\u2019re attempting to ap proximate. Instead, you\u2019re trying to get all the parameters, when acting in concert, to produce useful output. Since that useful 105Artificial neurons output is only going to approximate the tr uth, there will be so me level of imperfec - tion. Where and how those imperfections ma nifest is somewhat arbitrary, and by implication the parameters that control the output (and hence the imperfections) are somewhat arbitrary as well. This output results in neural network training\u2019s looking much like parameter estimation from a mechanical perspective, but you must remem - ber that the theoretical underpinnings are q uite different. A big part of the reason why neural netw orks have nonconvex error surfaces is due to the activation function. The ability of an ensemble of neurons to approximate a wide range of useful functions depends on the combination of the linear and nonlin - ear behavior inherent to each neuron. 5.1.1 All you need is activation The simplest unit in (deep) neural networks is a linear operation (scaling + offset) fol - lowed by an activation function. You had a li near operation in your latest model; the linear operation was the entire model. The activation function has the role of concen - trating the outputs of the preceding linear operation into a given range. Suppose that you\u2019re assigning a \u201cgood dogg o\u201d score to images. Pictures of retriev - ers and spaniels should have a high scor e; images of airplanes and garbage trucks should have a low score. Bear pictures should have a low-ish score too, though higher than garbage trucks. The problem is that you have to define a high score. Because you\u2019ve got the entire range of float32 to work with, you can go pretty high. Even if you say \u201cIt\u2019s a ten point scale,\u201d sometimes your model is going to produce a score of 11 out of 10. Remember that under the hood, it\u2019s all sum of w*x+b matrix multiplications, which won\u2019t natu - rally limit themselves to a specific range of outputs. What you want to do is firmly constrain the output of your linear operation to a specific range so that the consumer of this output isn\u2019t having to handle numerical inputs of puppies at 12/10, bears at -10, and garbage trucks at -1000. One possibility is to cap the output values. Anything below zero is set to zero, and anything above 10 is set to 10. You us e a simple activati on function called torch.nn.Hardtanh.2 Another family of functions that works well is torch.nn.Sigmoid, which is 1 / (1 + e ** -x), torch.tanh, and others that you\u2019ll see in a moment. These functions have a curve that asymptotically ap proaches zero or negative one as x goes to negative infinity, approaches one as x increases, and has a mostly constant slope at x == 0. Con- ceptually, functions shaped this way work well, because it means that your neuron (which, again, is a linear function followed by an activation) will be sensitive to an area in the middle of your linear function\u2019s outp ut; everything else gets lumped up next to the boundary values. As you see in figure 5.4, a garbage truck gets a score of -0.97, whereas bears, foxes, and wolves may end up somewhere in the -0.3 to 0.3 range. 2See https://pytorch.org/docs/stable/nn.html#hardtanh , but note that the default range is -1 to +1. Figure 5.4 Dogs, bears, and garbage trucks being mapped to \u201chow doglike\u201d via the tanh activation function 106 CHAPTER 5 Using a neural network to fit your data Garbage trucks are flagged as \u201cnot dogs,\u201d the good dog maps to \u201cclearly a dog,\u201d and the bear ends up somewhere in the middl e. In code, you see the exact values: >>> import math >>> math.tanh(-2.2) -0.9757431300314515 >>> math.tanh(0.1) 0.09966799462495582 >>> math.tanh(2.5) 0.9866142981514303 With the bear in the sensitive range, small changes to the bear result in a noticeable change in the result. You could swap from a g r i z z l y t o a p o l a r b e a r ( w h i c h h a s a vaguely more traditionally canine face) an d see a jump up the Y axis as you slide toward the \u201cvery much a dog\u201d end of the gr aph. Conversely, a koala bear would regis- ter as less doglike and would see a drop in the activated output. There isn\u2019t much you could do to the garbage truck to make it register as doglike, though. Even with drastic changes, you might see a shift on ly from -0.97 to -0.8 or so. Quite a few activation functions are available, some of which are pictured in figure 5.5. In the first column, you see the continuous functions Tanh and Softplus; the sec- ond column has \u201chard\u201d versions of th e activation functions to their left, Hardtanh and ReLU. ReLU (Rectified Linear Unit) deserves spec ial note, as it is currently considered to be one of the best-performing general acti vation functions, as many state-of-the-art results have used it. The Sigmoid activation function, also known as the logistic func - Garbage truck Bear Good doggo 107Artificial neurons tion, was widely used in early deep learni ng work but has fallen out of common use. Finally, the LeakyReLU function modifies the standard ReLU to have a small positive slope rather than being strictly zero for nega tive inputs. (Typically, this slope is 0.01, but it\u2019s shown here with slope 0.1 for clarity.) Figure 5.5 A collection of common and not-so-common activation functions Activation function s are curious, because with such a wide variety of proven-suc - cessful ones (many more than pictured in fi gure 5.5), it\u2019s clear that there are few, if any, strict requirements. As such, we\u2019re going to discuss so me generalities about activa - tion functions that can probably be disprove d in the specific. That said, by definition, activation functions3 Are nonlinear\u2014Repeated applications of w*x+b w i t h o u t a n a c t i v a t i o n f u n c t i o n results in a polynomial. Th e nonlinearity allows the overall network to approxi- mate more complex functions. Are differentiable\u2014They\u2019re differentiable so that gradients can be computed through them. Point discontinuities, as you see in Hardtanh or ReLU, are fine. Without those functions, the network either falls back to being a complicated polyno - mial or becomes difficult to train. 3Even these statements aren\u2019t always true, of course: See https://openai.com/blog/nonlinear-computation- in-linear-networks. 108 CHAPTER 5 Using a neural network to fit your data Also generally true (though less so), the functions Have at least one sensitive range, where no-trivial changes to the input result in a corresponding nontrivial change in the output. Have at least one insensitive (or saturated) range, where changes to the input result in little to no change in the output. By way of example, the Hardtanh function could easily be used to make piecewise-lin - ear approximations of a function due to co mbining the sensitive range with different weights and biases on the input. Often (but far from universally so), the ac tivation function has at least one of the following: A lower bound that is approached (or met) as the input goes to negative infinity A similar-but-inverse upper bound for positive infinity Thinking about what you know about how backpropagation works, you can figure out that the errors will propag ate backward through the activation more effectively when the inputs are in the response range, wher eas errors won\u2019t greatly affect neurons for which the input is saturated (because the grad ient will be close to zero due to the flat area around the output). All put together, this mechanism is pretty powerful. What we\u2019re saying is that in a network built out of linear + activation un its, when different inputs are presented to the network, (a) different un its respond in different rang es for the same inputs, and (b) the errors associated with those inputs will primarily affect the neurons operating in the sensitive range, leaving other units mo re or less unaffected by the learning pro - cess. In addition, thanks to the fact that derivatives of the activation with respect to its inputs are often close to one in the sensitiv e range, estimating the parameters of the linear transformation through gradient de scent for the units that operate in that range will look a lot like the linear fit. You\u2019re starting to get a deeper intuitio n about how joining many linear + activation units in parallel and stacking them one af ter the other leads to a mathematical object that is capable of approximating complicat ed functions. Different combinations of units respond to inputs in diffe rent ranges and for those parameters are relatively easy to optimize through gradient descent, becaus e learning will behave a lot like that of a linear function until the output saturates. 5.1.2 What learning means for a neural network Building models out of stacks of linear tr ansformations followed by differentiable acti- vations leads to models that can approx imate highly nonlinear processes whose parameters you can estimate surprisingly well through gradient descent. This fact remains true even when you\u2019re dealing with models with millions of parameters. What makes using deep neural networks so attracti ve is that it allows you not to worry too much about the exact function that represents your data, whether it\u2019s quadratic, piecewise polynomial, or something else. With a deep neural network model, you 109Artificial neurons have a universal approximator and a method to estimate its parameters. This approxi - mator can be customized to yo ur needs, in terms of mode l capacity and its ability to model complicated input/output relationships, by composing simple building blocks. Figure 5.6 shows some examples. Figure 5.6 Composing multiple linear units and tanh activation functions to produce nonlinear outputs The four top-left graphs show four neurons\u2014A, B, C, and D\u2014each with its own (arbitrarily chosen) weight an d bias. Each neuron uses the Tanh activation function, with a minimum of -1 and a maximum of 1. The varied weights and biases move the center point and change how drastically the transition from min to max goes, but they clearly are all the same general shape. Th e columns to the righ t show both pairs of neurons added together (A+B and then C+D). Here, you start to see some interesting properties that mimic a single layer of neur ons. A+B shows a slight S curve, with the extremes approaching zero, but both a posi tive and negative bump in the middle. Conversely, C+D has only a large positive bump, which peaks at a higher value than the single-neuron max of 1. In the third row, you start to compose your neurons as they would be in a two-layer network. Both C(A+B) and D(A+B) have the same positive-and-negative-bumps that A+B shows, but the positive peak is more subt le. The composition of C(A+B)+D(A+B) shows a new property: two clear negative bumps and possibly a subtle second positive peak to the left of the main area of interest. All this occurs with only four neurons in two layers! 110 CHAPTER 5 Using a neural network to fit your data Again, these neurons\u2019 parameters were chos en only to create a visually interesting result. Training consists of finding acceptable values for these weights and biases so that the resulting network carries out a task correctly, such as predicting likely tem - peratures given geographic coordinates and time of the year. By carrying out a task suc- cessfully, we mean obtaining a correct output on unseen data produced by the same data-generating process used for training data. A successfully trained network, through the value of its weights and biases, captures the inherent structure of the data in the form of meaningful numerical repr esentations that work correctly for previ - ously unseen data. Here\u2019s another step in yo ur realization of the mechan ics of learning: deep neural networks allow you to approximate highly nonlinear phenomena without having an explicit model for them. Instead, starting from a generic, untr ained model, you spe - cialize it on a task by providing it a set of inputs and outputs and a loss function to backpropagate from. Specializi ng a generic model to a task by using examples is what we refer to as learning, because the model wasn\u2019t built with that specific task in mind; no rules describing how that task worked were encoded in the model. F o r y o u r t h e r m o m e t e r e x p e r i e n c e , y ou assumed that both thermometers mea - sured temperatures linearly. That assumption is where we implicitly encoded a rule for our task: we hard-coded the shape of ou r input/output function; we couldn\u2019t have approximated anything other than data points sitting around a line. As the dimen - sionality of a problem grows (many inputs to many outputs) and input/output rela - tionships get complicated, assu ming a shape for the input/output function is unlikely to work. The job of a physicist or an applie d mathematician is often to come up with a functional description of a phenomenon from first principles so that it\u2019s possible to estimate the unknown parameters from me asurements and get an accurate model of the world. Deep neural networks, at the ot her end, are families of functions that can approximate a wide range of input/output relationships without necessarily requiring one to come up with an explanatory mo del of a phenomenon. In a way, you\u2019re renouncing an explanation in exchange fo r the possibility of tackling increasingly complicated problems. In another way, you sometimes lack the ability, information, or computational resources to build an explicit model of what you\u2019re presented with, so data-driven methods are your only way forward. 5.2 The PyTorch nn module All this talk about neural networks may be getting you curious about building one from scratch with PyTorch. The first step is replacing your linear model with a neural network unit. This step is a somewhat-use less step backward from a correctness per - spective, because you\u2019ve alread y verified that your calibration required only a linear function, but it\u2019ll still be in strumental for starting a suff iciently simple problem and scaling up later. PyTorch has a whole submodule dedicated to neural networks: torch.nn. This sub- module contains the building blocks needed to create all sorts of neural network 111The PyTorch nn module architectures. Those buil ding blocks are called modules in PyTorch parlance (and layers in other frameworks). A P y T o r c h m o d u l e i s a P y t h o n c l a s s d e r i v i n g f r o m t h e nn.Module b a s e c l a s s . A Module can have one or more Parameter instances as attribut es, which are tensors whose values are optimized during t h e t r a i n i n g p r o c e s s . ( T h i n k w and b in your linear model.) A Module can also have one or more submodules (subclasses of nn.Module) as attributes, and it can track their Parameters as well. NOTE T h e s u b m o d u l e s m u s t b e t o p - l e v e l attributes, not buried inside list or dict instances! Otherwise, the optimizer won\u2019t be able to locate the submod- ules (and, hence, their parameters). For situations in which your model requires a list or dict of submodules, PyTorch provides nn.ModuleList and nn.ModuleDict. Unsurprisingly, you can find a subclass of nn.Module called nn.Linear, which applies an affine transformation to its input (via the parameter attributes weight and bias); it\u2019s equivalent to what you implemented earlier in your thermometer experiments. Now start precisely where you left off and co nvert your previous code to a form that uses nn. A l l P y T o r c h - p r o v i d e d s u b c l a s s e s o f nn.Module h a v e t h e i r call m e t h o d d e f i n e d , which allows you to instantiate an nn.Linear and call it as though it were a function, as in the following listing. # In[5]: import torch.nn as nn linear_model = nn.Linear(1, 1) linear_model(t_un_val) # Out[5]: tensor ([[-0.9852], [-2.6876]], grad_fn=<AddmmBackward>) Calling an instance of nn.Module with a set of arguments ends up calling a method named forward with the same arguments. The forward method executes the forward computation; call d o e s o t h e r r a t h e r i m p o r t a n t c h o r e s b e f o r e a n d a f t e r c a l l i n g forward. So it\u2019s technically possible to call forward directly, and it produces the same output as call, but it shouldn\u2019t be done from user code: >>> y = model(x) Correct!>>> y = model.forward(x) Silent error. Don\u2019t do it! The following listing shows the implementation of Module.call (with some simplifi - cations made for clarity). Listing 5.1 code/p1ch6/1_neural_networks.ipynb You look into the constructor arguments in a moment. 112 CHAPTER 5 Using a neural network to fit your data def __call__(self, *input, **kwargs): for hook in self._forward_pre_hooks.values(): hook(self, input) result = self.forward(*input, **kwargs) for hook in self._forward_hooks.values(): hook_result = hook(self, input, result) # ... for hook in self._backward_hooks.values(): # ... return result As you can see, a lot of hooks won\u2019t get called properly if you use .forward(\u2026) directly. Now turn back to the linear model. The constructor to nn.Linear accepts three arguments: the number of input features, the number of output features, and whether the linear model includes a bias (defaulting to True here). # In[5]: import torch.nn as nn linear_model = nn.Linear(1, 1) linear_model(t_un_val) # Out[5]: tensor([[-0.9852], [-2.6876]], grad_fn=<AddmmBackward>) The number of features in this case refers to the size of the input and the output tensor for the module, so 1 and 1. If you used both temperature and barometric pres - sure in input, for example, you\u2019d have two fe atures in input and one feature in out - put. As you\u2019ll see, for more complex mode ls with several intermediate modules, the number of features is associated with the capacity of the model. You have an instance of nn.Linear with one input and one output feature, which requires one weight and one bias: # In[6]: linear_model.weight # Out[6]: Parameter containing: tensor([[-0.4992]], requires_grad=True) # In[7]: linear_model.bias # Out[7]: Parameter containing: tensor([0.1031], requires_grad=True) Listing 5.2 torch/nn/modules/module.py, line 483, class: Module The arguments are input size, output size, and bias defaulting to True. 113The PyTorch nn module You can call the module with some input: # In[8]: x = torch.ones(1) linear_model(x) # Out[8]: tensor([-0.3961], grad_fn=<AddBackward0>) Although PyTorch let you get away with it, you didn\u2019t provide an input with the right dimensionality. You have a model that takes one input and produces one output, but PyTorch nn.Module and its subclasses are designed to do so on multiple samples at the same time. To accommodate multiple sa mples, modules expect the zeroth dimen- sion of the input to be the nu mber of samples in the batch. Any module in nn is written to produce outputs for a batch of multiple inputs at the same time. Thus, assuming that you need to run nn.Linear on 10 samples, you can create an input tensor of size B x Nin, where B is the size of the batch and Nin the number of input features, and run it once through the model: # In[9]: x = torch.ones(10, 1) linear_model(x) # Out[9]: tensor([[-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961], [-0.3961]], grad_fn=<AddmmBackward>) Figure 5.7 shows a simila r situation with batched image data. The input is BxCxHxW with a batch size of three (say, images of a dog, bird, and then car), three channel dimensions (red, green, and blue), and an unspecified number of pixels for height and width. As you can see, the output is a tensor of size B x Nout, where Nout is the number of output features\u2014four, in this case. The reason we want to do this batching is multi-faceted. One big motivation is to make sure that the computation we\u2019re asking for is big enough to saturate the comput - ing resources we\u2019re using to perform the co mputation. GPUs in particular are highly parallelized, so a single input on a small mo del will leave most of the computing units idle. By providing batches of inputs, the ca lculation can be spread across the otherwise- idle units, which means that the batched results come back just as quickly as a single one would. Another benefit is that some advanced models will use statistical informa - tion from the entire batch, and those statistics get better with larger batch sizes. Figure 5.7 Three RGB images batched together and fed into a neural network. The output is a batch of three vectors of size 4. 114 CHAPTER 5 Using a neural network to fit your data Now turn back to the thermometer data. Your t_u and t_c were two 1D tensors of size B. Thanks to broadcasting, you could write your linear model as w * x + b, where w and b are two scalar parameters. This model works because you have one input fea - ture; if you had two, you\u2019d need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns. That\u2019s exactly what you need to do to switch to using nn.Linear. You reshape your B inputs to B x Nin, where Nin is 1. You can easily do this with unsqueeze: # In[2]: t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] t_c = torch.tensor(t_c).unsqueeze(1) t_u = torch.tensor(t_u).unsqueeze(1) t_u.shape # Out[2]: torch.Size([11, 1]) You\u2019re done. Now update your training co de. First, replace your handmade model with nn.Linear(1,1); then pass the linear model parameters to the optimizer: # In[10]: linear_model = nn.Linear(1, 1) optimizer = optim.SGD( linear_model.parameters(), lr=1e-2) Here, you add the extra dimension at axis 1. A redefinition from above. You replace [params] with this method call. 115The PyTorch nn module Earlier, it was your responsibility to crea te parameters and pass them as the first argu - ment to optim.SGD. Now you can ask any nn.Module for a list of parameters owned by it or any of its submodules by using the parameters method: # In[11]: linear_model.parameters() # Out[11]: <generator object Module.parameters at 0x0000020A2B022D58> # In[12]: list(linear_model.parameters()) # Out[12]: [Parameter containing: tensor([[0.3791]], requires_grad=True), Parameter containing: tensor([-0.5349], requires_grad=True)] This call recurses into submod ules defined in the module\u2019s init c o n s t r u c t o r a n d returns a flat list of all parameters encountered, so you can conveniently pass it to the optimizer constructor as you did earlier. You can already figure out what happens in the training loop. The optimizer is pro - vided a list of tensors that were defined with requires_grad = True. All Parameters are defined this way, by definition, becaus e they need to be optimized by gradient descent. When training_loss.backward() is called, grad is accumulated on the leaf nodes of the graph, which are precisely the pa rameters that were passed to the optimizer. At this point, the SGD optimizer has everything it needs. When optimizer.step() is called, it iterates through each Parameter a n d c h a n g e s i t b y a n a m o u n t p r o p o r- tional to what is stored in its grad attribute, which is clean design. Take a look at the training loop now: # In[13]: def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val): for epoch in range(1, n_epochs + 1): t_p_train = model(t_un_train) loss_train = loss_fn(t_p_train, t_c_train) t_p_val = model(t_un_val) loss_val = loss_fn(t_p_val, t_c_val) optimizer.zero_grad() loss_train.backward() optimizer.step() if epoch == 1 or epoch % 1000 == 0: print('Epoch {}, Training loss {}, Validation loss {}'.format( epoch, float(loss_train), float(loss_val))) The training loop hasn\u2019t changed prac tically except that now you don\u2019t pass params explicitly to model because the model itself holds its Parameters internally. Now the model is passed in instead of the individual params. The loss function is also passed in. You\u2019ll use it in a moment. 116 CHAPTER 5 Using a neural network to fit your data You can use one last bit from torch.nn: the loss. Indeed, nn comes with several common loss functions, among which nn.MSELoss (MSE stands for Mean Square Error), which is exactly what you defined earlier as your loss_fn. Loss functions in nn are still subclasses of nn.Module, so create an instance and call it as a function. In this case, you get rid of the handwritten loss_fn and replace it: # In[15]: linear_model = nn.Linear(1, 1) optimizer = optim.SGD(linear_model.parameters(), lr=1e-2) training_loop( n_epochs = 3000, optimizer = optimizer, model = linear_model, loss_fn = nn.MSELoss(), t_u_train = t_un_train, t_u_val = t_un_val, t_c_train = t_c_train, t_c_val = t_c_val) print() print(linear_model.weight) print(linear_model.bias) # Out[15]: Epoch 1, Training loss 92.3511962890625, Validation loss 57.714385986328125 Epoch 1000, Training loss 4.910993576049805, Validation loss 1.173926591873169 Epoch 2000, Training loss 3.014694929122925, Validation loss 2.8020541667938232 Epoch 3000, Training loss 2.857640504837036, Validation loss 4.464878559112549 Parameter containing: tensor([[5.5647]], requires_grad=True) Parameter containing: tensor([-18.6750], requires_grad=True) Everything else input into our training lo op stays the same. Even our results remain the same as before. Of course, getting the same results is expected, as a difference would imply a bug in one of the two implementations. It\u2019s been a long journey, with a lot to explore for these twenty-something lines of code. We hope that by now, the magic ha s vanished and left room for the mechanics. What you learn in this chapter will allo w you to own the code you write instead of merely poking at a black box when things get more complicated. You have one last step left to take: repl acing your linear model with a neural net- work as your approximating function. As we said earlier, using a neural network won\u2019t result in a higher-quality model, because the process underlying the calibration prob - lem is fundamentally linear. It\u2019s good to ma ke the leap from linear to neural network in a controlled environmen t, however, so that you won\u2019t feel lost later on. You\u2019re no longer using your handwritten loss function from earlier. 117The PyTorch nn module This section keeps everything else fixed, including the loss function, and redefines only the model. You\u2019ll build the simplest possible neural network: a linear module fol - lowed by an activation functi on feeding into another linear module. The first linear + activation layer is co mmonly referred to as a hidden l a y e r f o r h i s t o r i c a l r e a s o n s , because its outputs aren\u2019t observed directly but fed into the output layer. Whereas the input and the output of the model are both of size 1 (they have one input and one output feature), the size of th e output of the first linear module usually is larger than one. Recalling the earlier explanation on th e role of activations, this situation can lead different units to respond to differen t ranges of the input, which increases the capacity of the model. The last linear laye r takes the output of activations and com - bines them linearly to produce the output value. nn provides a simple way to co ncatenate modules through the nn.Sequential con- tainer: # In[16]: seq_model = nn.Sequential( nn.Linear(1, 13), nn.Tanh(), nn.Linear(13, 1)) seq_model # Out[16]: Sequential( (0): Linear(in_features=1, out_features=13, bias=True) (1): Tanh() (2): Linear(in_features=13, out_features=1, bias=True) ) The result is a model that takes the inputs expected by the first module specified as an argument of nn.Sequential, passes intermediate outputs to subsequent modules, and produces the output returned by the last module. The model fans out from 1 input feature to 13 hidden features, passes them through a tanh a c t i v a t i o n , a n d l i n e a r l y combines the resulting 13 numbers into 1 output feature. Calling model.parameters() collects weight and bias from both the first and the second linear modules. It\u2019s in structive to inspect the parameters in this case by print - ing their shapes: # In[17]: [param.shape for param in seq_model.parameters()] # Out[17]: [torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])] These are the tensors that the op timizer will get. Again, after model.backward() is called, all parameters are populated with their grad, and then the optimizer updates their values accordingly during the optimizer.step() call, which isn\u2019t too different from the previous linear model. After all, both models are differentiable models that can be trained with gradient descent. 13 was chosen arbitrarily. We wanted to pick a number that was a different size from the other various tensor shapes floating around. This 13 must match the first size, however. 118 CHAPTER 5 Using a neural network to fit your data A few notes on parameters of nn.Modules: when you\u2019re inspecting parameters of a model made up of several subm odules, it\u2019s handy to be able to identify parameters by their names. There\u2019s a method for that, called named_parameters: # In[18]: for name, param in seq_model.named_parameters(): print(name, param.shape) # Out[18]: 0.weight torch.Size([13, 1]) 0.bias torch.Size([13]) 2.weight torch.Size([1, 13]) 2.bias torch.Size([1]) In fact, the name of each module in Sequential is the ordinal with which the module appeared in the arguments. Interestingly, Sequential also accepts an OrderedDict4 in which you can name each module passed to Sequential: # In[19]: from collections import OrderedDict seq_model = nn.Sequential(OrderedDict([ ('hidden_linear', nn.Linear(1, 8)), ('hidden_activation', nn.Tanh()), ('output_linear', nn.Linear(8, 1)) ])) seq_model # Out[19]: Sequential( (hidden_linear): Linear(in_features=1, out_features=8, bias=True) (hidden_activation): Tanh() (output_linear): Linear(in_features=8, out_features=1, bias=True) ) This code allows you to get more explanatory names for submodules: # In[20]: for name, param in seq_model.named_parameters(): print(name, param.shape) # Out[20]: hidden_linear.weight torch.Size([8, 1]) hidden_linear.bias torch.Size([8]) output_linear.weight torch.Size([1, 8]) output_linear.bias torch.Size([1]) You can also get to a particular Parameter by accessing submodules as though they were attributes: 4Not all versions of Python specify the iteration order fo r dict, so we\u2019re using OrderedDict here to ensure the ordering of the layers and emphasize that the order of the layers matters. 119The PyTorch nn module # In[21]: seq_model.output_linear.bias # Out[21]: Parameter containing: tensor([-0.2194], requires_grad=True) This code is useful for inspecting paramete rs or their gradients, such as to monitor gradients during training, as you did the be ginning of this chapter. Suppose that you want to print out the gradients of weight of the linear portion of the hidden layer. You can run the training loop for the new neur al network model and then look at the resulting gradients after the last epoch: # In[22]: optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) training_loop( n_epochs = 5000, optimizer = optimizer, model = seq_model, loss_fn = nn.MSELoss(), t_u_train = t_un_train, t_u_val = t_un_val, t_c_train = t_c_train, t_c_val = t_c_val) print('output', seq_model(t_un_val)) print('answer', t_c_val) print('hidden', seq_model.hidden_linear.weight.grad) # Out[22]: Epoch 1, Training loss 207.2268524169922, Validation loss 106.6062240600586 Epoch 1000, Training loss 6.121204376220703, Validation loss 2.213937759399414 Epoch 2000, Training loss 5.273784637451172, Validation loss 0.0025627268478274345 Epoch 3000, Training loss 2.4436306953430176, Validation loss 1.9463319778442383 Epoch 4000, Training loss 1.6909029483795166, Validation loss 4.027190685272217 Epoch 5000, Training loss 1.4900192022323608, Validation loss 5.368413925170898 output tensor([[-1.8966], [11.1774]], grad_fn=<AddmmBackward>) answer tensor([[-4.], [14.]]) hidden tensor([[-0.0073], [ 4.0584], [-4.5080], [-4.4498], [ 0.0127], [-0.0073], [-4.1530], [-0.6572]]) Note that the learning rate has dropped a bit to help with stability. 120 CHAPTER 5 Using a neural network to fit your data You can also evaluate the model on the whole data to see how different it is from a line: # In[23]: from matplotlib import pyplot as plt t_range = torch.arange(20., 90.).unsqueeze(1) fig = plt.figure(dpi=600) plt.xlabel(\"Fahrenheit\") plt.ylabel(\"Celsius\") plt.plot(t_u.numpy(), t_c.numpy(), 'o') plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-') plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx') This code produces figure 5.8. Figure 5.8 The plot of the neural network model, with input data (circles), desired output ( Xs), and continuous line showing behavior between samples You can appreciate that the neural network has a tendency to overfit because it tries to chase the measurements, including the noisy ones. It doesn\u2019t do a bad job overall, though. 5.3 Subclassing nn.Module For larger and more complex projects, you need to leave nn.Sequential behind in favor of something that gives yo u more flexibility: subclassing nn.Module. To subclass nn.Module, at a minimum you need to define a .forward(\u2026)function that takes the input to the module and returns the output. If you use standard torch operations, autograd takes care of the backward pass automatically. NOTE O f t e n , y o u r e n t i r e model i s i m p l e m e n t e d a s a s u b c l a s s o f nn.Module, which can in turn contain submodul es that are also subclasses of nn.Module. 121Subclassing nn.Module We\u2019ll show you three ways to implement th e same network structure, using increas - ingly more complex PyTorch functionality to do so and varying the number of neu - rons in the hidden layer to make it easier to differentiate among the approaches. The first method is a simple instance of nn.Sequential, as shown in the following listing. # In[2]: seq_model = nn.Sequential( nn.Linear(1, 11), nn.Tanh(), nn.Linear(11, 1)) seq_model # Out[2]: Sequential( (0): Linear(in_features=1, out_features=11, bias=True) (1): Tanh() (2): Linear(in_features=11, out_features=1, bias=True) ) Although this code works, you don\u2019t have any semantic information about what the various layers are intended to be. You can re ctify that situation by giving each layer a label, using an ordered dictiona ry instead of a list as input: # In[3]: from collections import OrderedDict namedseq_model = nn.Sequential(OrderedDict([ ('hidden_linear', nn.Linear(1, 12)), ('hidden_activation', nn.Tanh()), ('output_linear', nn.Linear(12 , 1)) ])) namedseq_model # Out[3]: Sequential( (hidden_linear): Linear(in_features=1, out_features=12, bias=True) (hidden_activation): Tanh() (output_linear): Linear(in_features=12, out_features=1, bias=True) ) Much better. You don\u2019t have any ability to control the flow of data through the net - work, however, aside from the purely sequ ential pass-through provided by the (aptly named!) nn.Sequential c l a s s . Y o u c a n t a k e f u l l c o n t r o l o f t h e p r o c e s s i n g o f i n p u t data by subclassing nn.Module yourself: # In[4]: class SubclassModel(nn.Module): def __init__(self): super().__init__() Listing 5.3 code/p1ch6/3_nn_module_subclassing.ipynb The choice of 11 is somewhat arbitrary,  but the sizes of the two layers must match. 122 CHAPTER 5 Using a neural network to fit your data self.hidden_linear = nn.Linear(1, 13) self.hidden_activation = nn.Tanh() self.output_linear = nn.Linear(13, 1) def forward(self, input): hidden_t = self.hidden_linear(input) activated_t = self.hidden_activation(hidden_t) output_t = self.output_linear(activated_t) return output_t subclass_model = SubclassModel() subclass_model # Out[4]: SubclassModel( (hidden_linear): Linear(in_features=1, out_features=13, bias=True) (hidden_activation): Tanh() (output_linear): Linear(in_features=13, out_features=1, bias=True) ) This code ends up being somewhat more verbose, because yo u have to define the layers you want to have and then define how and in what order they should be applied in the forward function. That repetition grants you an incredible amount of flexibility in the sequential models, however, as you\u2019re now free to do all sorts of interesting things inside the forward f u n c t i o n . A l t h o u g h t h i s e x a m p l e i s u n l i k e l y t o m a k e s e n s e , y o u c o u l d implement activated_t = self.hidden_activation(hidden_t) if random.ran - dom() > 0.5 else hidden_t t o a p p l y t h e a c t i v a t i o n f unction only half the time! Because PyTorch uses a dynamic graph-based autograd, gradients would flow properly through the sometimes-present activation, no matter what random.random() returned! Typically, you want to use the constructo r of the module to define the submodules that we call in the forward function so that they can hold their parameters through - out the lifetime of your module. Yo u might instantiate two instances of nn.Linear in the constructor and use them in forward, for example. Interestingly, assigning an instance of nn.Module to an attribute in a nn.Module, as you did in the constructor here, automatically registers the module as a submodule, which gives modules access to the parameters of its submodules without further action by the user. G o i n g b a c k t o t h e n o n r a n d o m SubclassModel, you see that the printed output for that class is similar to the output for the sequential model with named parame - ters. This makes sense, because you used the same names and intended to implement the same architecture. If you look at the pa rameters of all three models, you also see similarities there (except, again, for the di fferences in the number of hidden neurons): # In[5]: for type_str, model in [('seq', seq_model), ('namedseq', namedseq_model), ('subclass', subclass_model)]: print(type_str) for name_str, param in model.named_parameters(): print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel())) print() 123Subclassing nn.Module # Out[5]: seq 0.weight torch.Size([11, 1]) 11 0.bias torch.Size([11]) 11 2.weight torch.Size([1, 11]) 11 2.bias torch.Size([1]) 1 namedseq hidden_linear.weight torch.Size([12, 1]) 12 hidden_linear.bias torch.Size([12]) 12 output_linear.weight torch.Size([1, 12]) 12 output_linear.bias torch.Size([1]) 1 subclass hidden_linear.weight torch.Size([13, 1]) 13 hidden_linear.bias torch.Size([13]) 13 output_linear.weight torch.Size([1, 13]) 13 output_linear.bias torch.Size([1]) 1 What happens here is that the named_parameters() call delves into all submodules assigned as attributes in the constructor and recursively calls named_parameters() on them. No matter how nested the submodule is, any nn.Module can access the list of all child parameters. By accessing their grad attribute, which will have been popu - lated by autograd, the optimizer knows how to change parameters so as to minimize the loss. NOTE C h i l d m o d u l e s c o n t a i n e d i n s i d e P y t h o n list or dict instances won\u2019t be registered automatically! Subclasses can register those modules manually with the add_module(name, module) method of nn.Module5 or can use the provided nn.ModuleList a n d nn.ModuleDict c l a s s e s ( w h i c h p r o v i d e a u t o - matic registration for contained instances). Looking back at the implementation of the SubclassModel class, and thinking about the utility of registering submodules in th e constructor so that you can access their parameters, it appears to be a bit of a waste to also register su bmodules that have no parameters, such as nn.Tanh. Wouldn\u2019t it be easier to call them directly in the forward function?6 It certainly would. P y T o r c h h a s functional c o u n t e r p a r t s o f e v e r y nn m o d u l e . B y functional, we mean \u201chaving no internal state\u201d or \u201cwhose output value is solely and fully determined by the value input arguments.\u201d Indeed, torch.nn.functional p r o v i d e s m a n y o f t h e s a m e modules you find in nn, but with all eventual parameters moved as an argument to the function call. The functional counterpart of nn.Linear, for example, is nn.func- tional.linear, which is a function that has signature linear(input, weight, bias=None). The weight and bias parameters are arguments to the function. 5https://pytorch.org/docs/stable/ nn.html#torch.nn.Module.add_module 6Aren\u2019t rhetorical questions great? 124 CHAPTER 5 Using a neural network to fit your data To get back to your model, it makes sense to keep using nn modules for nn.Linear so that SubclassModel can to manage all its Parameter instances during training. You can safely switch to the functional counte rparts of Tanh, however, because it has no parameters: # In[6]: class SubclassFunctionalModel(nn.Module): def __init__(self): super().__init__() self.hidden_linear = nn.Linear(1, 14) self.output_linear = nn.Linear(14, 1) def forward(self, input): hidden_t = self.hidden_linear(input) activated_t = torch.tanh(hidden_t) output_t = self.output_linear(activated_t) return output_t func_model = SubclassFunctionalModel() func_model # Out[6]: SubclassFunctionalModel( (hidden_linear): Linear(in_features=1, out_features=14, bias=True) (output_linear): Linear(in_features=14, out_features=1, bias=True) ) The functional version is a bit more concis e and fully equivalent to the non-functional version (as your models get more complicate d, the saved lines of code start to add up!) Note that it would still make sense to instantiate modules that require arguments for their initialization in the constructor. HardTanh, for example, takes optional min_val and max_val arguments, and rather than re peatedly state those arguments in the body of forward, you could create a HardTanh instance and reuse it. TIP A l t h o u g h g e n e r a l - p u r p o s e s c i e n t i f i c f u n c t i o n s l i k e tanh s t i l l e x i s t i n torch.nn.functional i n v e r s i o n 1 . 0 , t h o s e e n t r y p o i n t s a r e d e p r e c a t e d i n favor of ones in the top-level torch namespace. More niche functions remain in torch.nn.functional. Conclusion We covered a lot in this chapter, although we dealt with a simple problem. We dis - sected building differentiable models and trai ning them by using gradient descent, using raw autograd first and then relying on nn. By now, you should have confidence in your understanding of what\u2019s going on behind the scenes. We hope that this taste of PyTorch has given you an appetite for more! The self.hidden_activation = \u2026 line is missing here. That line was replaced with the equivalent functional call here. 125Subclassing nn.Module Exercises Experiment with the number of hidden ne urons in your simple neural network model, as well as the learning rate. \u2013W h a t c h a n g e s r e s u l t i n a m o r e linear output from the model? \u2013C a n y o u g e t t h e m o d e l t o o b v i o u s l y o v e r f i t t h e d a t a ? The third-hardest problem in physics is finding a proper wine to celebrate dis- coveries. Load the wine data from chap ter 3 and create a new model with the appropriate number of input parameters. \u2013H o w l o n g d o e s i t t a k e t o t r a i n c o m p a r e d t o t h e t e m p e r a t u r e d a t a y o u \u2019 v e been using? \u2013C a n y o u e x p l a i n w h a t f a c t o r s c ontribute to the training times? \u2013C a n y o u g e t t h e l o s s t o d e c r e a s e w h i l e t r a i n i n g o n t h i s d a t a s e t ? \u2013H o w w o u l d y o u g o a b o u t g r a p h i n g t h i s d a t a s e t ? Summary Neural networks can be automatically ad apted to specialize in the problem at hand. Neural networks allow easy access to th e analytical derivatives of the loss with respect to any parameter in the model, which makes evolving the parameters efficient. Thanks to its automated di fferentiation engine, PyTorch provides such derivatives effortlessly. Activation functions around linear transf ormations make neural networks capa- ble of approximating highly nonlinear functions, at the same time keeping them simple enough to optimize. More resources A tremendous number of books and other resources are available to help teach deep learning. We recommend the following: The official PyTorch website: https://pytorch.org Grokking Deep Learning, by Andrew W. Traska, is a great resource for developing a strong mental model and intuition on the mechanism underlying deep neural net- works. For a thorough introduction and reference to the field, we direct you to Deep Learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. b Last but not least, the full version of this book is available in Early Access now, with an estimated print date in late 2019: https://www.manning.com/books/deep- learning-with-pytorch. a)https://www.manning.com/books/grokking-deep-learning b)https://www.deeplearningbook.org 126 CHAPTER 5 Using a neural network to fit your data The nn module, together with the tensor stan dard library, provides all the build- ing blocks for creating neural networks. To recognize overfitting, it\u2019s essential to maintain the training set of data points separate from the validation set. There\u2019s no one recipe to combat overfitting, but getting more data (or more variability in the data) and resorting to simpler models are good starts. Anyone who does data science should be plotting data all the time. 127 Symbols .storage property 22 A absolute value, loss function and 73 activation function 103 and noncovex error surfaces of neural networks 105 continuous 106 differentiable 107 insensitive range 108 linear operation and role of 105 nonlinearity 107 sensitive range 108 algorithm, behind machine learning 67 applications, enabled by deep learning 15 ArXiV public preprint repository, PyTorch and 3 ASCII (American Standard Code for Information Interchange) 55 automated translation systems, recurrent neural networks and 54 automatic differentiation, and computing gradient automatically 7 B backpropagation 7 autograd feature 84 basic requirement for 83 calling backward 85 chain rule 83 batching, modules and 113 bias parameter additive constant 71 estimation 72 neuron 103, 109 Brahe, Tycho 68 C C++, PyTorch and 10 calc_forward function 98 cat function, time series and 53 categorical data 44 and one-hot encoding 58 categorical values 44, 65 cell, Jupyter Notebook and 13 chain rule 83 and computing derivative of the loss 76 channel dimension, single-channel data formats and 63 channel, single, CT scans and 63 code and large collections of numerical data 18 updating training code 114 color channel 60 computation graph 85 dynamic 8\u20139 static 6\u20137 computations fast and massively parallel 34 single-neuron 7 computer intelligence 2 computers, complicated tasks and level of performance 1 context manager 98\u201399 contiguous method 28 index 128 INDEX continuous values 43 converging optimization 79 convex function 72 cost function. See loss function CPU, copying tensor to GPU 34 creation ops, tensor API and 36 csv module, loading CSV file and 41 CT (Computed Tomography) scans 63 sample, volread function and loading 63 CUDA 10 D data deep learning and large amounts of 2 different types of 40 handled and stored by PyTorch 16 linear models and fitting 71, 99 loading from file 41 numerical values 43\u201344 plotting 82\u201383 Python and loading CSV file 41 reshaping 55, 65 tabular 40\u201348 time series and making rendering easier 52 transformation from one representation to another 15\u201316 utilities for loading and handling 11 volumetric 63\u201364 DataLoader class 11 Dataset class 11 deep learning and applications enabled by 15 and deep neural networks as generic functions 70 and natural language processing (NLP) 54 and wide range of complicated tasks 3 as disruptive technology 4 as general class of algorithms 2 automatically created features and 4 change of perspective 4\u20135 competitive landscape of 9\u201310 described 2 hardware for 12\u201313 historical overview and current landscape 4\u20135 immediate vs. deferred execution 5\u20139 learning algorithm and producing correct outputs 67 models expressed in idiomatic Python 2 operation system and 13 sources available to help teaching 125 Deep Learning with PyTorch Git repository 41 derivatives accumulating at leaf nodes 85 computing individual 76 gradients 76 DICOM (Digital Imaging Communication and Storage) 63 dictionaries, one-hot encoding and 57, 65 differentiable activations, models and 108 Dijkstra, Edsger W. 2 diverging optimization 79 dtype argument 30\u201331 dynamic graph engine 8 E eager mode 5 ecosystem, various niches in 9 encoding and embedding 58\u201359 compressing to more manageable size 58 numbers into colors 60 one-hot encoding. See one-hot encoding popular encodings 56 epoch 78 errors, propagating backward through activation 108 execution deferred 6 immediate deep learning libraries and 5 Pythagorean theorem and example of 5 vs. deffered 5 neural networks and 6\u20139 PyTorch and 12 F feature engineering deep learning and 4 machine learning and 4 features, defined 4 file descriptor 32 fixed nonlinear function. See activation function floating-point numbers and intermediate representations 16 and numbers in Python 18 and transformation from one form of data to another 15 forward method 111 functions, deep learning and approximating complex 2 129INDEX G Google Colab 34 GPUs (graphics processing units) and PyTorch tensors transferred to 34 moving tensors to 34\u201335 PyTorch tensors and 17 support for CUDA 34 using multiple 13 grad attribute 84\u201385 gradient descent and estimating parameters of highly nonlinear processes 108 and large neural network models 74 and loss decrease 75 differentiable models 117 estimating parameters of linear transformation through 108 stochastic 89 vanilla gradient descent 87 gradients computing automatically 7 dynamic graph-based autograd 122 monitoring during training 119 zeroing explicitly 85 graph libraries, static vs. dynamic 8 graph mode 8 graphs dynamic advantage over static graph approaches 9 changes during successive forward passes 9 symbolic 7 H h5py library 33 Hardtanh activation function 106, 108 I images and pixels with higher numerical precision 60 and the most common channels 60, 65 and transpose function 61 casting tensor to floating-point 62 collection of scalars 60 file formats 60 geometric transformations 62 loading into Python 60 multiple scalars per grid point 60 normalizing values of pixels 62 single scalar per grid point 60 tensor preallocation 61 image recognition, and transformation from one form of data to another 16 imageio module 60 indexing advanced 31, 47 range indexing 31 tensors and 31 indexing, slicing, joining, and mutating ops, tensor API and 36 init constructor 115 input, converting into floating numbers 15 insensitive (saturated) range 108 interval values 43, 65 J Jupyter Notebook 13 PyTorch and 13 K Kepler, Johannes and history of science 69 and laws on planetary motion 67\u201369 and learning from data 69 Keras 9 kernel, code evaluation and 13 L Lasagne 9 LeakyReLU activation function 107 learning algorithm 67, 99 learning process, mental model of 71, 102 learning rates 79 learning_rate, as scaling factor in machine learning 75 linear operation as the simplest units in neural networks 105 constraining output to specific range 105 lists, in Python as collections of objects 18\u201319 one-dimensional 18 passing to the constructor 20 loss computing derivative of 76\u201377 repeated evaluations of 76 loss decrease 74\u201375 loss function and calculation of loss 72 130 INDEX convex 72 defined 72 emphasizing and discounting errors 72 in nn 116 nn.MSELoss 116 penalization terms 94 rate of change with respect to model parameters 75, 99 repeated evaluations 76 scalar 74 square difference vs. absolute difference 73 M machine learning algorithm behind 67 scaling factor 75 math ops, tensor API and 36 max_val argument 124 mean square loss 74 minibatch 89 min_val argument 124 models and training neural networks 72 as subclasses of nn.Module 120 deep learning and generic 69, 99 differentiable 83, 102 highly adaptable 91 linear 71\u201372 convex function 72 replacing with neural network units 110, 116 loss function 72 parameter estimation 72 plot of neural network models 120 repeated evaluations of 76 Module.call, implementation of 111 modules and accommodating multiple samples 113 child modules 123 in PyTorch 111 named_parameters method 118 nn and concatenating 117 nn.Module identifying parameters by their names 118 subclassing 120\u2013124 nn.Module base class 111 OrderedDict 118 submodules 118 momentum argument 89 multidimensional arrays libraries dealing with 17 transposing and 28 See also tensors N natural language processing (NLP) 54 network structure, ways to implement the same 121\u2013123 network, pretrained, running on new data 12 neural networks and approximating highly nonlinear phenomena 110 and automatic differentiation 7 and differences between immediate and deferred execution 6\u20139 and error-squared loss function 104 and neurons 7 and nonconvex error surfaces 104\u2013105 and time series 49\u201354 artificial 102 channel 51 deep 2 and approximating complicated functions 92 and exhibiting convex loss 72 and transformation from one form of data to another 16 and universal approximator 108 as families of functions 110 as generic functions 70 linear operation 105 described 102 embeddings generated by using 59 gradient descent and large neural network models 74 input data range and best training performance 62 introduction of convolutional 60 model function 91 moderately large 12 multilayer 104 operations and parameters in 39 recurrent 54 successfully trained 110 tensors and outputs 39 two levels of operation 55 what learning means for 108\u2013110 neurons activation function 103 and typical mathematical expression for single 7 artificial 103 defined 103 layer of neurons 104 neural networks and 7 sensitive range and errors affecting 108 131INDEX single-neuron computation and dynamic graph 8 single-neuron computation and static graph 7 symbolic graph 7 See also neural networks neuroscience, modern artificial neural networks and 102 NLP. See natural language processing nn.Linear, subclass of nn.Module 111\u2013112 nn.Sequential container, concatenating modules 117 normalization 81 numeric encoding, PyTorch and 41 numeric types allocating tensors of the right 30 and tensors 30\u201331 dtype argument and 30 NumPy and loading SCV file 41 as the most popular multidimensional-array library 17 interoperability 31\u201332 PyTorch and seamless interoperability with 17 NumPy argument, standard, similarity with dtype argument 30 NumPy arrays and PyTorch Tensor class 3 and similarities with Tensor 2 converting to PyTorch tensors 43 numpy method 32 O one-hot encoding 52 and parsing characters in text 56 representing categorical data in tensors and 58 OpenCL 34 operations, as methods of tensor objects 36 optim module 90, 99 optimization process 78 changing inputs 80 loss decrease 75 optim submodule 87 unstable 79 vanilla gradient descent 87 optimizers 88\u201389, 99, 115 optimizer.step() 115, 117 SGD (Stochastic Gradient Descent) 89, 115 testing more 90 two methods exposed by 88 vanilla gradient descent 89 ordinal values 43, 65 overfitting 92, 99 extreme example of 93 how to recognize 126 scenarios for training and validation losses 96 P Pandas and loading CSV file 41 concept of data frame 40 parallelism ops, tensor API and 37 parameters adaptive learning_rate 80 and scaling rate of change 75 applying updates iteratively 78 as PyTorch scalars 73 backpropagation and updating 7 estimating 72 initializing 74 small updates 80 updating, potential problem 80 parameters method 115 penalization terms 94 points tensor 22 Project Gutenberg 55 Pythagorean theorem, example od immediate execution and 5 Python HDF5 33 computation graph and 6 lists as sequential collections of objects 18 numbers as full-fledged objects 18 PyTorch, described 2 Python interpreter PyTorch and non-Python code 10 and significant consolidation of deep learning tooling ecosystem 9 as introduction to deep learning 2 as deep learning library 11 autograd 83\u201387, 99 switching off 98 autograd-enabled tensor standard library 10 automation of generic function-fitting 70 creative use 10 described 2 dynamic graph engine 8 functional counterparts 123 high-performance C++ runtime 4 immediate execution 5\u20139 implementing complicated models 4 minimizing cognitive overhead 4, 14 main components of 10\u201312 132 INDEX modules as building blocks for creating neural networks 11, 111 nn module 110\u2013120 official website 125 production deployment capabilities 12 reasons for using 3\u201310 running directly from C 10 seamless interoperability with NumPy 17, 42 simplicity of 3 smaller-scope projects and 3 tensor operations offered by 35 Tensor, as multidimensional array 2 tensors as building blocks for representing data in 17, 39 transposing in 26\u201328 use of class indices 46 R random sampling ops, tensor API and 37 randperm function 94 rate of change 75, 99 computing 76 recurrent neural networks 54 ReLU (Rectified Linear Unit) activation function 106 representation deeper, and capturing more-complex structures 16 intermediate 16 transforming from one to another 15 requires_grad=True argument 84 RGB channels 60 S scatter_ method arguments for 45\u201346 one-hot encoding and 45 score as continuous variable 44 distance between scores 45 keeping in separate tensor 44 one-hot encoding 44 sensitive range 108 serialization ops, tensor API and 37 shape property 25 Sigmoid activation function 106 singleton dimension 46 size, defined 24 Size class 25 Softplus activation function 106 Stochastic Gradient Descent (SGD) 89 Storage, as contiguous, linear container for numbers 52 storages .storage property 22 accessing for given tensor 22 and direct use of storage instances 23 changing values of 23 defined 22 indexing manually 23 layout of 23 muliple tensors and indexing the same 22 Storage instance 22 storage offset, defined 24 stride changing the order of elements 27 defined 24 submodules, registering 123 subtensors changing 26 cloning 26 extracting 25 systems, image-based 60 T tabular data as the simplest form of data 40 as typically non-homogeneous 40 described 40 sets freely available on the internet 41 Tanh activation function 106 TensorFlow 8 eager mode of 9 TensorFlow library 9 tensors 2D 20 accessing 21 advanced indexing 31 and acceleration of mathematical operations 2 and data types represented by 19 and defining operations over 19 as building blocks for representing data in PyTorch 17 as fundamental data structures 17 as multidimensional arrays provided by PyTorch 10 binary tensor 47 compared with NumPy arrays 17 contiguous 28 conversion into NumPy arrays 31 converting data to 43\u201354 CPU- and GPU-based 34 defined 18 dimensionality of 17 133INDEX encoding real-world data, example of 41\u201348 fundamentals 18\u201322 grad attribute 84 groups of operations 36\u201337 homogeneous, versus tabular data 40 keeping score in separate 44 list indexing compared to tensor indexing 18 obtaining PyTorch tensors from NumPy arrays 32 params as an ancestor 84 params receiving too large updates 79 relationship among offset, size and stride 24 saving 32\u201334 serialization 32\u201334 shuffling elements of 94 size, indexing into a storage and 24 stride, indexing into a storage and 24 subtensors 26 tensor numeric values vs. Python object numeric values 20 transpose operation applied to 27 use of zeros or ones to initialize 21 verification of shared storage 26 zero-dimensional 73 text and recurrent neural networks 54 character-level encoding, example of 55\u201356 embedding 58\u201359 encoding 55 one-hot encoding 56\u201358 word-level encoding, example of 57 Theano library 9 time series 49\u201354 breaking up data set in wider observation periods 51 calling view on tensors 51 concatenation 52 data set 50 number of samples 51 options for rescaling variables 54 rearranging tensors 52 separate axes 49 transformation of 2D data set into 3D data set, example of 49\u201354 torch module 10, 19 and operations on and between tensors 36 torch.autograd 10 torch.distributed 12 torch.from_numpy function 34 torch.le function 46 torch.nn submodule 110 torch.nn, and modules for building neural networks 11 torch.nn.DataParallel 12 torch.nn.Hardtanh activation function 105 torch.nn.Sigmoid activation function 105 torch.optim 12 torch.Storage instances 22 torch.tanh activation function 105 torch.util.data 11 TorchScript, deferred execution model 12 training loop epoch 78 invoking 78 training loss 92 overfitting, scenarios for 96 training samples 72, 78, 99 training set 92, 99 and model performance 96 backpropagation 96 splitting data 94 train_is argument 98 train_loss, calling backward on 96\u201397 transpose function 36 U universal approximator, and modeling complex input/output relationships 109 unsqueeze, adding a singleton dimension 46, 52 V validation loss evaluating at every epoch 95 overfitting 93 scenarios for 96 training loss and 96 validation samples 99 validation set 92\u201393, 96, 99 val_loss, calling backward on 97 volumetric data 63, 65 W weight parameter derivative of loss function 77 estimation 72 linear scaling 71 neuron 103, 109 updates 80 Wikipedia 55 Z zero_ method 86",
  "job_description": "Role:\n\nYou are a Senior Software Engineer and Technical Mentor with real industry experience reviewing production systems, student projects, and early-stage startup codebases.",
  "current_question": "",
  "started_at": "2026-01-21T01:26:13.608290",
  "ended_at": null,
  "total_questions_asked": 0,
  "total_filler_words": 0,
  "average_wpm": 0.0,
  "exchanges": []
}